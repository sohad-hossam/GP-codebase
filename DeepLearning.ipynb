{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:31:21.951979Z","iopub.status.busy":"2024-05-25T20:31:21.951276Z","iopub.status.idle":"2024-05-25T20:31:24.799265Z","shell.execute_reply":"2024-05-25T20:31:24.798325Z","shell.execute_reply.started":"2024-05-25T20:31:21.951945Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\cude_env2\\lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language.build_library is deprecated. Use the new bindings instead.\n","  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n","c:\\ProgramData\\anaconda3\\envs\\cude_env2\\lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n","  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"]}],"source":["from PreProcessor import *\n","from DatasetRetrieval import *\n","_PreProcessor = PreProcessor()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["_dataset_retrieval_obj = DatasetRetrieval()\n","merged_df = _dataset_retrieval_obj.parseSqlite(\"Dataset/errai_dataset/errai.sqlite3\")\n","CC_UC_dict_true, UC_to_index, index_to_UC, CC_index_dict, index_to_CC = _dataset_retrieval_obj.getGlobalDatasetData(merged_df, 'errai_repo_clone')\n","CC_train_docs, UC_train_docs, CC_test_docs, UC_test_docs, CC_UC_train, CC_UC_test, CC_UC_train_labels, CC_UC_test_labels = _dataset_retrieval_obj.splitTestTrain(CC_UC_dict_true, UC_to_index, index_to_UC, CC_index_dict, index_to_CC)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2050\n"]}],"source":["print(len(CC_train_docs))"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["CC_train = _PreProcessor.setupDeepLearning(CC_train_docs,'CC','train')\n","UC_train = _PreProcessor.setupDeepLearning(UC_train_docs,'UC','train')\n","CC_test = _PreProcessor.setupDeepLearning(CC_test_docs,'CC','test')\n","UC_test = _PreProcessor.setupDeepLearning(UC_test_docs,'UC','test')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(CC_train_docs))"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["_PreProcessor.setUpUnknown(CC_train,'train')\n","_PreProcessor.setUpUnknown(UC_train,'train')\n","\n","_PreProcessor.setUpUnknown(CC_test,'test')\n","_PreProcessor.setUpUnknown(UC_test,'test')"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["with open('./pickles/DeepLearning/CC_train.pkl', 'wb') as f:\n","       pickle.dump(CC_train, f)\n","\n","with open('./pickles/DeepLearning/UC_train.pkl', 'wb') as f:\n","        pickle.dump(UC_train, f)\n","\n","with open('./pickles/DeepLearning/CC_test.pkl', 'wb') as f:\n","       pickle.dump(CC_test, f)\n","with open('./pickles/DeepLearning/UC_test.pkl', 'wb') as f:\n","        pickle.dump(UC_test, f)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["CC_train = np.load('./pickles/DeepLearning/CC_train.pkl',allow_pickle=True)\n","UC_train = np.load('./pickles/DeepLearning/UC_train.pkl',allow_pickle=True)\n","CC_test = np.load('./pickles/DeepLearning/CC_test.pkl',allow_pickle=True)\n","UC_test = np.load('./pickles/DeepLearning/UC_test.pkl',allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(CC_test[0:10])"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["# CC_word2vec = _PreProcessor.word2VecProcessor(function_names_train, function_segments_train, 'CC')\n","# UC_word2vec = _PreProcessor.word2VecProcessor(summaries_train, descriptions_train, 'UC')\n","# print(CC_word2vec)\n","word2vec_model = Word2Vec(sentences = CC_train + UC_train, vector_size=200, window=5, min_count=1, workers=4, epochs=10)\n","word2vec_model.save(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n","\n","#word2vec_model.build_vocab(UC_word2vec, update=True)\n","#word2vec_model.train(UC_word2vec, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n","\n","# word2vec_model.save(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n","word2vec_model = Word2Vec.load(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n","\n","word2vec_vocab = word2vec_model.wv.index_to_key\n","\n","# Initialize an empty embedding matrix\n","embedding_matrix = np.zeros((len(word2vec_vocab) + 1, word2vec_model.vector_size))\n","\n","# Fill the embedding matrix with the embeddings of each word\n","for i, word in enumerate(word2vec_vocab):\n","    embedding_vector = word2vec_model.wv[word]\n","    if embedding_vector is not None:\n","        embedding_matrix[i + 1] = embedding_vector\n","\n","with open('./pickles/DeepLearning/embedding_matrix.pkl', 'wb') as f:\n","       pickle.dump(embedding_matrix, f)\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:31:26.338226Z","iopub.status.busy":"2024-05-25T20:31:26.337410Z","iopub.status.idle":"2024-05-25T20:31:26.357811Z","shell.execute_reply":"2024-05-25T20:31:26.356823Z","shell.execute_reply.started":"2024-05-25T20:31:26.338192Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["3590\n"]}],"source":["embedding_matrix = np.load('./pickles/DeepLearning/embedding_matrix.pkl',allow_pickle=True)\n","print(len(embedding_matrix))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(UC_test[:2])"]},{"cell_type":"markdown","metadata":{},"source":["### Converting dataset to indices"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["_PreProcessor.vocabToIndex(word2vec_vocab)\n","_PreProcessor.dataSetToIndex(CC_train)\n","_PreProcessor.dataSetToIndex(UC_train)\n","\n","_PreProcessor.dataSetToIndex(CC_test)\n","_PreProcessor.dataSetToIndex(UC_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(UC_train[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# min_len = 100000000\n","# #cc => 4000\n","# print(len(UC_train))\n","# import statistics\n","# data = []\n","\n","# for cc in UC_train:\n","#     data.append(len(cc))\n","# # Sample data\n","\n","# # Calculate the mean\n","# mean_value = statistics.mean(data)\n","# print(mean_value)\n","\n","# counter=0\n","# for cc in UC_train:\n","#     if len(cc) > mean_value + 800:\n","#         counter+=1\n","# print(counter)\n","print(len(CC_train))"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["x_train, labels = _PreProcessor.setUpLabels(CC_train,UC_train,\"dataset/errai_dataset/train.csv\")"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["x_test, labels_test = _PreProcessor.setUpLabels(CC_test,UC_test,\"dataset/errai_dataset/test.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(x_train))\n","print(len(labels))\n","print(len(x_test))\n","print(len(labels_test))"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["with open('./pickles/DeepLearning/x_train.pkl', 'wb') as f:\n","       pickle.dump(x_train, f)\n","with open('./pickles/DeepLearning/labels.pkl', 'wb') as f:\n","        pickle.dump(labels, f)"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["with open('./pickles/DeepLearning/x_test.pkl', 'wb') as f:\n","       pickle.dump(x_test, f)\n","with open('./pickles/DeepLearning/labels_test.pkl', 'wb') as f:\n","        pickle.dump(labels_test, f)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:31:31.517931Z","iopub.status.busy":"2024-05-25T20:31:31.517584Z","iopub.status.idle":"2024-05-25T20:32:12.443178Z","shell.execute_reply":"2024-05-25T20:32:12.441922Z","shell.execute_reply.started":"2024-05-25T20:31:31.517906Z"},"trusted":true},"outputs":[],"source":["x_train = np.load('./pickles/DeepLearning/x_train.pkl',allow_pickle=True)\n","labels = np.load('./pickles/DeepLearning/labels.pkl',allow_pickle=True)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:32:12.446016Z","iopub.status.busy":"2024-05-25T20:32:12.445616Z","iopub.status.idle":"2024-05-25T20:32:15.959049Z","shell.execute_reply":"2024-05-25T20:32:15.958170Z","shell.execute_reply.started":"2024-05-25T20:32:12.445980Z"},"trusted":true},"outputs":[],"source":["x_test = np.load('./pickles/DeepLearning/x_test.pkl',allow_pickle=True)\n","labels_test = np.load('./pickles/DeepLearning/labels_test.pkl',allow_pickle=True)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:32:15.960455Z","iopub.status.busy":"2024-05-25T20:32:15.960131Z","iopub.status.idle":"2024-05-25T20:32:15.966552Z","shell.execute_reply":"2024-05-25T20:32:15.965563Z","shell.execute_reply.started":"2024-05-25T20:32:15.960428Z"},"trusted":true},"outputs":[],"source":["class TracibilityLinkDataset(Dataset):\n","\n","    def __init__(self, x_train, y_train):\n","        \n","        self.tensor_x_train = x_train\n","        self.tensor_y_train = torch.tensor(y_train)\n","\n","\n","    # len and get item are very important --> used by dataloader\n","    def __len__(self):\n","        return len(self.tensor_y_train)\n","\n","    def __getitem__(self, idx):\n","        return self.tensor_x_train[idx], self.tensor_y_train[idx]\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:32:15.969202Z","iopub.status.busy":"2024-05-25T20:32:15.968921Z","iopub.status.idle":"2024-05-25T20:32:17.845301Z","shell.execute_reply":"2024-05-25T20:32:17.844103Z","shell.execute_reply.started":"2024-05-25T20:32:15.969179Z"},"trusted":true},"outputs":[],"source":["dataset_train = TracibilityLinkDataset(x_train, labels)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:32:17.847031Z","iopub.status.busy":"2024-05-25T20:32:17.846642Z","iopub.status.idle":"2024-05-25T20:32:17.982608Z","shell.execute_reply":"2024-05-25T20:32:17.981350Z","shell.execute_reply.started":"2024-05-25T20:32:17.846980Z"},"trusted":true},"outputs":[],"source":["dataset_test = TracibilityLinkDataset(x_test, labels_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# torch.save(dataset_train,'./dataset/teiid_dataset/DeepLearningDataset.pt')\n"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/dataset-200/dataset_train_200.pt'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/dataset-200/dataset_train_200.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cude_env2\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cude_env2\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cude_env2\\lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/dataset-200/dataset_train_200.pt'"]}],"source":["dataset_train = torch.load('/kaggle/input/dataset-200/dataset_train_200.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset_test = torch.load('/kaggle/input/dataset-200/dataset_test_200.pt')\n"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"ename":"InvalidParameterError","evalue":"The 'classes' parameter of compute_class_weight must be an instance of 'numpy.ndarray'. Got [0, 1] instead.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)","Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mscikit_class_weight\u001b[39;00m\n\u001b[0;32m      3\u001b[0m class_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m class_weight_value \u001b[38;5;241m=\u001b[39m \u001b[43mscikit_class_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_y_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m class_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(class_weight_value)) \u001b[38;5;66;03m#2\u001b[39;00m\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cude_env2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\cude_env2\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n","\u001b[1;31mInvalidParameterError\u001b[0m: The 'classes' parameter of compute_class_weight must be an instance of 'numpy.ndarray'. Got [0, 1] instead."]}],"source":["import sklearn.utils.class_weight as scikit_class_weight\n","\n","class_list = [0,1]\n","class_weight_value = scikit_class_weight.compute_class_weight(class_weight ='balanced', classes = class_list, y = dataset_train.tensor_y_train.tolist())\n","class_weight = dict()\n","print(len(class_weight_value)) #2\n","\n","for i in range(2):\n","    class_weight[i] = 1\n","    \n","print(class_weight)\n","# Build the dictionary using the weight obtained the scikit function\n","for i in range(len(class_list)):\n","    class_weight[class_list[i]] = class_weight_value[i]\n","\n","print(class_weight)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["x_train_small = []\n","label_small = []\n","counter_1 = 0\n","counter_0 = 0\n","while(counter_1 < 11 or counter_0 < 11):\n","    index = random.randint(0, len(x_train))\n","\n","    if labels[index] == 1 and counter_1 < 11 :\n","        x_train_small.append(x_train[index])\n","        label_small.append(1)\n","        counter_1 +=1\n","\n","    elif labels[index] == 0 and counter_0 < 11 :\n","        x_train_small.append(x_train[index])\n","        label_small.append(0)\n","        counter_0 +=1\n","dataset_small = TracibilityLinkDataset(x_train_small, label_small)    "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:32:17.985231Z","iopub.status.busy":"2024-05-25T20:32:17.984794Z","iopub.status.idle":"2024-05-25T20:32:18.006876Z","shell.execute_reply":"2024-05-25T20:32:18.005767Z","shell.execute_reply.started":"2024-05-25T20:32:17.985197Z"},"trusted":true},"outputs":[],"source":["class DLModel(nn.Module):\n","\n","  def __init__(self,embedding_matrix : np.array, embedding_dim : tuple,UC_size: int, CC_size:int, hidden_size:int = 64, classes: int = 2):\n","      super(DLModel, self).__init__()\n","\n"," \n","      self.embedding = nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim = embedding_dim[1], _weight = torch.tensor(embedding_matrix))\n","      \n","      self.lstm_CC = nn.LSTM(embedding_dim[1],hidden_size,batch_first=True, bidirectional=False)\n","        \n","      # self.linear_post_flatten = nn.Linear(CC_size*hidden_size+UC_size*hidden_size,128)\n","      # self.linear_post_flatten2 = nn.Linear(128,32)\n","\n","      self.conv_layer = nn.Conv1d(CC_size+UC_size,hidden_size*2 ,3,stride=3)\n","      self.linear = nn.Linear(hidden_size*2*21,classes)\n","\n","      self.softmax = nn.Softmax(dim=1)\n","      \n","      # self.cosine_similarity = nn.CosineSimilarity(dim=1) # code = 1*(vocab_size)*(feature_vector) , req = code = 1*1000*200 \n","\n","      # self.sigmoid = nn.Sigmoid()\n","    \n","  def forward(self, CC,UC ):\n","    #function_names.shape --> batch_no * no_tokens* 150\n","    \n","    CC_embedding  = self.embedding(CC)\n","    UC_embedding = self.embedding(UC)\n","    \n","    #function_names.shape --> batch_no * no_tokens* 150\n","    \n","    \n","    UC_lstm,_ = self.lstm_CC (UC_embedding.float())\n","    CC_lstm,_ = self.lstm_CC (CC_embedding.float())\n","\n","    \n","    #function_names.shape --> batch_no * no_tokens* 150\n","\n","    conv = self.conv_layer (torch.cat([UC_lstm,CC_lstm],axis=1).float())\n","    conv_flatten = conv.reshape(conv.shape[0],-1)\n","\n","    # UC_lstm_flatten = UC_lstm.reshape(UC_lstm.shape[0],-1)\n","    # CC_lstm_flatten = CC_lstm.reshape(CC_lstm.shape[0],-1) # batch_no * (feature_vector * no_tokens)\n","\n","\n","    # linear_output_one = self.linear_post_flatten(torch.cat([UC_lstm_flatten,CC_lstm_flatten],axis=1).float())\n","    # print(linear_output_one)\n","    # linear_output_two = self.linear_post_flatten2(linear_output_one.float())\n","    # print(conv.shape)\n","    linear_output = self.linear(conv_flatten)\n","    \n","    \n","\n","    return linear_output\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["UC_size  = 1000\n","CC_size = 4000"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:32:18.008812Z","iopub.status.busy":"2024-05-25T20:32:18.008493Z","iopub.status.idle":"2024-05-25T20:32:18.032906Z","shell.execute_reply":"2024-05-25T20:32:18.031847Z","shell.execute_reply.started":"2024-05-25T20:32:18.008777Z"},"trusted":true},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","\n","def weighted_binary_cross_entropy(output, target):\n","        \n","#     loss = weights[1] *(target * torch.log(output)) + \\\n","#            weights[0] * ((1 - target) * torch.log(1 - output))\n","#     output --> kber ,loss \n","    loss = (target * torch.log(output)) + \\\n","            ((1 - target) * torch.log(1 - output))\n","\n","    return torch.mean(loss)\n","\n","def customCollate(batch: list):\n","    # batch --> tuple (x,y)\n","    # x --> list of 4 lists\n","    x_batch, y_batch = zip(*batch)\n","    \n","    CC,UC = zip(*x_batch)\n","    \n","    CC_padded = []\n","    UC_padded = []\n","    \n","    for i  in range(len(x_batch)):\n","        \n","        CC_padded.append( list(CC[i]) + [0] * (CC_size - len(CC[i])) )\n","        UC_padded.append( list(UC[i]) + [0] * (UC_size - len(UC[i])) )\n","        \n","\n","    return  CC_padded,UC_padded, y_batch\n","\n","def train(model, train_dataset, batch_size=50, epochs=1 ,learning_rate=0.001):\n","    \n","    # (1) create the dataloader of the training set IMPORTANT (make the shuffle=True)\n","    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True ,collate_fn = customCollate)\n","    \n","    \n","    # (2) make the criterion cross entropy loss\n","    criterion = torch.nn.CrossEntropyLoss()\n","    # (3) create the optimizer (Adam)\n","    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n","\n","    \n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n","\n","    if use_cuda:\n","        model = model.cuda()\n","        criterion = criterion.cuda()\n","   \n","\n","    for epoch_num in range(epochs):\n","        total_acc_train = 0\n","        total_loss_train = 0\n","        n_samples = 0\n","        y_size = 0\n","        true_pos = 0\n","        false_neg = 0\n","        false_pos = 0 \n","        true_neg = 0\n","        for CC,UC, y_batch in tqdm(train_dataloader):\n","            # torch.cuda.empty_cache()     \n","            CC_tensor =torch.tensor(CC, device=device)\n","            UC_tensor = torch.tensor(UC, device=device)\n","           \n","            y_batch_tensor = torch.tensor(y_batch,dtype = torch.float32 ,device=device)\n","            \n","            output = model.forward(CC_tensor,UC_tensor)\n","#             _,predicted=torch.max(output)  #512 * 104  for every word in every sentence we choose one tag form the seventen tag \n","#             output_0_1 = torch.tensor((output > 0.5), dtype=torch.float32)\n","            \n","            # for i,value in enumerate(output_0_1): #calc the fals negtive\n","            #     if value == 0 and  y_batch_tensor[i] == 1:\n","            #       false_neg+=1\n","            #     elif value == 1 and  y_batch_tensor[i] == 1:\n","            #       true_pos+=1\n","            #     elif value == 1 and  y_batch_tensor[i] == 0:\n","            #       false_pos += 1\n","            #     else:\n","            #       true_neg += 1\n","            \n","#             weights = torch.where(y_batch_tensor == 0, torch.tensor(class_weights[0]).to(device), torch.tensor(class_weights[1]).to(device)).to(device)\n","            \n","            _,predicted=torch.max(output,dim=1)\n","            batch_loss = criterion(output,y_batch_tensor.long())\n","            # batch_loss = criterion(output, y_batch_tensor)\n","            # torch.cuda.empty_cache()\n","            total_loss_train += batch_loss\n","            # print(torch.cuda.memory_reserved()/1024**3)\n","              \n","\n","            # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n","            \n","    #       (9) calculate the batch accuracy (just add the number of correct predictions)\n","            acc = (predicted==y_batch_tensor).sum().item()\n","            n_samples += y_batch_tensor.size(0)\n","            total_acc_train += acc\n","\n","\n","            optimizer.zero_grad()\n","\n","    #       (11) do the backward pass\n","            batch_loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","    #       (12) update the weights with your optimizer\n","            optimizer.step()\n","            # (10) zero your gradients\n","            \n","    #   epoch loss\n","        epoch_loss = total_loss_train / n_samples\n","\n","    #   (13) calculate the accuracy\n","        epoch_acc =100*total_acc_train/n_samples\n","\n","        print(\n","           f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n","           | Train Accuracy: {epoch_acc}\\n')\n","        print(\"true_pos = \", true_pos)\n","        print(\"false_neg = \",false_neg)\n","        print(\"false_pos = \",false_pos)\n","        print(\"true_neg = \", true_neg)\n","        \n","\n","  ##############################################################################################################"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T20:32:21.245815Z","iopub.status.busy":"2024-05-25T20:32:21.245129Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 86/86 [06:57<00:00,  4.85s/it]"]},{"name":"stdout","output_type":"stream","text":["Epochs: 1 | Train Loss: 0.01494288444519043            | Train Accuracy: 51.951104842501174\n","\n","true_pos =  0\n","false_neg =  0\n","false_pos =  0\n","true_neg =  0\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model = DLModel(embedding_matrix, embedding_matrix.shape,UC_size,CC_size, hidden_size=64, classes=2)    \n","# model.load_state_dict(torch.load('ModelCrossentropyLoss_lstmLayer+linear_'+str(i-1)+'_epoch_weight24_0.001.pth'))\n","# model.train()\n","train(model, dataset_train)\n","# torch.save(model.state_dict(), 'ModelCrossentropyLoss_lstmLayer+linear_'+str(i)+'_epoch_weight24_0.001.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'SelfAttentionModel_?epochs_200_tokens.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["epoch_loss = total_loss_train / torch.tensor(y_batch).size(0)\n","\n","    #   (13) calculate the accuracy\n","epoch_acc =100*total_acc_train/n_samples"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = DLModel(embedding_matrix, embedding_matrix.shape)\n","model.load_state_dict(torch.load('/kaggle/working/SelfAttentionModel_?epochs_200_tokens.pth'))\n","model.train()"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[],"source":["def customCollatetest(batch: list):\n","    # batch --> tuple (x,y)\n","    # x --> list of 4 lists\n","    x_batch, y_batch = zip(*batch)\n","    \n","    CC,UC = zip(*x_batch)\n","    \n","    CC_padded = []\n","    UC_padded = []\n","    \n","    for i  in range(len(x_batch)):\n","        \n","        CC_padded.append( list(CC[i]) + [0] * (CC_size - len(CC[i])) )\n","        UC_padded.append( list(UC[i]) + [0] * (UC_size - len(UC[i])) )\n","        \n","\n","    return  CC_padded,UC_padded, y_batch\n","    \n","\n","def evaluate(model, test_dataset,batch_size=5):\n","  \"\"\"\n","  This function takes a NER model and evaluates its performance (accuracy) on a test data\n","  Inputs:\n","  - model: a NER model\n","  - test_dataset: dataset of type NERDataset\n","  \"\"\"\n","  ########################### TODO: Replace the Nones in the following code ##########################\n","\n","  # (1) create the test data loader\n","  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, collate_fn=customCollatetest , shuffle=False)\n","\n","  total_acc_test = 0\n","  n_samples = 0\n","\n","  use_cuda = torch.cuda.is_available()\n","    \n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n","    # device=\"cpu\"\n","  if use_cuda:\n","        model = model.cuda()\n","  with torch.no_grad():\n","    false_neg = 0\n","    true_pos = 0\n","    false_pos = 0\n","    true_neg = 0\n","    for  CC,UC, y_batch in tqdm(test_dataloader):\n","        \n","            UC_padded_tensor = torch.tensor(UC, device=device)\n","            CC_padded_tensor = torch.tensor(CC, device=device)\n","\n","            y_batch_tensor = torch.tensor(y_batch,dtype = torch.float32,device=device)\n","#             y_batch_tensor = torch.where(y_batch_tensor == 0 , torch.tensor(-1).to(device), torch.tensor(1).to(device)).to(device)\n","            output = model.forward(CC_padded_tensor,UC_padded_tensor)\n","#             print(output)\n","            _,predicted=torch.max(output,dim=1)\n","\n","            for i,value in enumerate(predicted): #calc the fals negtive\n","                if value == 0 and  y_batch_tensor[i] == 1:\n","                  false_neg+=1\n","                elif value == 1 and  y_batch_tensor[i] == 1:\n","                  true_pos+=1\n","                elif value == 1 and  y_batch_tensor[i] == 0:\n","                  false_pos += 1\n","                else:\n","                  true_neg += 1\n","                \n","            n_true = (predicted==y_batch_tensor).sum().item()\n","\n","            n_samples += y_batch_tensor.size(0)\n","            total_acc_test += n_true\n","       \n","    # (6) calculate the over all accuracy\n","    acc_test =100*total_acc_test/n_samples\n","  ##################################################################################################\n","    print(\"true_pos = \", true_pos)\n","    print(\"false_neg = \",false_neg)\n","    print(\"false_pos = \",false_pos)\n","    print(\"true_neg = \", true_neg)\n","\n","    recall = true_pos / (true_pos+false_neg)\n","    precesion =  true_pos / (true_pos+false_pos)\n","    f1Score = (2*precesion*recall)/(precesion+recall)\n","    print(f'\\nTest Accuracy: {acc_test}')\n","    print(f'\\nTest Recall: {recall}') \n","    print(f'\\nTest precesion: {precesion}') \n","    print(f'\\nF1 Score: {f1Score}')"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 272/272 [01:13<00:00,  3.71it/s]"]},{"name":"stdout","output_type":"stream","text":["true_pos =  261\n","false_neg =  417\n","false_pos =  112\n","true_neg =  566\n","\n","Test Accuracy: 60.9882005899705\n","\n","Test Recall: 0.38495575221238937\n","\n","Test precesion: 0.6997319034852547\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["model.eval()\n","evaluate(model, dataset_test)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5073911,"sourceId":8501749,"sourceType":"datasetVersion"},{"datasetId":5084403,"sourceId":8516170,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}
