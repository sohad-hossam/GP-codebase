{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language.build_library is deprecated. Use the new bindings instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n",
      "c:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "from PreProcessor import *\n",
    "\n",
    "_PreProcessor = PreProcessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.10.1\n",
      "  Downloading scipy-1.10.1-cp39-cp39-win_amd64.whl.metadata (58 kB)\n",
      "     ---------------------------------------- 0.0/59.0 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/59.0 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 20.5/59.0 kB 217.9 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 41.0/59.0 kB 279.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 59.0/59.0 kB 310.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from scipy==1.10.1) (1.26.4)\n",
      "Downloading scipy-1.10.1-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "   ---------------------------------------- 0.0/42.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/42.5 MB 871.5 kB/s eta 0:00:49\n",
      "   ---------------------------------------- 0.1/42.5 MB 1.3 MB/s eta 0:00:33\n",
      "   ---------------------------------------- 0.2/42.5 MB 1.5 MB/s eta 0:00:29\n",
      "   ---------------------------------------- 0.3/42.5 MB 1.6 MB/s eta 0:00:27\n",
      "   ---------------------------------------- 0.4/42.5 MB 1.8 MB/s eta 0:00:25\n",
      "    --------------------------------------- 0.6/42.5 MB 2.2 MB/s eta 0:00:19\n",
      "    --------------------------------------- 0.7/42.5 MB 2.1 MB/s eta 0:00:21\n",
      "    --------------------------------------- 0.8/42.5 MB 2.1 MB/s eta 0:00:20\n",
      "    --------------------------------------- 0.9/42.5 MB 2.2 MB/s eta 0:00:20\n",
      "    --------------------------------------- 0.9/42.5 MB 2.1 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 1.1/42.5 MB 2.1 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 1.2/42.5 MB 2.1 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 1.5/42.5 MB 2.4 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 1.6/42.5 MB 2.5 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 1.8/42.5 MB 2.4 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 1.8/42.5 MB 2.3 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 1.8/42.5 MB 2.3 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 2.1/42.5 MB 2.5 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 2.2/42.5 MB 2.5 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 2.4/42.5 MB 2.5 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 2.5/42.5 MB 2.5 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 2.7/42.5 MB 2.6 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 2.7/42.5 MB 2.5 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 2.9/42.5 MB 2.6 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 3.1/42.5 MB 2.6 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 3.1/42.5 MB 2.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 3.3/42.5 MB 2.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 3.5/42.5 MB 2.7 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 3.6/42.5 MB 2.6 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 3.9/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 4.1/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 4.3/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 4.4/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 4.5/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 4.6/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 4.7/42.5 MB 2.7 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 4.9/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 5.0/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 5.2/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 5.3/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 5.5/42.5 MB 2.8 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 5.7/42.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 5.8/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 5.9/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 6.1/42.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 6.3/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 6.5/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 6.6/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 6.7/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 6.8/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 6.9/42.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 7.0/42.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 7.2/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 7.3/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 7.6/42.5 MB 2.9 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 7.7/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 7.7/42.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 7.9/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 8.0/42.5 MB 2.8 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 8.2/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 8.4/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   ------- -------------------------------- 8.4/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 8.6/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 8.8/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 8.8/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 8.9/42.5 MB 2.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 9.1/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 9.1/42.5 MB 2.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 9.3/42.5 MB 2.8 MB/s eta 0:00:12\n",
      "   -------- ------------------------------- 9.5/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 9.7/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 9.8/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.2/42.5 MB 2.9 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 10.4/42.5 MB 3.0 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 10.4/42.5 MB 3.0 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 10.5/42.5 MB 2.9 MB/s eta 0:00:11\n",
      "   --------- ------------------------------ 10.5/42.5 MB 2.9 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 10.7/42.5 MB 2.9 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 11.0/42.5 MB 3.0 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 11.2/42.5 MB 3.0 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 11.3/42.5 MB 3.1 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 11.6/42.5 MB 3.0 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 11.8/42.5 MB 3.0 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 12.0/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 12.0/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 12.1/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 12.2/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 12.5/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 12.7/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 12.9/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.1/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.2/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.4/42.5 MB 3.2 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.4/42.5 MB 3.2 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.6/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.7/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 13.7/42.5 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 13.8/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 14.0/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 14.2/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 14.3/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 14.5/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 14.6/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 14.8/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 14.9/42.5 MB 3.0 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 14.9/42.5 MB 3.1 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 15.1/42.5 MB 3.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 15.1/42.5 MB 3.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 15.1/42.5 MB 3.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 15.1/42.5 MB 3.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 15.1/42.5 MB 3.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 15.1/42.5 MB 3.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 15.1/42.5 MB 3.0 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 16.5/42.5 MB 3.1 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 16.6/42.5 MB 3.1 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 16.7/42.5 MB 3.1 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 16.9/42.5 MB 3.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 17.0/42.5 MB 3.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 17.2/42.5 MB 3.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 17.3/42.5 MB 3.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 17.5/42.5 MB 3.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 17.7/42.5 MB 3.1 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 17.9/42.5 MB 3.1 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 18.0/42.5 MB 3.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 18.2/42.5 MB 3.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 18.4/42.5 MB 3.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 18.5/42.5 MB 3.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 18.6/42.5 MB 3.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 18.8/42.5 MB 3.2 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 18.9/42.5 MB 3.1 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 19.1/42.5 MB 3.2 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 19.4/42.5 MB 3.3 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 19.5/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 19.7/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 19.9/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 20.1/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 20.3/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 20.5/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 20.6/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 20.7/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 20.8/42.5 MB 3.4 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 20.9/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 21.2/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 21.4/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 21.7/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 21.8/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 22.0/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 22.1/42.5 MB 3.3 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 22.3/42.5 MB 3.4 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 22.5/42.5 MB 3.4 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 22.7/42.5 MB 3.4 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 22.9/42.5 MB 3.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 23.0/42.5 MB 3.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 23.1/42.5 MB 3.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 23.3/42.5 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 23.5/42.5 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 23.6/42.5 MB 3.4 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 23.7/42.5 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 23.8/42.5 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 23.9/42.5 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 24.0/42.5 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 24.4/42.5 MB 3.4 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 24.7/42.5 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 24.9/42.5 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 25.0/42.5 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 25.1/42.5 MB 3.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 25.2/42.5 MB 3.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 25.4/42.5 MB 3.9 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 25.5/42.5 MB 3.8 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 25.8/42.5 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.0/42.5 MB 3.7 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.1/42.5 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.1/42.5 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.2/42.5 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.4/42.5 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.4/42.5 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.4/42.5 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.4/42.5 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.4/42.5 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 26.5/42.5 MB 3.1 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 26.8/42.5 MB 3.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 27.2/42.5 MB 3.2 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 27.9/42.5 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 27.9/42.5 MB 3.4 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 28.1/42.5 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 28.1/42.5 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 28.1/42.5 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 28.1/42.5 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 28.1/42.5 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 28.1/42.5 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 28.3/42.5 MB 3.1 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 28.4/42.5 MB 3.1 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 28.7/42.5 MB 3.1 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 28.9/42.5 MB 3.2 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 29.6/42.5 MB 3.3 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 29.7/42.5 MB 3.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 29.8/42.5 MB 3.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 29.8/42.5 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 30.0/42.5 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 30.0/42.5 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 30.0/42.5 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 30.6/42.5 MB 3.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 30.7/42.5 MB 3.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 30.8/42.5 MB 3.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.9/42.5 MB 3.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.9/42.5 MB 3.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.9/42.5 MB 3.2 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 31.0/42.5 MB 3.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 31.1/42.5 MB 3.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 31.3/42.5 MB 3.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 31.5/42.5 MB 3.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 31.5/42.5 MB 3.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 31.7/42.5 MB 3.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 31.7/42.5 MB 3.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 31.8/42.5 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 31.9/42.5 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.0/42.5 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.1/42.5 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.1/42.5 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.3/42.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.3/42.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.4/42.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.5/42.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.6/42.5 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.7/42.5 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.7/42.5 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.8/42.5 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.9/42.5 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.9/42.5 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 32.9/42.5 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.1/42.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.1/42.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.2/42.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.3/42.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.3/42.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.3/42.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.3/42.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.3/42.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.6/42.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.6/42.5 MB 2.5 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.6/42.5 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.7/42.5 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.8/42.5 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.8/42.5 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.8/42.5 MB 2.4 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 33.9/42.5 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.0/42.5 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.0/42.5 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.1/42.5 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.1/42.5 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.2/42.5 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.3/42.5 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.3/42.5 MB 2.3 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.4/42.5 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.5/42.5 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.6/42.5 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.7/42.5 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.7/42.5 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.8/42.5 MB 2.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.8/42.5 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.8/42.5 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 34.9/42.5 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 35.0/42.5 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 35.0/42.5 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 35.0/42.5 MB 2.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.1/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.2/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.2/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.2/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.3/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.3/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.4/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.5/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.6/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.6/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.7/42.5 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.8/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 35.9/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 36.0/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 36.1/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 36.2/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 36.3/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 36.4/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 36.4/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 36.5/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 36.6/42.5 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 36.7/42.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 36.8/42.5 MB 2.0 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 36.9/42.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 37.0/42.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 37.0/42.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 37.1/42.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 37.2/42.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.3/42.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.4/42.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.5/42.5 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.5/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.6/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.7/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.8/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 37.9/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 38.0/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 38.1/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 38.2/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.3/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.4/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.5/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.6/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.7/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.7/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.8/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.8/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.9/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 38.9/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 39.0/42.5 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 39.0/42.5 MB 1.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 39.1/42.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 39.2/42.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 39.2/42.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 39.2/42.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 39.3/42.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.4/42.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.4/42.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.4/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.5/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.6/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.6/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.6/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.7/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.7/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.8/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.8/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.8/42.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 39.8/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 40.0/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 40.0/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 40.0/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 40.0/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 40.2/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 40.2/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 40.3/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 40.4/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 40.5/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 40.5/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 40.7/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 40.7/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 40.8/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 40.8/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 40.9/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 41.0/42.5 MB 1.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 41.1/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.2/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.3/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.4/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.5/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.6/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.7/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.8/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.9/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.0/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.1/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.1/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.3/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.3/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.4/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.5/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.5/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.5/42.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 42.5/42.5 MB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.0\n",
      "    Uninstalling scipy-1.13.0:\n",
      "      Successfully uninstalled scipy-1.13.0\n",
      "Successfully installed scipy-1.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\basse\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\~cipy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\basse\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\~cipy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "# !pip install scipy==1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names_train,function_segments_train = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/train_CC\",'CC','train')\n",
    "descriptions_train, summaries_train = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/train_UC\",'UC','train')\n",
    "\n",
    "function_names_test,function_segments_test = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/test_CC\",'CC','test')\n",
    "descriptions_test, summaries_test = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/test_UC\",'UC','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PreProcessor.setUpUnknown(function_names_train,function_segments_train,'CC')\n",
    "_PreProcessor.setUpUnknown(descriptions_train,summaries_train,'UC')\n",
    "\n",
    "_PreProcessor.setUpUnknown(function_names_test,function_segments_test,'CC')\n",
    "_PreProcessor.setUpUnknown(descriptions_test,summaries_test,'UC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/function_names_train.pkl', 'wb') as f:\n",
    "       pickle.dump(function_names_train, f)\n",
    "with open('./pickles/DeepLearning/function_segments_train.pkl', 'wb') as f:\n",
    "        pickle.dump(function_segments_train, f)\n",
    "with open('./pickles/DeepLearning/descriptions_train.pkl', 'wb') as f:\n",
    "        pickle.dump(descriptions_train, f)\n",
    "with open('./pickles/DeepLearning/summaries_train.pkl', 'wb') as f:\n",
    "        pickle.dump(summaries_train, f)\n",
    "\n",
    "with open('./pickles/DeepLearning/Vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(_PreProcessor.Vocab, f)\n",
    "\n",
    "with open('./pickles/DeepLearning/function_names_test.pkl', 'wb') as f:\n",
    "       pickle.dump(function_names_test, f)\n",
    "with open('./pickles/DeepLearning/function_segments_test.pkl', 'wb') as f:\n",
    "        pickle.dump(function_segments_test, f)\n",
    "with open('./pickles/DeepLearning/descriptions_test.pkl', 'wb') as f:\n",
    "        pickle.dump(descriptions_test, f)\n",
    "with open('./pickles/DeepLearning/summaries_test.pkl', 'wb') as f:\n",
    "        pickle.dump(summaries_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names_train = np.load('./pickles/DeepLearning/function_names_train.pkl',allow_pickle=True)\n",
    "function_segments_train = np.load('./pickles/DeepLearning/function_segments_train.pkl',allow_pickle=True)\n",
    "descriptions_train = np.load('./pickles/DeepLearning/descriptions_train.pkl',allow_pickle=True)\n",
    "summaries_train = np.load('./pickles/DeepLearning/summaries_train.pkl',allow_pickle=True)\n",
    "vocab = np.load('./pickles/DeepLearning/Vocab.pkl',allow_pickle=True)\n",
    "function_names_test = np.load('./pickles/DeepLearning/function_names_test.pkl',allow_pickle=True)\n",
    "function_segments_test = np.load('./pickles/DeepLearning/function_segments_test.pkl',allow_pickle=True)\n",
    "descriptions_test = np.load('./pickles/DeepLearning/descriptions_test.pkl',allow_pickle=True)\n",
    "summaries_test = np.load('./pickles/DeepLearning/summaries_test.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['convert'], ['convert', 'plan'], ['get', 'id'], ['convert', 'node'], ['check', 'valid', 'capabl'], ['valid', 'aggreg', 'function', 'evalu'], ['alia', 'command'], ['check', 'share', 'sourc', 'command'], ['updat', 'group', 'name'], ['correct', 'project', 'intern', 'tabl'], ['prepar', 'add'], ['set', 'rout', 'name'], ['rewrit', 'multi', 'sourc', 'command'], ['visit', 'node']]\n",
      "['convert', 'plan']\n",
      "convert\n"
     ]
    }
   ],
   "source": [
    "print(function_names_train[0])\n",
    "print(function_names_train[0][1])\n",
    "print(function_names_train[0][1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dataset to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences\u001b[38;5;241m=\u001b[39m\u001b[43mfunction_names_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m(), vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_word2vec_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "# model = Word2Vec(sentences=function_names_train, vector_size=200, window=5, min_count=1, workers=4)\n",
    "# model.save(\"my_word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PreProcessor.vocabToIndex()\n",
    "_PreProcessor.dataSetToIndex(function_names_train,function_segments_train,'CC')\n",
    "_PreProcessor.dataSetToIndex(descriptions_train,summaries_train,'UC')\n",
    "\n",
    "_PreProcessor.dataSetToIndex(function_names_test,function_segments_test,'CC')\n",
    "_PreProcessor.dataSetToIndex(descriptions_test,summaries_test,'UC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, labels = _PreProcessor.setUpLabels(function_names_train,function_segments_train,descriptions_train,summaries_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/x_train.pkl', 'wb') as f:\n",
    "       pickle.dump(x_train, f)\n",
    "with open('./pickles/DeepLearning/labels.pkl', 'wb') as f:\n",
    "        pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./pickles/DeepLearning/x_train.pkl',allow_pickle=True)\n",
    "labels = np.load('./pickles/DeepLearning/labels.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[tensor([1]), tensor([1, 2]), tensor([3, 4]), tensor([1, 5]), tensor([6, 7, 8]), tensor([ 7,  9, 10, 11]), tensor([12, 13]), tensor([ 6, 14, 15, 13]), tensor([16, 17, 18]), tensor([19, 20, 21, 22]), tensor([23, 24]), tensor([25, 26, 18]), tensor([27, 28, 15, 13]), tensor([221,   5])], [tensor([29, 30, 31, 31, 29, 29, 30, 31, 32, 33, 34, 30, 31, 32,  1,  2, 35, 36,\n",
      "        35, 33, 34,  1,  2, 35,  5, 36, 35,  5, 37,  5, 36,  5, 36,  5,  1,  2,\n",
      "         2,  5, 38, 36, 39, 40, 41, 39, 40, 41, 39, 40, 41, 39, 29, 30, 31, 32,\n",
      "        36,  2, 36,  5, 33, 34, 30, 31, 32, 33, 34, 37,  2, 36,  2, 37,  2, 36,\n",
      "         5, 36,  2, 14, 13, 42]), tensor([ 1, 43,  5,  2, 35, 37,  5,  1,  5,  1,  5,  2,  5,  1,  5, 44, 45,  2,\n",
      "         5,  3, 46, 47,  1,  2,  2,  5,  3, 48, 46, 37,  5, 49, 50,  1,  5,  1,\n",
      "         5, 51, 52,  5, 53, 54, 55, 49, 50,  3, 56, 49, 50, 49, 50,  3, 56, 57,\n",
      "        58,  1,  2, 59, 56,  2,  5, 46,  5,  2,  5,  3, 56, 37,  5, 46,  1,  2,\n",
      "        46,  5,  2,  5,  3, 60,  5, 61, 60, 62, 49, 50, 63,  5, 46,  5,  3, 64,\n",
      "        65, 66,  2,  5,  3, 64, 65, 66, 46,  5,  3, 60,  5, 61, 60, 62, 46,  5,\n",
      "        64, 65, 67, 37,  5, 68, 46, 46,  3, 56, 68, 46, 49, 50, 24, 46, 68, 46,\n",
      "        49, 50, 24, 46, 46, 69, 35, 70,  5,  1,  5]), tensor([ 4, 71, 49]), tensor([37,  5, 36,  ...,  5, 36,  5]), tensor([ 80,  81,  18,  78,   3,  81,  18,  79,   4,  93,  94,   7,  81,  18,\n",
      "         15,   8,  93,  93,  94,  95,   8,  81,  18,  39, 320,  93, 320,  39,\n",
      "         93,   3,  15,  64,   8, 321,  39,  40,  41,  39,  40, 103, 104,  38,\n",
      "        320,  40, 103, 180,  57,  40, 103, 104,  38,  81,  18]), tensor([  3,  10, 322,   3,  10, 322,   3, 179, 113, 323, 179,  40,  41,  39,\n",
      "         40, 103, 104,  38,  40, 103, 180,  57,  40, 103, 104,  38,   3,  10,\n",
      "        322,   3,  81,  18]), tensor([ 13,  13,  13, 324,  12,  17,  79,   4,   8, 180,  96,  17, 325,  79,\n",
      "          4,  78,  93,  94,   8, 180,  96,   8,  40, 326, 327,  79,   4,  78,\n",
      "         93,  94,  12, 290,  79,   4,   8, 180,  96,   8,  40, 108, 105,  79,\n",
      "          4,  78,  93,  94,   8, 180,  96,   8,  40, 326, 327,  79,   4,  78,\n",
      "         93,  94,  12,  71, 175,  12,  71,  12,  17,  12, 290,  15,  91, 328,\n",
      "         13,   3,  15,  91, 328,  12,  17, 236, 237, 238, 236, 239,   3, 329,\n",
      "        330, 239,   3, 236,  79, 237, 238,  79, 236,   3,  79,   5,   3,  79,\n",
      "         18,  80,  15,  18,  79,   3,  15,  18, 331,  91, 332,  15,  18, 182,\n",
      "        332, 328,   3, 331,  91,  15,  18,   3, 328,  67, 325, 332, 332,  67,\n",
      "        325, 175,  12, 275, 239,   3,  12, 275,  86,  86,  86, 203, 175,   3,\n",
      "         86,  13,  86, 120,  80, 333,  17,  35,  80,  80, 334, 217,  86, 269,\n",
      "         86, 269, 333, 269,   3, 105,   3,  17,  72, 333,  17,  24, 269,   3,\n",
      "        105,   3,  17,  72,   3,  18, 175, 333,  17, 333,  17,  13, 224, 175,\n",
      "        175,  40,  78,  39, 208,  38, 102,  39,  40, 103, 104,  38, 208,  38,\n",
      "        335,  39,   3, 320,  40,  41,  39,  40,  41,  39,   3, 320,  13]), tensor([178,  70, 228, 202, 336,  81,  13,  80,  80,  79,  18,   5,   3,  79,\n",
      "         18,  13, 337,   5,   3,  13,  14,  81, 338,  21,  15, 339, 340, 341,\n",
      "         61, 205,  79, 141,  79,  18, 341,  61, 205, 342,  79, 141,  79,  18,\n",
      "        343,  78, 344, 343,  79,   3,  18, 141,  79,  18, 337,  40,  40,  40,\n",
      "         40, 337,  40,   3, 217,  40,   3,  90, 153,   5,  14,  13,   3, 337,\n",
      "        345,  51, 346, 347, 348, 229, 204, 162, 349,  54, 345, 144,   5,   3,\n",
      "         50,   5,   3,  50,   3,  60,   5,  61,  60, 125,   5,   3,  50,   3,\n",
      "         64,  65, 125, 127, 125, 127,  60, 148,  22,   5,   3,  50,   3, 301,\n",
      "         46,   5, 345,  45,   5,   5,   3,  50,  14,  13, 252, 337,   5, 345,\n",
      "          5,  65, 350, 159, 351,  14, 153,  65,   5,  65,   4,  14,   4,   3,\n",
      "        352,   5,  65,  14,  47,  65,  65, 350, 159, 351,  14, 153,  65,  65,\n",
      "          4,  14,   4,   3, 352,  65,  14,  47,  65,  14,  47, 345,  65,  14,\n",
      "         47,   5,  65,  65]), tensor([ 80,  17,  18,   5,   3,  17,  49,   3,  18, 299,   3,  17,  72,  18,\n",
      "         17,  18,  75,  72,  72, 299,   3,  20,  72,  72,  17,  72,  17,  72,\n",
      "         17,  18]), tensor([  5,   3,  17, 182,   5,  17,  72,  17,   5,   3,  17,  49, 341,  61,\n",
      "        205,  79, 141,  78,   3,  81,  18,  78,   3,  79,   4,  17,   3,  78,\n",
      "          4, 341,  61, 205, 342,  79, 141,  78,   3,  81,  18,  78,   3,  79,\n",
      "          4,  17,   3,  78,   4,   5,  20,  72,   5,   3,  64,   5,  61,  65,\n",
      "        115, 106,  75,  72, 353, 290, 251, 180, 251,  75,  17,  17,  78,  20,\n",
      "         72, 141, 353, 290,   5,   5,  64,   5,  61,  65, 115, 106, 353, 290,\n",
      "          5,   3,  50,   5,   3,  50,   3,  60,   5,  61,  60,  20,  50, 112,\n",
      "         20,  19, 115, 106, 354,   5,  20,   5, 107,  20,   5,   3,   4, 107,\n",
      "        108,  72,  20,  72,   5, 153,   5,  23,  24,   5,   5,   5,  64,   5,\n",
      "         61,  65, 115, 106,  20,  72, 107,  24,  46,   5, 107]), tensor([115,  75,   2,   5, 106,   5,   3,  64,   5,  61,  65, 115, 106,  36,\n",
      "          5,  75, 106, 201, 355, 347, 355,   5, 356, 347,   5,   3,  64,   5,\n",
      "         61,  65, 357, 356,  36,   5, 355,   5, 356, 355,   5, 356, 347, 355,\n",
      "          5, 182, 347,   5,   3,  64,   5,  61,  65, 357, 182,  36,   5, 355,\n",
      "          5, 182, 355,   5, 182, 347, 355, 134, 153, 356, 347,   5,   3,  64,\n",
      "          5,  61,  65, 357, 134, 356,  36,   5, 355, 134, 153, 356, 355, 134,\n",
      "        153, 356, 347, 355, 134, 125, 201, 347,   5,   3,  64,   5,  61,  65,\n",
      "        357, 134, 125, 201,  36,   5, 355, 134, 125, 201, 355, 134, 125, 201,\n",
      "        347, 355, 125, 201, 347,   5,   3,  64,   5,  61,  65, 357, 125, 201,\n",
      "         36,   5, 355, 125, 201, 355, 125, 201,  36,   5]), tensor([358, 241, 184,  18,  77,  79,   4,   5,   3,  64,   5,  61,  65,  79,\n",
      "          4,  79,   4,  79,   4, 343,  78,   4,  13, 177, 155,  79,   4, 177,\n",
      "        155,  13,   3,  79,   4,  13, 178,  13, 359, 122,  17,  72,  17,  17,\n",
      "        203, 175,   3,  17,  13,  45,  17,  72,  17,  17,  49,  79,   4,  78,\n",
      "          3,  79,   4,  17,   3,  78,   4,  80, 360,  18,  78,   3,  81,  18,\n",
      "         79,   4, 153,   5,  79,  18, 360,  18, 153,   5,  79,   4,  79,   4,\n",
      "        153,   5, 361,  77,   5,   3,  64,  65, 361,  15,  40,  78,  39,  40,\n",
      "         41,  39,  40, 103, 104,  38,  40, 103, 180,  57,  40, 103, 104,  38]), tensor([105, 163,  13, 177, 155, 177, 155, 223, 177, 155,  13, 332, 351, 362,\n",
      "        223,   3, 351,  87, 362,  49, 332, 351, 362, 362,  49, 362,   3, 351,\n",
      "         60, 332, 351,  78,  28,  15,  75, 362,   3,  78,   4, 105,  15, 362,\n",
      "          3, 105, 362, 271, 362,  67,  15,  61,  61,  15, 163,  15,  13,  73,\n",
      "         73, 223,  73,  13, 223,   3,  76, 182,  75,  72, 250, 223,   3,  76,\n",
      "          3,  77,  78,   4, 250,   3,  78,   4,  78,  28,  15,  75,  78,   4,\n",
      "        105,  15, 105, 223,   3,  87,   3, 223,   3,  76, 271, 223,   3,  87,\n",
      "        271, 163,  15,  13, 114,  13,  90,  90, 198,  90, 114,  13,  13,   3,\n",
      "         90, 244,  90, 244,  90, 363, 244,  90, 363,   3, 130, 105,  75,  72,\n",
      "         75,  72, 364,  75,  72, 363,   3, 130, 105,  78,  28,  15,  75, 364,\n",
      "          3,  78,   4, 174, 175, 365,  61, 363,   3, 132, 105, 163, 163, 141,\n",
      "        363,   3, 132, 105,  61,  61, 163, 363,   3, 132, 105, 163]), tensor([ 93, 175,   7, 223, 105, 223, 224, 175,  93, 175, 225, 221,   5, 223])], tensor([1191, 4974, 1538,  380,  412, 1756, 4595, 8583,  181,  516, 3304,  182,\n",
      "        4098,  380, 4976, 2254,  478, 4977,  380, 3541, 2091, 4978, 1308,  380,\n",
      "         495, 4976, 2254,  358,  380,  456,  462, 8583, 1914,  380, 3541, 8583,\n",
      "        2234,  354, 2123, 4981, 3304, 1598, 1583,  909,  170, 2235,  456,  375,\n",
      "         827, 2123,   39]), tensor([ 170,  456, 2123,  526,  380, 3572])]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracibilityLinkDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train):\n",
    "\n",
    "        # # getting the max number of funtions in a file along all files\n",
    "        # #getting the longest name of a function along all functions in all files\n",
    "        # max_number_function = max(len(_) for _ in x_train_function_names)\n",
    "        # max_function_name_len = max(len(function) for file in x_train_function_names for function in file)\n",
    "\n",
    "        # #getting the longest segment of a function along all functions in all files\n",
    "        # max_function_segment_len = max(len(function) for file in x_train_function_segments for function in file)\n",
    "\n",
    "        # x_train_function_names_padded = [file + [[]] * (max_number_function - len(file)) for file in x_train_function_names]\n",
    "        # x_train_function_names_padded = [function + [vocab_size+1] * (max_number_function - len(function)) for file in x_train_function_names_padded for function in file]\n",
    "        \n",
    "        # # convert it to a tensor to be used in model\n",
    "        # self.tensor_x_train_function_names_padded = torch.tensor(x_train_function_names_padded)\n",
    "        \n",
    "        self.tensor_x_train = x_train\n",
    "        self.tensor_y_train = torch.tensor(y_train)\n",
    "\n",
    "\n",
    "    # len and get item are very important --> used by dataloader\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_x_train[idx], self.tensor_y_train[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TracibilityLinkDataset(x_train, labels)\n",
    "torch.save(dataset_train,'./dataset/teiid_dataset/DeepLearningDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = torch.load('./dataset/teiid_dataset/DeepLearningDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModel(nn.Module):\n",
    "\n",
    "  def __init__(self,embedding_matrix : np.array, embedding_dim : tuple, classes: int = 2):\n",
    "      super(DLModel, self).__init__()\n",
    "\n",
    "      # first embedding layer: input size --> vocab size, output size --> feature vector of vocan size(one hot encoding), weigths --> one hot encoding\n",
    "      # matrix (vocab * vocab)\n",
    "      self.one_hot_embedding = nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim = embedding_dim[1]/2, _weight = embedding_matrix)\n",
    "      \n",
    "      # self-attention layer --> num_heads = 1, query, key, value will all be the input\n",
    "      self.attention_layer = nn.MultiheadAttention(embed_dim=embedding_dim[1], num_heads=1, batch_first = True)\n",
    "\n",
    "      # softmax ?\n",
    "      # self.linear = nn.Linear(2 * hidden_size_layer_chars, classes, bias=True)\n",
    "\n",
    "  def forward(self,  function_names, function_segments, descriptions, summaries ):\n",
    "    \n",
    "    function_names_embeddings = [self.one_hot_embedding(fn) for fn in function_names]\n",
    "    function_segments_embeddings = [self.one_hot_embedding(fs) for fs in function_segments]\n",
    "\n",
    "    descriptions_embedding  = self.one_hot_embedding(descriptions)\n",
    "    summaries_embedding = self.one_hot_embedding(summaries)\n",
    "    \n",
    "    print(len(function_names_embeddings))\n",
    "    print(len(function_segments_embeddings))\n",
    "    print(len(descriptions_embedding))\n",
    "    print(len(summaries_embedding))\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # feature_vector = self.embedding_words(sentences) # sentences * words *253\n",
    "\n",
    "    # feature_vector = feature_vector.to(dtype=torch.float)\n",
    "    # lstm_output = self.lstm_layer_words(feature_vector) # sentences * words *200\n",
    "    # #print(lstm_output[0].shape)  \n",
    "    # # looping over lstm_output to remove unwanted feature vectors\n",
    "    # no_of_sentences, length_of_sentence, feature_vector_size = lstm_output[0].shape\n",
    "    # lstm_output_words= lstm_output[0]\n",
    "    # lstm_output_words_2d=lstm_output_words.reshape(-1,lstm_output_words.shape[2])\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "    # embedding_char_output_4d=self.embedding_char(words)  # sentences * words * chars * one hot encoding for each char\n",
    "    # #words * chars * one hot encoding for each char\n",
    "    # embedding_char_output_3d=embedding_char_output_4d.reshape(-1,embedding_char_output_4d.shape[2],embedding_char_output_4d.shape[3])\n",
    "    # #print(embedding_char_output_3d.shape)\n",
    "\n",
    "    # #print(lstm_output_words_2d[0]) # words*hot encoding vector\n",
    "    # embedding_char_output_3d_concatenated=np.zeros((embedding_char_output_3d.shape[0],embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[2]+lstm_output_words_2d.shape[1]))\n",
    "\n",
    "    # # for word in range(len(lstm_output_words_2d)):\n",
    "    # #        for chars in range(len(embedding_char_output_3d[word])):\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "    # lstm_output_words_2d=lstm_output_words_2d.detach().numpy()\n",
    "\n",
    "    # lstm_output_words_2d = np.repeat(lstm_output_words_2d, repeats=embedding_char_output_3d.shape[1], axis=0)\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "\n",
    "    # lstm_output_words_2d=lstm_output_words_2d.reshape(lstm_output_words_2d.shape[0]//embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[1],lstm_output_words_2d.shape[1])\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "\n",
    "    # embedding_char_output_3d_concatenated = np.append(embedding_char_output_3d.detach().numpy(),lstm_output_words_2d,axis=2)  # Append along rows\n",
    "\n",
    "    # embedding_char_output_3d_concatenated = torch.tensor(embedding_char_output_3d_concatenated)\n",
    "    # #print(embedding_char_output_3d_concatenated.shape)\n",
    "\n",
    "    # embedding_char_output_3d_concatenated = embedding_char_output_3d_concatenated.to(dtype=torch.float)\n",
    "    # #lstm_output_intermediate = self.intermediate_lstm_layer(embedding_char_output_3d_concatenated)\n",
    "    # #print(embedding_char_output_3d_concatenated.shape)\n",
    "    # lstm_output_char = self.lstm_layer_char(embedding_char_output_3d_concatenated)\n",
    "    # final_output = self.linear(lstm_output_char[0])  # words * 13 char * 15 classs\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def customCollate(batch: list):\n",
    "    # batch --> tuple (x,y)\n",
    "    # x --> list of 4 lists \n",
    "    x_batch, y_batch = zip(*batch)\n",
    "\n",
    "    function_names, function_segments,descriptions , summaries= zip(*x_batch)\n",
    "\n",
    "    function_names_padded = []  # Integer tensor\n",
    "    function_segments_padded = []\n",
    "    \n",
    "    for index,(file_name,file_segment) in enumerate(zip(function_names,function_segments)):\n",
    "        function_names_padded.append(pad_sequence(file_name, batch_first=True, padding_value = 0))\n",
    "        function_segments_padded.append(pad_sequence(file_segment, batch_first=True, padding_value = 0) ) \n",
    "\n",
    "   \n",
    "    descriptions_padded = pad_sequence(descriptions, batch_first=True, padding_value=0)\n",
    "    summaries_padded = pad_sequence(summaries, batch_first=True, padding_value = 0)\n",
    "    \n",
    "    return function_names_padded,function_segments_padded,descriptions_padded, summaries_padded,y_batch\n",
    "\n",
    "def train(model, train_dataset, batch_size=10, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, collate_fn=customCollate)\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "\n",
    "    # (3) create the optimizer (Adam)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        n_samples=0\n",
    "        batch_num = 0\n",
    "        for function_names_padded,function_segments_padded,descriptions_padded, summaries_padded, y_batch in train_dataloader:\n",
    "\n",
    "    #         # (6) do the forward pass\n",
    "\n",
    "              output = model.forward(function_names_padded,function_segments_padded,descriptions_padded, summaries_padded)\n",
    "              batch_num+=1\n",
    "              print(batch_num)\n",
    "            \n",
    "    #         # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "    #         # label: batch_size * 253 * 13\n",
    "    #         # output: 25300 word * 13 char * 15 \n",
    "    #         # k2eny fket el array l words msh sentence of words\n",
    "\n",
    "    #         # batch_loss = criterion(output.view(-1, output.size(2)), train_label.view(-1))\n",
    "\n",
    "    #         # # (8) append the batch loss to the total_loss_train\n",
    "\n",
    "    #         # total_loss_train += batch_loss\n",
    "            \n",
    "    #         # # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "    #         # _,predicted=torch.max(output,2)  #512 * 104  for every word in every sentence we choose one tag form the seventen tag \n",
    "    #         # acc=(predicted==train_label.view(-1, train_label.shape[2])).sum().item()\n",
    "    #         # n_samples += train_label.size(0)*train_label.size(1)*train_label.size(2)\n",
    "    #         # total_acc_train += acc\n",
    "    #         # print(100*total_acc_train/n_samples)\n",
    "\n",
    "    #         # # (10) zero your gradients\n",
    "    #         # optimizer.zero_grad()\n",
    "\n",
    "    #         # # (11) do the backward pass\n",
    "    #         # batch_loss.backward()\n",
    "    #         # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "    #         # # (12) update the weights with your optimizer\n",
    "    #         # optimizer.step()\n",
    "    #     # epoch loss\n",
    "    #     # epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    #     # # (13) calculate the accuracy\n",
    "    #     # epoch_acc =100*total_acc_train/n_samples\n",
    "\n",
    "    #     # print(\n",
    "    #     #     f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "    #     #     | Train Accuracy: {epoch_acc}\\n')\n",
    "    #     break\n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Shape of weight does not match num_embeddings and embedding_dim",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m one_hot_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mlen\u001b[39m(vocab\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDLModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mone_hot_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m, in \u001b[0;36mDLModel.__init__\u001b[1;34m(self, embedding_matrix, embedding_dim, classes)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28msuper\u001b[39m(DLModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# first embedding layer: input size --> vocab size, output size --> feature vector of vocan size(one hot encoding), weigths --> one hot encoding\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# matrix (vocab * vocab)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_hot_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# self-attention layer --> num_heads = 1, query, key, value will all be the input\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMultiheadAttention(embed_dim\u001b[38;5;241m=\u001b[39membedding_dim[\u001b[38;5;241m1\u001b[39m], num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_first \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:147\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(_weight, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse \u001b[38;5;241m=\u001b[39m sparse\n",
      "\u001b[1;31mAssertionError\u001b[0m: Shape of weight does not match num_embeddings and embedding_dim"
     ]
    }
   ],
   "source": [
    "one_hot_embedding = torch.eye(len(vocab.keys()) + 2)\n",
    "model = DLModel(one_hot_embedding, one_hot_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8584, 8584])\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4511750400 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print(len(labels), len(x_train))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     39\u001b[0m     batch_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m function_names_padded,function_segments_padded,descriptions_padded, summaries_padded, y_batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#         # (6) do the forward pass\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m           output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_names_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfunction_segments_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdescriptions_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummaries_padded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m           batch_num\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     46\u001b[0m           \u001b[38;5;28mprint\u001b[39m(batch_num)\n",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m, in \u001b[0;36mDLModel.forward\u001b[1;34m(self, function_names, function_segments, descriptions, summaries)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,  function_names, function_segments, descriptions, summaries ):\n\u001b[0;32m     18\u001b[0m   function_names_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_hot_embedding(fn) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m function_names]\n\u001b[1;32m---> 19\u001b[0m   function_segments_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_hot_embedding(fs) \u001b[38;5;28;01mfor\u001b[39;00m fs \u001b[38;5;129;01min\u001b[39;00m function_segments]\n\u001b[0;32m     21\u001b[0m   descriptions_embedding  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_hot_embedding(descriptions)\n\u001b[0;32m     22\u001b[0m   summaries_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_hot_embedding(summaries)\n",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,  function_names, function_segments, descriptions, summaries ):\n\u001b[0;32m     18\u001b[0m   function_names_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_hot_embedding(fn) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m function_names]\n\u001b[1;32m---> 19\u001b[0m   function_segments_embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m fs \u001b[38;5;129;01min\u001b[39;00m function_segments]\n\u001b[0;32m     21\u001b[0m   descriptions_embedding  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_hot_embedding(descriptions)\n\u001b[0;32m     22\u001b[0m   summaries_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mone_hot_embedding(summaries)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 4511750400 bytes."
     ]
    }
   ],
   "source": [
    "train(model, dataset_train)\n",
    "# print(len(labels), len(x_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
