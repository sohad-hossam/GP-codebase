{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language.build_library is deprecated. Use the new bindings instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n",
      "c:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "from PreProcessor import *\n",
    "\n",
    "_PreProcessor = PreProcessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names_train,function_segments_train = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/train_CC\",'CC','train')\n",
    "descriptions_train, summaries_train = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/train_UC\",'UC','train')\n",
    "function_names_test,function_segments_test = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/test_CC\",'CC','test')\n",
    "descriptions_test, summaries_test = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/test_UC\",'UC','test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PreProcessor.setUpUnknown(function_names_train,function_segments_train,'CC')\n",
    "_PreProcessor.setUpUnknown(descriptions_train,summaries_train,'UC')\n",
    "\n",
    "_PreProcessor.setUpUnknown(function_names_test,function_segments_test,'CC')\n",
    "_PreProcessor.setUpUnknown(descriptions_test,summaries_test,'UC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/function_names_train.pkl', 'wb') as f:\n",
    "       pickle.dump(function_names_train, f)\n",
    "with open('./pickles/DeepLearning/function_segments_train.pkl', 'wb') as f:\n",
    "        pickle.dump(function_segments_train, f)\n",
    "with open('./pickles/DeepLearning/descriptions_train.pkl', 'wb') as f:\n",
    "        pickle.dump(descriptions_train, f)\n",
    "with open('./pickles/DeepLearning/summaries_train.pkl', 'wb') as f:\n",
    "        pickle.dump(summaries_train, f)\n",
    "\n",
    "with open('./pickles/DeepLearning/Vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(_PreProcessor.Vocab, f)\n",
    "\n",
    "with open('./pickles/DeepLearning/function_names_test.pkl', 'wb') as f:\n",
    "       pickle.dump(function_names_test, f)\n",
    "with open('./pickles/DeepLearning/function_segments_test.pkl', 'wb') as f:\n",
    "        pickle.dump(function_segments_test, f)\n",
    "with open('./pickles/DeepLearning/descriptions_test.pkl', 'wb') as f:\n",
    "        pickle.dump(descriptions_test, f)\n",
    "with open('./pickles/DeepLearning/summaries_test.pkl', 'wb') as f:\n",
    "        pickle.dump(summaries_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names_train = np.load('./pickles/DeepLearning/function_names_train.pkl',allow_pickle=True)\n",
    "function_segments_train = np.load('./pickles/DeepLearning/function_segments_train.pkl',allow_pickle=True)\n",
    "descriptions_train = np.load('./pickles/DeepLearning/descriptions_train.pkl',allow_pickle=True)\n",
    "summaries_train = np.load('./pickles/DeepLearning/summaries_train.pkl',allow_pickle=True)\n",
    "vocab = np.load('./pickles/DeepLearning/Vocab.pkl',allow_pickle=True)\n",
    "function_names_test = np.load('./pickles/DeepLearning/function_names_test.pkl',allow_pickle=True)\n",
    "function_segments_test = np.load('./pickles/DeepLearning/function_segments_test.pkl',allow_pickle=True)\n",
    "descriptions_test = np.load('./pickles/DeepLearning/descriptions_test.pkl',allow_pickle=True)\n",
    "summaries_test = np.load('./pickles/DeepLearning/summaries_test.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC_word2vec = _PreProcessor.word2VecProcessor(function_names_train, function_segments_train, 'CC')\n",
    "UC_word2vec = _PreProcessor.word2VecProcessor(summaries_train, descriptions_train, 'UC')\n",
    "# print(CC_word2vec)\n",
    "word2vec_model = Word2Vec(sentences = CC_word2vec + UC_word2vec, vector_size=200, window=5, min_count=1, workers=4, epochs=10)\n",
    "word2vec_model.save(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n",
    "\n",
    "#word2vec_model.build_vocab(UC_word2vec, update=True)\n",
    "#word2vec_model.train(UC_word2vec, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
    "\n",
    "# word2vec_model.save(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n",
    "word2vec_model = Word2Vec.load(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n",
    "\n",
    "word2vec_vocab = word2vec_model.wv.index_to_key\n",
    "\n",
    "# Initialize an empty embedding matrix\n",
    "embedding_matrix = np.zeros((len(word2vec_vocab) + 1, word2vec_model.vector_size))\n",
    "\n",
    "# Fill the embedding matrix with the embeddings of each word\n",
    "for i, word in enumerate(word2vec_vocab):\n",
    "    embedding_vector = word2vec_model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i + 1] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6245\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dataset to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PreProcessor.vocabToIndex(word2vec_vocab)\n",
    "_PreProcessor.dataSetToIndex(function_names_train,function_segments_train)\n",
    "_PreProcessor.dataSetToIndex(descriptions_train,summaries_train)\n",
    "\n",
    "_PreProcessor.dataSetToIndex(function_names_test,function_segments_test)\n",
    "_PreProcessor.dataSetToIndex(descriptions_test,summaries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x_train, labels \u001b[38;5;241m=\u001b[39m \u001b[43m_PreProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetUpLabels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_names_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfunction_segments_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdescriptions_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43msummaries_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\College Items\\GP code\\GP-codebase\\PreProcessor.py:530\u001b[0m, in \u001b[0;36mPreProcessor.setUpLabels\u001b[1;34m(self, function_names_train, function_segments_train, descriptions_train, summaries_train)\u001b[0m\n\u001b[0;32m    528\u001b[0m index_UC \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(DataSet_train\u001b[38;5;241m.\u001b[39mloc[row, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUC\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    529\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(DataSet_train\u001b[38;5;241m.\u001b[39mloc[row, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunction_names_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_code\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(function_segments_train[index_code]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(descriptions_train[index_UC]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(summaries_train[index_UC]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    531\u001b[0m     Features\u001b[38;5;241m.\u001b[39mappend([function_names_train[index_code],function_segments_train[index_code],descriptions_train[index_UC],summaries_train[index_UC]])\n\u001b[0;32m    532\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, labels = _PreProcessor.setUpLabels(function_names_train,function_segments_train,descriptions_train,summaries_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/x_train.pkl', 'wb') as f:\n",
    "       pickle.dump(x_train, f)\n",
    "with open('./pickles/DeepLearning/labels.pkl', 'wb') as f:\n",
    "        pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./pickles/DeepLearning/x_train.pkl',allow_pickle=True)\n",
    "labels = np.load('./pickles/DeepLearning/labels.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracibilityLinkDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train):\n",
    "\n",
    "        # # getting the max number of funtions in a file along all files\n",
    "        # #getting the longest name of a function along all functions in all files\n",
    "        # max_number_function = max(len(_) for _ in x_train_function_names)\n",
    "        # max_function_name_len = max(len(function) for file in x_train_function_names for function in file)\n",
    "\n",
    "        # #getting the longest segment of a function along all functions in all files\n",
    "        # max_function_segment_len = max(len(function) for file in x_train_function_segments for function in file)\n",
    "\n",
    "        # x_train_function_names_padded = [file + [[]] * (max_number_function - len(file)) for file in x_train_function_names]\n",
    "        # x_train_function_names_padded = [function + [vocab_size+1] * (max_number_function - len(function)) for file in x_train_function_names_padded for function in file]\n",
    "        \n",
    "        # # convert it to a tensor to be used in model\n",
    "        # self.tensor_x_train_function_names_padded = torch.tensor(x_train_function_names_padded)\n",
    "        \n",
    "        self.tensor_x_train = x_train\n",
    "        self.tensor_y_train = torch.tensor(y_train)\n",
    "\n",
    "\n",
    "    # len and get item are very important --> used by dataloader\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_x_train[idx], self.tensor_y_train[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TracibilityLinkDataset(x_train, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dataset_train,'./dataset/teiid_dataset/DeepLearningDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = torch.load('./dataset/teiid_dataset/DeepLearningDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "  def __init__(self, input_dim):\n",
    "    super(SelfAttention, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.query = nn.Linear(input_dim, input_dim) # [batch_size, seq_length, input_dim]\n",
    "    self.key = nn.Linear(input_dim, input_dim) # [batch_size, seq_length, input_dim]\n",
    "    self.value = nn.Linear(input_dim, input_dim)\n",
    "    self.softmax = nn.Softmax(dim = 2)\n",
    "   \n",
    "  def forward(self, x): # x.shape (batch_size, seq_length, input_dim)\n",
    "    queries = self.query(x)\n",
    "    keys = self.key(x)\n",
    "    values = self.value(x)\n",
    "\n",
    "    score = torch.bmm(queries, keys.transpose(1, 2))/(self.input_dim**0.5)\n",
    "    print(torch.cuda.memory_reserved()/1024**3)\n",
    "\n",
    "    attention = self.softmax(score)\n",
    "    weighted = torch.bmm(attention, values)\n",
    "    return weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModel(nn.Module):\n",
    "\n",
    "  def __init__(self,embedding_matrix : np.array, embedding_dim : tuple, classes: int = 2):\n",
    "      super(DLModel, self).__init__()\n",
    "\n",
    "      # first embedding layer: input size --> vocab size, output size --> feature vector of vocan size(one hot encoding), weigths --> one hot encoding\n",
    "      # matrix (vocab * vocab)\n",
    "      self.embedding = nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim = embedding_dim[1], _weight = torch.tensor(embedding_matrix))\n",
    "      \n",
    "      # self-attention layer --> num_heads = 1, query, key, value will all be the input\n",
    "      # self.attention_layer_UC = nn.MultiheadAttention(embed_dim=embedding_dim[1], num_heads=1, batch_first = True)\n",
    "      # self.attention_layer_CC = nn.MultiheadAttention(embed_dim=embedding_dim[1], num_heads=1, batch_first = True)\n",
    "      self.attention_layer_UC = SelfAttention(embedding_dim[1])\n",
    "      self.attention_layer_CC = SelfAttention(embedding_dim[1])\n",
    "\n",
    "\n",
    "      # softmax ?\n",
    "      # self.linear = nn.Linear(2 * hidden_size_layer_chars, classes, bias=True)\n",
    "\n",
    "  def forward(self, function_names, function_segments, descriptions, summaries ):\n",
    "\n",
    "    # print(torch.cuda.memory_reserved()/1024**3)\n",
    "\n",
    "    function_names_embeddings  = self.embedding(function_names)\n",
    "\n",
    "    function_names_attention = self.attention_layer_CC(function_names_embeddings.float())\n",
    "    # del function_names_embeddings\n",
    "    # del function_names_attention\n",
    "\n",
    "    function_segments_embeddings = self.embedding(function_segments)\n",
    "    # print(function_segments_embeddings.element_size()*function_segments_embeddings.numel()/1024**3)\n",
    "\n",
    "\n",
    "    function_segments_attention = self.attention_layer_CC(function_segments_embeddings.float())\n",
    "    # del function_segments_embeddings\n",
    "    # del function_segments_attention\n",
    "\n",
    "\n",
    "\n",
    "    descriptions_embedding  = self.embedding(descriptions)\n",
    "    descriptions_attention = self.attention_layer_UC(descriptions_embedding.float())\n",
    "    # del descriptions_embedding\n",
    "    # del descriptions_attention\n",
    "\n",
    "\n",
    "    summaries_embedding = self.embedding(summaries)\n",
    "\n",
    "    summaries_attention = self.attention_layer_UC(summaries_embedding.float())\n",
    "    # del summaries_embedding\n",
    "    # del summaries_attention\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # print(function_names_attention[0].shape)\n",
    "    # print(function_segments_attention[0].shape)\n",
    "\n",
    "    # print(descriptions_attention[0].shape)\n",
    "    # print(summaries_attention[0].shape)\n",
    "\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # feature_vector = self.embedding_words(sentences) # sentences * words *253\n",
    "\n",
    "    # feature_vector = feature_vector.to(dtype=torch.float)\n",
    "    # lstm_output = self.lstm_layer_words(feature_vector) # sentences * words *200\n",
    "    # #print(lstm_output[0].shape)  \n",
    "    # # looping over lstm_output to remove unwanted feature vectors\n",
    "    # no_of_sentences, length_of_sentence, feature_vector_size = lstm_output[0].shape\n",
    "    # lstm_output_words= lstm_output[0]\n",
    "    # lstm_output_words_2d=lstm_output_words.reshape(-1,lstm_output_words.shape[2])\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "    # embedding_char_output_4d=self.embedding_char(words)  # sentences * words * chars * one hot encoding for each char\n",
    "    # #words * chars * one hot encoding for each char\n",
    "    # embedding_char_output_3d=embedding_char_output_4d.reshape(-1,embedding_char_output_4d.shape[2],embedding_char_output_4d.shape[3])\n",
    "    # #print(embedding_char_output_3d.shape)\n",
    "\n",
    "    # #print(lstm_output_words_2d[0]) # words*hot encoding vector\n",
    "    # embedding_char_output_3d_concatenated=np.zeros((embedding_char_output_3d.shape[0],embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[2]+lstm_output_words_2d.shape[1]))\n",
    "\n",
    "    # # for word in range(len(lstm_output_words_2d)):\n",
    "    # #        for chars in range(len(embedding_char_output_3d[word])):\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "    # lstm_output_words_2d=lstm_output_words_2d.detach().numpy()\n",
    "\n",
    "    # lstm_output_words_2d = np.repeat(lstm_output_words_2d, repeats=embedding_char_output_3d.shape[1], axis=0)\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "\n",
    "    # lstm_output_words_2d=lstm_output_words_2d.reshape(lstm_output_words_2d.shape[0]//embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[1],lstm_output_words_2d.shape[1])\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "\n",
    "    # embedding_char_output_3d_concatenated = np.append(embedding_char_output_3d.detach().numpy(),lstm_output_words_2d,axis=2)  # Append along rows\n",
    "\n",
    "    # embedding_char_output_3d_concatenated = torch.tensor(embedding_char_output_3d_concatenated)\n",
    "    # #print(embedding_char_output_3d_concatenated.shape)\n",
    "\n",
    "    # embedding_char_output_3d_concatenated = embedding_char_output_3d_concatenated.to(dtype=torch.float)\n",
    "    # #lstm_output_intermediate = self.intermediate_lstm_layer(embedding_char_output_3d_concatenated)\n",
    "    # #print(embedding_char_output_3d_concatenated.shape)\n",
    "    # lstm_output_char = self.lstm_layer_char(embedding_char_output_3d_concatenated)\n",
    "    # final_output = self.linear(lstm_output_char[0])  # words * 13 char * 15 classs\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "def customCollate(batch: list):\n",
    "    # batch --> tuple (x,y)\n",
    "    # x --> list of 4 lists\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "\n",
    "    function_names, function_segments, descriptions , summaries = zip(*x_batch)\n",
    "    # function_names_padded = list(copy.deepcopy(function_names))\n",
    "    # function_segments_padded = list(copy.deepcopy(function_segments))\n",
    "\n",
    "    # function_names -> batch_size * num_of_functions * len(function)\n",
    "    # max_function_names_2nd_dim = max(len(file) for file in function_names)\n",
    "    # function_names_padded = [file + [[]] * (max_function_names_2nd_dim - len(file)) for file in function_names]\n",
    "\n",
    "    # for i, file in enumerate(function_names_padded): \n",
    "    #     max_function_names = max(len(function) for function in file)\n",
    "    #     for j, function in enumerate(file):\n",
    "    #         function_names_padded[i][j].extend([0] * (max_function_names - len(function)))\n",
    "    #     function_names_padded[i] = torch.tensor(function_names_padded[i], dtype=torch.int64)\n",
    "\n",
    "    # for i, file in enumerate(function_names_padded): \n",
    "    #         for j, function in enumerate(file):\n",
    "    #             function_names_padded[i][j].extend([0] * (max_function_names_3rd_dim - len(function)))\n",
    "    #             # function_names_padded[i][j] = torch.tensor(function_names_padded[i][j])\\n\",\n",
    "    #         #function_names_padded[i] = torch.tensor(function_names_padded[i],dtype=torch.int64)\n",
    "    \n",
    "    \n",
    "    # for i, file in enumerate(function_segments): \n",
    "    #     max_function_segments = max(len(function) for function in file)\n",
    "    #     for j, function in enumerate(file):\n",
    "    #         function_segments_padded[i][j].extend([0] * (max_function_segments - len(function)))\n",
    "    #     function_segments_padded[i] = torch.tensor(function_segments_padded[i], dtype=torch.int64, device=device)\n",
    "\n",
    "\n",
    "    function_names_padded = pad_sequence(function_names, batch_first=True, padding_value=0)\n",
    "    function_segments_padded = pad_sequence(function_segments, batch_first=True, padding_value = 0)\n",
    "    descriptions_padded = pad_sequence(descriptions, batch_first=True, padding_value=0)\n",
    "    summaries_padded = pad_sequence(summaries, batch_first=True, padding_value = 0)\n",
    "\n",
    "    \n",
    "    return  function_names_padded, function_segments_padded, descriptions_padded, summaries_padded, y_batch\n",
    "\n",
    "def train(model, train_dataset, batch_size=10, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, collate_fn=customCollate)\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n",
    "    # device= \"cpu\"\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    # (3) create the optimizer (Adam)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        n_samples = 0\n",
    "        for function_names_padded, function_segments_padded, descriptions_padded, summaries_padded, y_batch in train_dataloader:\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "              function_names_padded_tensor = torch.tensor(function_names_padded, device=device)\n",
    "              function_segments_padded_tensor = torch.tensor(function_segments_padded, device=device)\n",
    "              descriptions_padded_tensor = torch.tensor(descriptions_padded, device=device)\n",
    "              fsummaries_padded_tensor = torch.tensor(summaries_padded, device=device)\n",
    "              output = model.forward(function_names_padded_tensor, function_segments_padded_tensor, descriptions_padded_tensor, fsummaries_padded_tensor)\n",
    "\n",
    "              \n",
    "            \n",
    "    #         # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "    #         # label: batch_size * 253 * 13\n",
    "    #         # output: 25300 word * 13 char * 15 \n",
    "    #         # k2eny fket el array l words msh sentence of words\n",
    "\n",
    "    #         # batch_loss = criterion(output.view(-1, output.size(2)), train_label.view(-1))\n",
    "\n",
    "    #         # # (8) append the batch loss to the total_loss_train\n",
    "\n",
    "    #         # total_loss_train += batch_loss\n",
    "            \n",
    "    #         # # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "    #         # _,predicted=torch.max(output,2)  #512 * 104  for every word in every sentence we choose one tag form the seventen tag \n",
    "    #         # acc=(predicted==train_label.view(-1, train_label.shape[2])).sum().item()\n",
    "    #         # n_samples += train_label.size(0)*train_label.size(1)*train_label.size(2)\n",
    "    #         # total_acc_train += acc\n",
    "    #         # print(100*total_acc_train/n_samples)\n",
    "\n",
    "    #         # # (10) zero your gradients\n",
    "    #         # optimizer.zero_grad()\n",
    "\n",
    "    #         # # (11) do the backward pass\n",
    "    #         # batch_loss.backward()\n",
    "    #         # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "    #         # # (12) update the weights with your optimizer\n",
    "    #         # optimizer.step()\n",
    "    #     # epoch loss\n",
    "    #     # epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    #     # # (13) calculate the accuracy\n",
    "    #     # epoch_acc =100*total_acc_train/n_samples\n",
    "\n",
    "    #     # print(\n",
    "    #     #     f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "    #     #     | Train Accuracy: {epoch_acc}\\n')\n",
    "        print(\"finished\")\n",
    "        break\n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basse\\AppData\\Local\\Temp\\ipykernel_9668\\4147167403.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  function_names_padded_tensor = torch.tensor(function_names_padded, device=device)\n",
      "C:\\Users\\basse\\AppData\\Local\\Temp\\ipykernel_9668\\4147167403.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  function_segments_padded_tensor = torch.tensor(function_segments_padded, device=device)\n",
      "C:\\Users\\basse\\AppData\\Local\\Temp\\ipykernel_9668\\4147167403.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  descriptions_padded_tensor = torch.tensor(descriptions_padded, device=device)\n",
      "C:\\Users\\basse\\AppData\\Local\\Temp\\ipykernel_9668\\4147167403.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fsummaries_padded_tensor = torch.tensor(summaries_padded, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0390625\n",
      "9.171875\n",
      "9.173828125\n",
      "9.173828125\n",
      "9.173828125\n",
      "10.025390625\n",
      "10.02734375\n",
      "10.02734375\n",
      "10.02734375\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 13.37 GiB (GPU 0; 4.00 GiB total capacity; 8.56 GiB already allocated; 0 bytes free; 9.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m DLModel(embedding_matrix, embedding_matrix\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# print(len(labels), len(x_train))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[65], line 78\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     76\u001b[0m           descriptions_padded_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(descriptions_padded, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     77\u001b[0m           fsummaries_padded_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(summaries_padded, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 78\u001b[0m           output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_names_padded_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_segments_padded_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptions_padded_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfsummaries_padded_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m#         # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#         # label: batch_size * 253 * 13\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m#         # output: 25300 word * 13 char * 15 \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m#     #     f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m#     #     | Train Accuracy: {epoch_acc}\\n')\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[64], line 34\u001b[0m, in \u001b[0;36mDLModel.forward\u001b[1;34m(self, function_names, function_segments, descriptions, summaries)\u001b[0m\n\u001b[0;32m     30\u001b[0m function_segments_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(function_segments)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# print(function_segments_embeddings.element_size()*function_segments_embeddings.numel()/1024**3)\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m function_segments_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_layer_CC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_segments_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# del function_segments_embeddings\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# del function_segments_attention\u001b[39;00m\n\u001b[0;32m     40\u001b[0m descriptions_embedding  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(descriptions)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[63], line 15\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(x)\n\u001b[0;32m     13\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x)\n\u001b[1;32m---> 15\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_reserved()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     18\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(score)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 13.37 GiB (GPU 0; 4.00 GiB total capacity; 8.56 GiB already allocated; 0 bytes free; 9.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model = DLModel(embedding_matrix, embedding_matrix.shape)\n",
    "train(model, dataset_train)\n",
    "# print(len(labels), len(x_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
