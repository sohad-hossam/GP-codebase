{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from PreProcessor import *\n",
    "\n",
    "_PreProcessor = PreProcessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names_train,function_segments_train = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/train_CC\",'CC','train')\n",
    "descriptions_train, summaries_train = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/train_UC\",'UC','train')\n",
    "function_names_test,function_segments_test = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/test_CC\",'CC','test')\n",
    "descriptions_test, summaries_test = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/test_UC\",'UC','test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PreProcessor.setUpUnknown(function_names_train,function_segments_train,'CC')\n",
    "_PreProcessor.setUpUnknown(descriptions_train,summaries_train,'UC')\n",
    "\n",
    "_PreProcessor.setUpUnknown(function_names_test,function_segments_test,'CC')\n",
    "_PreProcessor.setUpUnknown(descriptions_test,summaries_test,'UC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/function_names_train.pkl', 'wb') as f:\n",
    "       pickle.dump(function_names_train, f)\n",
    "with open('./pickles/DeepLearning/function_segments_train.pkl', 'wb') as f:\n",
    "        pickle.dump(function_segments_train, f)\n",
    "with open('./pickles/DeepLearning/descriptions_train.pkl', 'wb') as f:\n",
    "        pickle.dump(descriptions_train, f)\n",
    "with open('./pickles/DeepLearning/summaries_train.pkl', 'wb') as f:\n",
    "        pickle.dump(summaries_train, f)\n",
    "\n",
    "with open('./pickles/DeepLearning/Vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(_PreProcessor.Vocab, f)\n",
    "\n",
    "with open('./pickles/DeepLearning/function_names_test.pkl', 'wb') as f:\n",
    "       pickle.dump(function_names_test, f)\n",
    "with open('./pickles/DeepLearning/function_segments_test.pkl', 'wb') as f:\n",
    "        pickle.dump(function_segments_test, f)\n",
    "with open('./pickles/DeepLearning/descriptions_test.pkl', 'wb') as f:\n",
    "        pickle.dump(descriptions_test, f)\n",
    "with open('./pickles/DeepLearning/summaries_test.pkl', 'wb') as f:\n",
    "        pickle.dump(summaries_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names_train = np.load('./pickles/DeepLearning/function_names_train.pkl',allow_pickle=True)\n",
    "function_segments_train = np.load('./pickles/DeepLearning/function_segments_train.pkl',allow_pickle=True)\n",
    "descriptions_train = np.load('./pickles/DeepLearning/descriptions_train.pkl',allow_pickle=True)\n",
    "summaries_train = np.load('./pickles/DeepLearning/summaries_train.pkl',allow_pickle=True)\n",
    "vocab = np.load('./pickles/DeepLearning/Vocab.pkl',allow_pickle=True)\n",
    "function_names_test = np.load('./pickles/DeepLearning/function_names_test.pkl',allow_pickle=True)\n",
    "function_segments_test = np.load('./pickles/DeepLearning/function_segments_test.pkl',allow_pickle=True)\n",
    "descriptions_test = np.load('./pickles/DeepLearning/descriptions_test.pkl',allow_pickle=True)\n",
    "summaries_test = np.load('./pickles/DeepLearning/summaries_test.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(function_names_train[0])\n",
    "# print(function_names_train[0][1])\n",
    "# print(function_names_train[0][1][0])\n",
    "# new_files=[]\n",
    "# function_segments_train = [[['hi my name is', 'lol'], ['bassant']], [[''], ['bassant2']]]\n",
    "# function_names_train =[[['title1', 'title2'], ['title2']], [['title3'], ['title4']]]\n",
    "\n",
    "# for file_name, file_seg in zip(function_names_train, function_segments_train):\n",
    "#     print(file_name)\n",
    "#     print(file_seg)\n",
    "    \n",
    "#     print('--------------------')\n",
    "# descriptions_train, summaries_train, function_names_train, function_segments_train\n",
    "# for file_name, file_seg in zip(function_names_train, function_segments_train):\n",
    "#     temp = []\n",
    "#     for name in file_name:\n",
    "#         temp.extend(name)\n",
    "#     temp.append(\"</s>\")\n",
    "#     for seg in file_seg:\n",
    "#         temp.extend(seg)\n",
    "#     new_files.append(temp)\n",
    "# print(new_files[0])\n",
    "\n",
    "CC_word2vec = _PreProcessor.word2VecProcessor(function_names_train, function_segments_train, 'CC')\n",
    "UC_word2vec = _PreProcessor.word2VecProcessor(summaries_train, descriptions_train, 'UC')\n",
    "\n",
    "word2vec_model = Word2Vec(sentences = CC_word2vec+UC_word2vec, vector_size=200, window=5, min_count=1, workers=4, epochs=10)\n",
    "word2vec_model.save(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n",
    "\n",
    "#word2vec_model.build_vocab(UC_word2vec, update=True)\n",
    "#word2vec_model.train(UC_word2vec, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
    "\n",
    "# word2vec_model.save(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'e': 1, 't': 2, 'a': 3, 'r': 4, 'n': 5, 'o': 6, 'i': 7, 's': 8, 'c': 9, 'l': 10, 'p': 11, 'd': 12, 'u': 13, 'm': 14, 'g': 15, 'b': 16, 'v': 17, 'f': 18, 'h': 19, 'x': 20, 'y': 21, 'q': 22, 'j': 23, 'k': 24, 'w': 25, '_': 26, 'z': 27, '<': 28, '/': 29, '>': 30, '*': 31, '`': 32, '─': 33, 'ー': 34, '└': 35, 'ド': 36, 'ボ': 37, '├': 38, 'キ': 39, '語': 40, '日': 41, '本': 42, 'の': 43, '你': 44, '好': 45, '“': 46, 'и': 47, 'а': 48, 'к': 49, 'б': 50, '”': 51, 'ш': 52, 'о': 53, '名': 54, '平': 55, '仮': 56}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = word2vec_model.wv.key_to_index\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert debug analysi record record debug debug analysi record println non nl analysi record println convert plan tree process tree non nl convert plan tree node process tree node relat node process node process node convert plan plan node teiid process except queri planner except queri planner except queri planner except debug analysi record println process plan process node non nl analysi record println non nl relat plan process plan relat plan process node process plan share command clearconvert plan convert current node plan tree relat node convert node convert node plan node convert node assert true plan node get child count convert plan plan node get first child relat node next parent convert node convert node may head node go end chain next parent get children next parent next parent get children call convert plan recurs children plan node child node plan node get children relat node child convert plan child node plan node get type node constant type op next parent union node child node get properti info oper plan node get properti info oper child node get type node constant type op child node properti info use relat node grand child child get children grand child next parent add child grand child next parent add child child root tree top node convert nodeget id id gener nextconvert node relat node process node node get type node constant type project group symbol group group symbol node get properti node constant info group group insert insert insert node get first child get properti info virtual command element symbol element insert get variabl object group id group get metadata id object model id metadata get model id group id string model name metadata get full name model id metadata virtual group group id metadata temporari tabl group id insert plan execut node ipen insert plan execut node get id metadata processor plan plan processor plan node get first child get properti info processor plan assert plan ipen processor plan plan ipen refer insert get valu process node ipen project node pinod project node get id pinod group group pinod element element pinod model name model name pinod constraint criteria node get properti info constraint pinod sourc hint sourc hint node get properti info sourc hint node properti info upsert pinod upsert true process node pinod sourc capabl cap cap finder find capabl model name cap support capabl capabl insert pinod mode org teiid queri processor relat project node mode cap support capabl capabl batch updat pinod mode org teiid queri processor relat project node mode batch pinod mode org teiid queri processor relat project node mode singl pinod transact support transact support cap get sourc properti capabl transact support queri metadata except teiid compon except queri plugin event teiid express symbol node get properti node constant info project col project node pnode project node get id pnode select symbol symbol process node pnode node properti info window function window function project node wfpn window function project node get id partial project window function may alreadi push check express filter express express child symbol node get first child get properti node constant info output col express ex symbol ex symbol get express ex child symbol contain ex filter add ex window function window function rule assign output element get window function filter window function empti todo check select window function express output element express window function collect project express express singl element symbol express node get first child get properti info output col output element add singl element symbol wfpn element output element wfpn init pnode add child wfpn window function wf window function valid aggreg function evalu wf get function node constant type join join type jtype join type node get properti node constant info join type join strategi type stype join strategi type node get properti node constant info join strategi join node jnode join node get id jnode join type jtype jnode left distinct node properti node constant info left distinct jnode right distinct node properti node constant info right distinct join crit node get properti node constant info join criteria string dep valu sourc string node get properti node constant info depend valu sourc sort option left sort sort option node get properti node constant info sort left stype join strategi type merg stype join strategi type enhanc sort merg join strategi mj strategi stype equal join strategi type enhanc sort enhanc sort merg join strategi esmj strategi enhanc sort merg join strategi left sort sort option node get properti node constant info sort right esmj strategi semi dep node properti info semi dep mj strategi esmj strategi mj strategi merg join strategi left sort sort option node get properti node constant info sort right fals node properti info singl match assert true jtype join type join left outer mj strategi singl match true jnode join strategi mj strategi left express node get properti node constant info left express right express node get properti node constant info right express jnode join express left express right express join crit node get properti node constant info non equi join criteria stype join strategi type nest tabl nest tabl join strategi ntj strategi nest tabl join strategi jnode join strategi ntj strategi symbol refer symbol node get properti info right nest refer ntj strategi right refer nest loop join strategi nlj strategi nest loop join strategi jnode join strategi nlj strategi criteria join crit criteria combin criteria join crit jnode join criteria join crit process node jnode jnode depend valu sourc dep valu sourc node constant type access processor plan plan processor plan node get properti node constant info processor plan plan plan execut node pe node criteria crit criteria node get properti node constant info procedur criteria crit refer node get properti node constant info procedur input default node get properti node constant info procedur default pe node depend procedur execut node get id crit refer default pe node plan execut node get id pe node processor plan plan process node pe node access node node command command command node get properti node constant info atom request object model id node get properti node constant info model id model id todo ideal want handl partial result differ ad node sourc warn easi say user need take step static capabl check valid capabl model id metadata cap finder evaluat visitor ev node properti node constant info depend command store procedur refer node get properti node constant info procedur input default node get properti node constant info procedur default criteria crit criteria node get properti node constant info procedur criteria depend procedur access node dep access node depend procedur access node get id crit refer default process node dep access node node dep access node creat depend access node depend access node dep access node depend access node get id model id dep access node pushdown capabl util support capabl depend join model id metadata cap finder dep access node max size capabl util get max criteria size model id metadata cap finder dep access node max predic capabl util get max depend predic model id metadata cap finder dep access node use bind capabl util support capabl depend join bind model id metadata cap finder todo allow translat drive properti simplist check whether queri complex execut queri queri queri command queri get group queri get get claus size queri get get claus get unari claus queri get dep access node complex queri true check see index least one depend set group symbol group group symbol queri get get group found fals criteria crit criteria separ criteria queri get criteria crit depend criteria depend criteria dsc depend criteria crit calcul cost util get key use element collector visitor get element dsc get express true group metadata found true found dep access node complex queri true process node dep access node node dep access node node evalu express true creat access node node access node get id process node node special handl system tabl current cannot perform project command queri process node correct project intern tabl node node queri metadata except err teiid compon except queri plugin event teiid err rout name node node command eval fals command insert insert insert insert command insert get queri express insert queri express queri command alia command node insert get queri express model id insert get valu size express ex express insert get valu get criteria capabl valid visitor push languag object ex model id metadata cap finder analysi record replac express symbol let rewrit know replac insert get valu express symbol ex eval true command queri command command alia command node command model id ev evaluat visitor need evalu visitor model id metadata cap finder eval model id capabl sensit check need eval string model name metadata get full name model id sourc capabl cap cap finder find capabl model name criteria capabl valid visitor cap visitor criteria capabl valid visitor model id metadata cap finder cap cap visitor check evalu fals deep pre order navig nav deep pre order navig ev void visit node org teiid queri sql languag object obj cap visitor valid obj express obj accept visitor cap visitor super visit node obj command accept visitor nav cap visitor valid non support construct push eval ev evalu possibl evalu level process deep pre order navig visit command ev node evalu express ev requir evalu evalu level process eval node command command model id string full name metadata get full name model id sourc capabl cap cap finder find capabl full name node transact support transact support cap get sourc properti capabl transact support group symbol plan node sub plan group symbol plan node node get properti info sub plan make sens allow multisourc affect elev access node node get model id metadata multi sourc node get model id vdb meta data vdb context get vdb node evalu express true forc rewrit node element node get properti node constant info output col node properti info multi sourc express ex rewrit multi sourc command node get command node connector bind express ex node multi sourc true string sourc name string node get properti info sourc name node connector bind express constant sourc name sub plan node evalu node minim project command check valid share node ev ev get determin level compar determin instruct determinist command result cachabl check share sourc command node node sub plan queri command qc queri command command qc get qc queri command sub plan size group symbol relat plan plan group symbol relat plan entri group symbol plan node entri sub plan entri relat plan sub plan convert entri get valu element symbol elem resolv util resolv element group entri get key metadata sub plan output element elem plan put entri get key sub plan queri command queri command queri command entri get key elem qc get add queri command node sub plan plan node constant type select criteria crit criteria node get properti node constant info select criteria node collect properti info output col late optim creat depend join subqueri introduc criteria output element todo cleaner logic current expect run join implement rerun assign output element seem excess node properti info output col node get first child get properti info output col select node selnod select node get id selnod criteria crit parent sourc selnod project express express node get properti node constant info project col chanc posit refer pre evalu postit fals refer ref refer collector visitor get refer crit ref posit postit true selnod evalu express postit process node selnod node constant type sort node constant type dup remov node get type node constant type dup remov process node dup remov node get id sort node sort node sort node get id order order order node get properti node constant info sort order order sort node sort element order get order item node properti node constant info dup remov sort node mode mode dup remov sort process node sort node node constant type group group node gnode group node get id gnode rollup node properti info rollup symbol group symbol node get properti node constant info symbol gnode output map group gnode remov duplic node properti node constant info dup remov express col node get properti node constant info group col order order order node get properti info sort order order col express expr express express ex col expr add symbol get express ex order order rule choos join strategi creat express symbol express expr express seen express col size order get order item size order item order item order get order item get express ex symbol get express order item get symbol seen add ex ex element symbol ex group get map express element symbol ex order item symbol express symbol expr ex non nl order add variabl express symbol expr col get order asc non nl order gnode order order get order item express ex group group get valu express node get first child get properti node constant info project col ex aggreg symbol valid aggreg function evalu aggreg symbol ex process node gnode node constant type sourc object sourc node get properti node constant info tabl function sourc xml tabl xml tabl xt xml tabl sourc handl project filter rather repeat path analysi per plan basi updat group name node xt express integ element relat node creat lookup xt get project symbol col node get properti node constant info output col project index relat node get project index element col xml column filter column xml column project index length col project index filter column add xt get column get col xt get queri express use document project filter column analysi record process node xml helper get instanc xml tabl node get id xt filter column sourc object tabl object tabl ot object tabl sourc object tabl node otn object tabl node get id handl project filter rather repeat path analysi per plan basi updat group name node ot express integ element relat node creat lookup ot get project symbol express col express node get properti node constant info output col project index relat node get project index element col object column filter column object column project index length col project index filter column add ot get column get col otn project column filter column otn tabl ot process node otn sourc text tabl text tabl node ttn text tabl node get id text tabl tt text tabl sourc updat group name node tt ttn tabl tt process node ttn sourc tabl tabl node atn tabl node get id tabl tabl sourc updat group name node atn tabl process node atn symbol symbol symbol node get properti node constant info symbol symbol plan node child node get last child child get type node constant type project child get type node constant type select updat project col base upon origin output child properti node constant info project col child get properti node constant info output col child get type node constant type op child get properti info oper oper union child properti node constant info output col node get properti node constant info output col cannot directli updat child properti child get convert join creat project instead initi purpos impact perform project node node project node get id node select symbol express child get properti node constant info output col prepar add node node node constant type op oper op oper node get properti node constant info oper use node get properti node constant info use valu op oper union relat node union node union node get id use process node union node dup remov node properti node constant info dup remov dup remov process node dup remov node get id sort node node sort node get id node mode mode dup remov sort process node node union node element node get properti node constant info output col process node add child union node join node join join node get id join join strategi merg join strategi sort option sort distinct sort option sort distinct true push sort enforc order sinc null equal left express node get first child get properti node constant info output col right express node get last child get properti node constant info output col join join type op oper except join type join anti semi join type join semi join join express left express right express process node join node constant type tupl limit express row limit express node get properti node constant info max tupl limit express offset express node get properti node constant info offset tupl count limit node ln limit node get id row limit offset ln implicit node properti info implicit limit process node ln node constant type process node node get id queri planner except queri plugin event teiid queri plugin util gs queri plugin event teiid node constant get node type string node get type process node process node prepar add node process node process nodecheck valid capabl string full name metadata get full name model id cap finder valid full name sourc capabl cap cap finder find capabl full name except caus cap caus except cap get sourc properti capabl invalid except queri planner except queri plugin event teiid caus queri plugin util gs queri plugin event teiid full namevalid aggreg function evalu get function descriptor get function descriptor get pushdown push must pushdown queri planner except queri plugin event teiid queri plugin util gs queri plugin event teiid get function descriptor get full namealia command command command command clone alia group model id capabl util support group alias model id metadata cap finder capabl util support capabl queri inlin view model id metadata cap finder alia column model id capabl util support capabl queri select express model id metadata cap finder capabl util support capabl queri inlin view model id metadata cap finder alia gener visitor alia gener alia group alia column sourc hint sh command get sourc hint sh alia group vdb meta data vdb context get dqp work context get vdb model meta data model vdb get model node get model name string sourc name model get sourc name specif hint sp sourc name size sp sh get specif hint sourc name get sh use alias sp sp use alias visitor alia map context get alia map refer refer refer collector visitor get refer command refer empti string correl group tree string string insensit order refer ref refer ref correl ref get express get group symbol correl group add ref get express get group symbol get name visitor correl group correl group command accept visitor visitor queri metadata except err teiid compon except queri plugin event teiid err teiid runtim except get caus queri planner except queri planner except get caus commandcheck share sourc command creat top level key avoid full command string string model name node get model name command cmd node get command share full scan intern sourc wast buffer core constant system model equal model name core constant system admin model equal model name temp metadata adapt temp model get name equal model name cmd queri queri queri queri cmd queri get order queri get criteria access node share command get cmd later may reus number time requir special handl clean end later fals node get parent node get parent get type node constant type join node get parent get properti info join strategi join strategi type nest tabl node get parent get last child node later true node node get parent share command put cmd node later node info regist request paramet share access info node info id share id get increment node info share count info info regist request paramet share access info info id share id get increment info share count info share count later info share count node info infoupdat group name string group name node get group next get name tt get group symbol name group name element symbol symbol tt get project symbol symbol group symbol group symbol group namecorrect project intern tabl node get group size node group symbol group node get group next core constant system model equal metadata get full name metadata get model id group get metadata id core constant system admin model equal metadata get full name metadata get model id group get metadata id node project symbol node get properti node constant info output col element symbol acut column resolv util resolv element group group metadata project symbol equal acut column node node properti node constant info output col acut column node get parent node get parent get type node constant type project parent alreadi project correct output col enough node project node pnode project node get id pnode select symbol project symbol node access node prepar add node node node properti node constant info output col project symbol pnode add child node pnodeprepar add output element plan node col node get properti node constant info output col process node element col cost estim number estim node cardin number node get properti node constant info est cardin process node estim node cardin estim node cardin number estim node size number node get properti node constant info est size process node estim node size estim node size number estim dep access cardin number node get properti node constant info est dep cardin process node estim dep access cardin estim dep access cardin number estim dep join cost number node get properti node constant info est dep join cost process node estim dep join cost estim dep join cost number estim join cost number node get properti node constant info est join cost process node estim join cost estim join cost process nodeset rout name look connector bind name object model id node get properti node constant info model id model id model id temp metadata id command store procedur model id store procedur command get model id command creat command drop collect group symbol group group collector visitor get group command true group symbol group group next model id metadata get model id group get metadata id string cb name metadata get full name model id access node model name cb name access node model id model id access node conform object node get properti info conform sourc queri metadata except queri planner except queri plugin event teiid queri plugin util gs queri plugin event teiidrewrit multi sourc command express result command store procedur store procedur obj store procedur command sp paramet param obj get paramet valu param next sp paramet param param next param get paramet type sp paramet metadata multi sourc element param get metadata id express sourc param get express param remov param use sourc constant constant sourc result sourc command insert insert obj insert command obj get variabl size element symbol elem obj get variabl get object metadata id elem get metadata id metadata multi sourc element metadata id express sourc express obj get valu get obj get variabl remov obj get valu remov result sourc command filter command criteria criteria separ criteria filter command command get criteria compar criteria compar criteria cc compar criteria cc get left express element symbol element symbol es element symbol cc get left express metadata multi sourc element es get metadata id evaluat visitor becom constant cc get right express result result equal cc get right express constant constant result cc get right express resultvisit node cap visitor valid obj express obj accept visitor cap visitor super visit node obj</s>\n"
     ]
    }
   ],
   "source": [
    "print(CC_word2vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dataset to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(\"my_word2vec_model\")\n",
    "word_index_x = {word: idx + 1 for idx, word in enumerate(word2vec_model.wv.index_to_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_index_x.get(\"__unk__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingMatrix(word_index_x: dict[str, int], word2vec_model: np.ndarray):\n",
    "    # The first index is reserved for padding other wise the embedding matrix is filled as feature vector\n",
    "    embedding_matrix = np.zeros((len(word2vec_model.wv.index_to_key) + 1, word2vec_model.vector_size))\n",
    "    for word, idx in word_index_x.items():\n",
    "        embedding_matrix[idx] = word2vec_model.wv[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "def getWordIndex(x_train: np.ndarray) -> list:\n",
    "    x_train_words_indices = list()\n",
    "    for sentence in x_train:\n",
    "        temp = list()\n",
    "        for word in sentence:\n",
    "            if(word_index_x.get(word) != None):\n",
    "                temp.append(word_index_x[word])\n",
    "            else:\n",
    "                # In the case of unknown words --> their index = 105\n",
    "                temp.append(105)\n",
    "        x_train_words_indices.append(temp)\n",
    "    return x_train_words_indices\n",
    "\n",
    "embedding_matrix = getEmbeddingMatrix(word_index_x, word2vec_model)\n",
    "x_train_words_indices = getWordIndex(new_files + descriptions_train + summaries_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PreProcessor.vocabToIndex()\n",
    "_PreProcessor.dataSetToIndex(function_names_train,function_segments_train,'CC')\n",
    "_PreProcessor.dataSetToIndex(descriptions_train,summaries_train,'UC')\n",
    "\n",
    "_PreProcessor.dataSetToIndex(function_names_test,function_segments_test,'CC')\n",
    "_PreProcessor.dataSetToIndex(descriptions_test,summaries_test,'UC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, labels = _PreProcessor.setUpLabels(function_names_train,function_segments_train,descriptions_train,summaries_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/x_train.pkl', 'wb') as f:\n",
    "       pickle.dump(x_train, f)\n",
    "with open('./pickles/DeepLearning/labels.pkl', 'wb') as f:\n",
    "        pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./pickles/DeepLearning/x_train.pkl',allow_pickle=True)\n",
    "labels = np.load('./pickles/DeepLearning/labels.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracibilityLinkDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train):\n",
    "\n",
    "        # # getting the max number of funtions in a file along all files\n",
    "        # #getting the longest name of a function along all functions in all files\n",
    "        # max_number_function = max(len(_) for _ in x_train_function_names)\n",
    "        # max_function_name_len = max(len(function) for file in x_train_function_names for function in file)\n",
    "\n",
    "        # #getting the longest segment of a function along all functions in all files\n",
    "        # max_function_segment_len = max(len(function) for file in x_train_function_segments for function in file)\n",
    "\n",
    "        # x_train_function_names_padded = [file + [[]] * (max_number_function - len(file)) for file in x_train_function_names]\n",
    "        # x_train_function_names_padded = [function + [vocab_size+1] * (max_number_function - len(function)) for file in x_train_function_names_padded for function in file]\n",
    "        \n",
    "        # # convert it to a tensor to be used in model\n",
    "        # self.tensor_x_train_function_names_padded = torch.tensor(x_train_function_names_padded)\n",
    "        \n",
    "        self.tensor_x_train = x_train\n",
    "        self.tensor_y_train = torch.tensor(y_train)\n",
    "\n",
    "\n",
    "    # len and get item are very important --> used by dataloader\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_x_train[idx], self.tensor_y_train[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TracibilityLinkDataset(x_train, labels)\n",
    "torch.save(dataset_train,'./dataset/teiid_dataset/DeepLearningDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = torch.load('./dataset/teiid_dataset/DeepLearningDataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModel(nn.Module):\n",
    "\n",
    "  def __init__(self,embedding_matrix : np.array, embedding_dim : tuple, classes: int = 2):\n",
    "      super(DLModel, self).__init__()\n",
    "\n",
    "      # first embedding layer: input size --> vocab size, output size --> feature vector of vocan size(one hot encoding), weigths --> one hot encoding\n",
    "      # matrix (vocab * vocab)\n",
    "      self.one_hot_embedding = nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim = embedding_dim[1]/2, _weight = embedding_matrix)\n",
    "      \n",
    "      # self-attention layer --> num_heads = 1, query, key, value will all be the input\n",
    "      self.attention_layer = nn.MultiheadAttention(embed_dim=embedding_dim[1], num_heads=1, batch_first = True)\n",
    "\n",
    "      # softmax ?\n",
    "      # self.linear = nn.Linear(2 * hidden_size_layer_chars, classes, bias=True)\n",
    "\n",
    "  def forward(self,  function_names, function_segments, descriptions, summaries ):\n",
    "    \n",
    "    function_names_embeddings = [self.one_hot_embedding(fn) for fn in function_names]\n",
    "    function_segments_embeddings = [self.one_hot_embedding(fs) for fs in function_segments]\n",
    "\n",
    "    descriptions_embedding  = self.one_hot_embedding(descriptions)\n",
    "    summaries_embedding = self.one_hot_embedding(summaries)\n",
    "    \n",
    "    print(len(function_names_embeddings))\n",
    "    print(len(function_segments_embeddings))\n",
    "    print(len(descriptions_embedding))\n",
    "    print(len(summaries_embedding))\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # feature_vector = self.embedding_words(sentences) # sentences * words *253\n",
    "\n",
    "    # feature_vector = feature_vector.to(dtype=torch.float)\n",
    "    # lstm_output = self.lstm_layer_words(feature_vector) # sentences * words *200\n",
    "    # #print(lstm_output[0].shape)  \n",
    "    # # looping over lstm_output to remove unwanted feature vectors\n",
    "    # no_of_sentences, length_of_sentence, feature_vector_size = lstm_output[0].shape\n",
    "    # lstm_output_words= lstm_output[0]\n",
    "    # lstm_output_words_2d=lstm_output_words.reshape(-1,lstm_output_words.shape[2])\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "    # embedding_char_output_4d=self.embedding_char(words)  # sentences * words * chars * one hot encoding for each char\n",
    "    # #words * chars * one hot encoding for each char\n",
    "    # embedding_char_output_3d=embedding_char_output_4d.reshape(-1,embedding_char_output_4d.shape[2],embedding_char_output_4d.shape[3])\n",
    "    # #print(embedding_char_output_3d.shape)\n",
    "\n",
    "    # #print(lstm_output_words_2d[0]) # words*hot encoding vector\n",
    "    # embedding_char_output_3d_concatenated=np.zeros((embedding_char_output_3d.shape[0],embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[2]+lstm_output_words_2d.shape[1]))\n",
    "\n",
    "    # # for word in range(len(lstm_output_words_2d)):\n",
    "    # #        for chars in range(len(embedding_char_output_3d[word])):\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "    # lstm_output_words_2d=lstm_output_words_2d.detach().numpy()\n",
    "\n",
    "    # lstm_output_words_2d = np.repeat(lstm_output_words_2d, repeats=embedding_char_output_3d.shape[1], axis=0)\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "\n",
    "    # lstm_output_words_2d=lstm_output_words_2d.reshape(lstm_output_words_2d.shape[0]//embedding_char_output_3d.shape[1],embedding_char_output_3d.shape[1],lstm_output_words_2d.shape[1])\n",
    "    # #print(lstm_output_words_2d.shape)\n",
    "\n",
    "    # embedding_char_output_3d_concatenated = np.append(embedding_char_output_3d.detach().numpy(),lstm_output_words_2d,axis=2)  # Append along rows\n",
    "\n",
    "    # embedding_char_output_3d_concatenated = torch.tensor(embedding_char_output_3d_concatenated)\n",
    "    # #print(embedding_char_output_3d_concatenated.shape)\n",
    "\n",
    "    # embedding_char_output_3d_concatenated = embedding_char_output_3d_concatenated.to(dtype=torch.float)\n",
    "    # #lstm_output_intermediate = self.intermediate_lstm_layer(embedding_char_output_3d_concatenated)\n",
    "    # #print(embedding_char_output_3d_concatenated.shape)\n",
    "    # lstm_output_char = self.lstm_layer_char(embedding_char_output_3d_concatenated)\n",
    "    # final_output = self.linear(lstm_output_char[0])  # words * 13 char * 15 classs\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def customCollate(batch: list):\n",
    "    # batch --> tuple (x,y)\n",
    "    # x --> list of 4 lists \n",
    "    x_batch, y_batch = zip(*batch)\n",
    "\n",
    "    function_names, function_segments,descriptions , summaries= zip(*x_batch)\n",
    "\n",
    "    function_names_padded = []  # Integer tensor\n",
    "    function_segments_padded = []\n",
    "    \n",
    "    for index,(file_name,file_segment) in enumerate(zip(function_names,function_segments)):\n",
    "        function_names_padded.append(pad_sequence(file_name, batch_first=True, padding_value = 0))\n",
    "        function_segments_padded.append(pad_sequence(file_segment, batch_first=True, padding_value = 0) ) \n",
    "\n",
    "   \n",
    "    descriptions_padded = pad_sequence(descriptions, batch_first=True, padding_value=0)\n",
    "    summaries_padded = pad_sequence(summaries, batch_first=True, padding_value = 0)\n",
    "    \n",
    "    return function_names_padded,function_segments_padded,descriptions_padded, summaries_padded,y_batch\n",
    "\n",
    "def train(model, train_dataset, batch_size=10, epochs=1, learning_rate=0.01):\n",
    "\n",
    "    # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, collate_fn=customCollate)\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "\n",
    "    # (3) create the optimizer (Adam)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        n_samples=0\n",
    "        batch_num = 0\n",
    "        for function_names_padded,function_segments_padded,descriptions_padded, summaries_padded, y_batch in train_dataloader:\n",
    "\n",
    "    #         # (6) do the forward pass\n",
    "\n",
    "              output = model.forward(function_names_padded,function_segments_padded,descriptions_padded, summaries_padded)\n",
    "              batch_num+=1\n",
    "              print(batch_num)\n",
    "            \n",
    "    #         # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "    #         # label: batch_size * 253 * 13\n",
    "    #         # output: 25300 word * 13 char * 15 \n",
    "    #         # k2eny fket el array l words msh sentence of words\n",
    "\n",
    "    #         # batch_loss = criterion(output.view(-1, output.size(2)), train_label.view(-1))\n",
    "\n",
    "    #         # # (8) append the batch loss to the total_loss_train\n",
    "\n",
    "    #         # total_loss_train += batch_loss\n",
    "            \n",
    "    #         # # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "    #         # _,predicted=torch.max(output,2)  #512 * 104  for every word in every sentence we choose one tag form the seventen tag \n",
    "    #         # acc=(predicted==train_label.view(-1, train_label.shape[2])).sum().item()\n",
    "    #         # n_samples += train_label.size(0)*train_label.size(1)*train_label.size(2)\n",
    "    #         # total_acc_train += acc\n",
    "    #         # print(100*total_acc_train/n_samples)\n",
    "\n",
    "    #         # # (10) zero your gradients\n",
    "    #         # optimizer.zero_grad()\n",
    "\n",
    "    #         # # (11) do the backward pass\n",
    "    #         # batch_loss.backward()\n",
    "    #         # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "    #         # # (12) update the weights with your optimizer\n",
    "    #         # optimizer.step()\n",
    "    #     # epoch loss\n",
    "    #     # epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    #     # # (13) calculate the accuracy\n",
    "    #     # epoch_acc =100*total_acc_train/n_samples\n",
    "\n",
    "    #     # print(\n",
    "    #     #     f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "    #     #     | Train Accuracy: {epoch_acc}\\n')\n",
    "    #     break\n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_embedding = torch.eye(len(vocab.keys()) + 2)\n",
    "model = DLModel(one_hot_embedding, one_hot_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_hot_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataset_train)\n",
    "# print(len(labels), len(x_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
