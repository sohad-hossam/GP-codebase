{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language.build_library is deprecated. Use the new bindings instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n",
      "c:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\tree_sitter\\__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "\n",
    "from PreProcessor import *\n",
    "\n",
    "_PreProcessor = PreProcessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names_train,function_segments_train = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/train_CC\",'CC','train')\n",
    "descriptions_train, summaries_train = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/train_UC\",'UC','train')\n",
    "function_names_test,function_segments_test = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/test_CC\",'CC','test')\n",
    "descriptions_test, summaries_test = _PreProcessor.setupDeepLearning(\"./dataset/teiid_dataset/test_UC\",'UC','test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PreProcessor.setUpUnknown(function_names_train,function_segments_train,'CC')\n",
    "_PreProcessor.setUpUnknown(descriptions_train,summaries_train,'UC')\n",
    "\n",
    "_PreProcessor.setUpUnknown(function_names_test,function_segments_test,'CC')\n",
    "_PreProcessor.setUpUnknown(descriptions_test,summaries_test,'UC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/function_names_train.pkl', 'wb') as f:\n",
    "       pickle.dump(function_names_train, f)\n",
    "with open('./pickles/DeepLearning/function_segments_train.pkl', 'wb') as f:\n",
    "        pickle.dump(function_segments_train, f)\n",
    "with open('./pickles/DeepLearning/descriptions_train.pkl', 'wb') as f:\n",
    "        pickle.dump(descriptions_train, f)\n",
    "with open('./pickles/DeepLearning/summaries_train.pkl', 'wb') as f:\n",
    "        pickle.dump(summaries_train, f)\n",
    "\n",
    "with open('./pickles/DeepLearning/Vocab.pkl', 'wb') as f:\n",
    "        pickle.dump(_PreProcessor.Vocab, f)\n",
    "\n",
    "with open('./pickles/DeepLearning/function_names_test.pkl', 'wb') as f:\n",
    "       pickle.dump(function_names_test, f)\n",
    "with open('./pickles/DeepLearning/function_segments_test.pkl', 'wb') as f:\n",
    "        pickle.dump(function_segments_test, f)\n",
    "with open('./pickles/DeepLearning/descriptions_test.pkl', 'wb') as f:\n",
    "        pickle.dump(descriptions_test, f)\n",
    "with open('./pickles/DeepLearning/summaries_test.pkl', 'wb') as f:\n",
    "        pickle.dump(summaries_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_names_train = np.load('./pickles/DeepLearning/function_names_train.pkl',allow_pickle=True)\n",
    "function_segments_train = np.load('./pickles/DeepLearning/function_segments_train.pkl',allow_pickle=True)\n",
    "descriptions_train = np.load('./pickles/DeepLearning/descriptions_train.pkl',allow_pickle=True)\n",
    "summaries_train = np.load('./pickles/DeepLearning/summaries_train.pkl',allow_pickle=True)\n",
    "vocab = np.load('./pickles/DeepLearning/Vocab.pkl',allow_pickle=True)\n",
    "function_names_test = np.load('./pickles/DeepLearning/function_names_test.pkl',allow_pickle=True)\n",
    "function_segments_test = np.load('./pickles/DeepLearning/function_segments_test.pkl',allow_pickle=True)\n",
    "descriptions_test = np.load('./pickles/DeepLearning/descriptions_test.pkl',allow_pickle=True)\n",
    "summaries_test = np.load('./pickles/DeepLearning/summaries_test.pkl',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC_word2vec = _PreProcessor.word2VecProcessor(function_names_train, function_segments_train, 'CC')\n",
    "UC_word2vec = _PreProcessor.word2VecProcessor(summaries_train, descriptions_train, 'UC')\n",
    "# print(CC_word2vec)\n",
    "word2vec_model = Word2Vec(sentences = CC_word2vec + UC_word2vec, vector_size=200, window=5, min_count=1, workers=4, epochs=10)\n",
    "word2vec_model.save(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n",
    "\n",
    "#word2vec_model.build_vocab(UC_word2vec, update=True)\n",
    "#word2vec_model.train(UC_word2vec, total_examples=word2vec_model.corpus_count, epochs=word2vec_model.epochs)\n",
    "\n",
    "# word2vec_model.save(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n",
    "word2vec_model = Word2Vec.load(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n",
    "\n",
    "word2vec_vocab = word2vec_model.wv.index_to_key\n",
    "\n",
    "# Initialize an empty embedding matrix\n",
    "embedding_matrix = np.zeros((len(word2vec_vocab) + 1, word2vec_model.vector_size))\n",
    "\n",
    "# Fill the embedding matrix with the embeddings of each word\n",
    "for i, word in enumerate(word2vec_vocab):\n",
    "    embedding_vector = word2vec_model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i + 1] = embedding_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/embedding_matrix.pkl', 'wb') as f:\n",
    "       pickle.dump(embedding_matrix, f)\n",
    "with open('./pickles/DeepLearning/word2vec_vocab.pkl', 'wb') as f:\n",
    "       pickle.dump(word2vec_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6245\n"
     ]
    }
   ],
   "source": [
    "print(len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dataset to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PreProcessor.vocabToIndex(word2vec_vocab)\n",
    "_PreProcessor.dataSetToIndex(function_names_train,function_segments_train)\n",
    "_PreProcessor.dataSetToIndex(descriptions_train,summaries_train)\n",
    "\n",
    "_PreProcessor.dataSetToIndex(function_names_test,function_segments_test)\n",
    "_PreProcessor.dataSetToIndex(descriptions_test,summaries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, labels = _PreProcessor.setUpLabels(function_names_train,function_segments_train,descriptions_train,summaries_train,\"./dataset/teiid_dataset/train_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, labels_test = _PreProcessor.setUpLabels(function_names_test,function_segments_test,descriptions_test,summaries_test,\"./dataset/teiid_dataset/test_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/x_train.pkl', 'wb') as f:\n",
    "       pickle.dump(x_train, f)\n",
    "with open('./pickles/DeepLearning/labels.pkl', 'wb') as f:\n",
    "        pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/x_test.pkl', 'wb') as f:\n",
    "       pickle.dump(x_test, f)\n",
    "with open('./pickles/DeepLearning/labels_test.pkl', 'wb') as f:\n",
    "        pickle.dump(labels_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./pickles/DeepLearning/x_train.pkl',allow_pickle=True)\n",
    "labels = np.load('./pickles/DeepLearning/labels.pkl',allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.load('./pickles/DeepLearning/x_test.pkl',allow_pickle=True)\n",
    "labels_test = np.load('./pickles/DeepLearning/labels_test.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracibilityLinkDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train):\n",
    "        \n",
    "        self.tensor_x_train = x_train\n",
    "        self.tensor_y_train = torch.tensor(y_train)\n",
    "\n",
    "\n",
    "    # len and get item are very important --> used by dataloader\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_x_train[idx], self.tensor_y_train[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TracibilityLinkDataset(x_train, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_test \u001b[38;5;241m=\u001b[39m TracibilityLinkDataset(\u001b[43mx_test\u001b[49m, labels_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_test = TracibilityLinkDataset(x_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(dataset_train,'./dataset/teiid_dataset/DeepLearningDataset.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train = torch.load('./dataset/teiid_dataset/DeepLearningDataset.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.checkpoint as checkpoint \n",
    "class DLModel(nn.Module):\n",
    "\n",
    "  def __init__(self,embedding_matrix : np.array, embedding_dim : tuple, classes: int = 2):\n",
    "      super(DLModel, self).__init__()\n",
    "\n",
    "      # first embedding layer: input size --> vocab size, output size --> feature vector of vocan size(one hot encoding), weigths --> one hot encoding\n",
    "      # matrix (vocab * vocab)\n",
    "      self.embedding = nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim = embedding_dim[1], _weight = torch.tensor(embedding_matrix))\n",
    "      \n",
    "      # self-attention layer --> num_heads = 1, query, key, value will all be the input\n",
    "      self.attention_layer_UC = nn.MultiheadAttention(embed_dim=embedding_dim[1], num_heads=1, batch_first = True)\n",
    "      self.attention_layer_CC = nn.MultiheadAttention(embed_dim=embedding_dim[1], num_heads=1, batch_first = True)\n",
    "\n",
    "      self.conv_layer_2d =torch.nn.Conv2d(1, 1, kernel_size = (5,5), stride = (2,2))\n",
    "      \n",
    "      self.pool_layer_2d = torch.nn.AvgPool2d(kernel_size = (5,5), stride = (2, 2))\n",
    "      \n",
    "      self.cosine_similarity = torch.nn.CosineSimilarity(dim=1) # code = 1*2000*200 , req = code = 1*1000*200 \n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "  def forward(self, function_names, function_segments, descriptions, summaries ):\n",
    "\n",
    "    function_names_embeddings  = self.embedding(function_names)\n",
    "    function_segments_embeddings = self.embedding(function_segments)\n",
    "    descriptions_embedding  = self.embedding(descriptions)\n",
    "    summaries_embedding = self.embedding(summaries)\n",
    "\n",
    "    function_names_attention = self.attention_layer_CC(function_names_embeddings.float(),function_names_embeddings.float(),function_names_embeddings.float())\n",
    "\n",
    "    function_segments_attention = self.attention_layer_CC(function_segments_embeddings.float(),function_segments_embeddings.float(),function_segments_embeddings.float())\n",
    "   \n",
    "    descriptions_attention = self.attention_layer_UC(descriptions_embedding.float(),descriptions_embedding.float(),descriptions_embedding.float())\n",
    "\n",
    "    summaries_attention = self.attention_layer_UC(summaries_embedding.float(),summaries_embedding.float(),summaries_embedding.float())\n",
    "\n",
    "    concatenated_name_seg = torch.cat((function_names_attention[0], function_segments_attention[0]), dim = 1)\n",
    "\n",
    "    summaries_descrip_seg = torch.cat((descriptions_attention[0], summaries_attention[0]), dim = 1)\n",
    "\n",
    "    names_seg_conv = self.conv_layer_2d(concatenated_name_seg.unsqueeze(1))\n",
    "    # print(\"names_seg_conv.shape = \" , names_seg_conv.shape)\n",
    "\n",
    "    # summaries_descrip_conv = self.conv_layer_2d(summaries_descrip_seg.unsqueeze(1))\n",
    "    # print(\"summaries_descrip_seg.shape = \",summaries_descrip_seg.shape)\n",
    "\n",
    "    name_seg = self.pool_layer_2d (names_seg_conv)\n",
    "    # print(\"name_seg.shape = \" , name_seg.shape)\n",
    "\n",
    "    summaries_descrip_flatten = summaries_descrip_seg.view(summaries_descrip_seg.shape[0],-1)\n",
    "    name_seg_flatten = name_seg.view(name_seg.shape[0],-1) \n",
    "\n",
    "    # print(\"name_seg_flatten.shape = \" , name_seg_flatten.shape)\n",
    "    # print(\"summaries_descrip_flatten.shape = \" , summaries_descrip_flatten.shape)\n",
    "\n",
    "    num_repeats = (name_seg_flatten.shape[1] + summaries_descrip_flatten.shape[1] - 1) // summaries_descrip_flatten.shape[1] \n",
    "\n",
    "    #inreasing th size of the req document to be the same as the code documet to allow for cosine simiraity \n",
    "    summaries_descrip_flatten_tiled = summaries_descrip_flatten.repeat((1,num_repeats))\n",
    "    summaries_descrip_flatten_tiled_segmented = summaries_descrip_flatten_tiled[:,:name_seg_flatten.shape[1]]\n",
    "    # print(\"summaries_descrip_flatten_tiled.shape\",summaries_descrip_flatten_tiled_segmented.shape)\n",
    "\n",
    "    consine_sim_code_req = self.cosine_similarity(name_seg_flatten,summaries_descrip_flatten_tiled_segmented)\n",
    "    consine_sim_code_req_0_1 = self.sigmoid (consine_sim_code_req)\n",
    "\n",
    "    # summary_desc = self.pool_layer_2d (summaries_descrip_seg)\n",
    "    # print(\"summary_desc.shape = \",summary_desc.shape)\n",
    "\n",
    "    # print(torch.cuda.memory_reserved()/1024**3)\n",
    "    \n",
    "    # print(\"consine_sim_code_req.shape\", consine_sim_code_req.shape)\n",
    "    return consine_sim_code_req_0_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def customCollate(batch: list):\n",
    "    # batch --> tuple (x,y)\n",
    "    # x --> list of 4 lists\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "\n",
    "    function_names, function_segments, descriptions , summaries = zip(*x_batch)\n",
    "\n",
    "    function_names_padded = pad_sequence(function_names, batch_first=True, padding_value=0)\n",
    "    function_segments_padded = pad_sequence(function_segments, batch_first=True, padding_value = 0)\n",
    "    descriptions_padded = pad_sequence(descriptions, batch_first=True, padding_value=0)\n",
    "    summaries_padded = pad_sequence(summaries, batch_first=True, padding_value = 0)\n",
    "\n",
    "    return  function_names_padded, function_segments_padded, descriptions_padded, summaries_padded, y_batch\n",
    "\n",
    "def train(model, train_dataset, batch_size=5, epochs=1, learning_rate=0.01):\n",
    "    class_weights = torch.FloatTensor([0.5008920921894572, 280.74009508716324])\n",
    "    \n",
    "    # (1) create the dataloader of the training set IMPORTANT (make the shuffle=True)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=customCollate)\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n",
    "    device=\"cpu\"\n",
    "#     if use_cuda:\n",
    "#         model = model.cuda()\n",
    "#         criterion = criterion.cuda()\n",
    "\n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.BCELoss(weight = class_weights).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "    \n",
    "   \n",
    "    # (3) create the optimizer (Adam)\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        n_samples = 0\n",
    "        for function_names_padded, function_segments_padded, descriptions_padded, summaries_padded, y_batch in tqdm(train_dataloader):\n",
    "            # torch.cuda.empty_cache()     \n",
    "            function_names_padded_tensor = torch.tensor(function_names_padded, device=device)\n",
    "            function_segments_padded_tensor = torch.tensor(function_segments_padded, device=device)\n",
    "            descriptions_padded_tensor = torch.tensor(descriptions_padded, device=device)\n",
    "            fsummaries_padded_tensor = torch.tensor(summaries_padded, device=device)\n",
    "            y_batch_tensor = torch.tensor(y_batch,dtype = torch.float32,device=device)\n",
    "\n",
    "            output = model.forward(function_names_padded_tensor, function_segments_padded_tensor, descriptions_padded_tensor, fsummaries_padded_tensor)\n",
    "\n",
    "            output_0_1 = torch.tensor((output > 0.5), dtype=torch.float32)\n",
    "            \n",
    "            batch_loss = torch.tensor(criterion(output, y_batch_tensor),requires_grad=True)\n",
    "            # torch.cuda.empty_cache()\n",
    "            total_loss_train += batch_loss\n",
    "            # print(torch.cuda.memory_reserved()/1024**3)\n",
    "              \n",
    "\n",
    "            # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "            \n",
    "    #       (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "            n_true = sum((output_0_1 == y_batch_tensor))\n",
    "            n_samples += y_batch_tensor.size(0)\n",
    "            total_acc_train += n_true\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    #       (11) do the backward pass\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "    #       (12) update the weights with your optimizer\n",
    "            optimizer.step()\n",
    "            # (10) zero your gradients\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    #   epoch loss\n",
    "        epoch_loss = total_loss_train / y_batch_tensor.size(0)\n",
    "\n",
    "    #   (13) calculate the accuracy\n",
    "        epoch_acc =100*total_acc_train/n_samples\n",
    "\n",
    "        print(\n",
    "           f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "           | Train Accuracy: {epoch_acc}\\n')\n",
    "        break\n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/566871 [00:00<?, ?it/s]C:\\Users\\basse\\AppData\\Local\\Temp\\ipykernel_14556\\3920393980.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  function_names_padded_tensor = torch.tensor(function_names_padded, device=device)\n",
      "C:\\Users\\basse\\AppData\\Local\\Temp\\ipykernel_14556\\3920393980.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  function_segments_padded_tensor = torch.tensor(function_segments_padded, device=device)\n",
      "C:\\Users\\basse\\AppData\\Local\\Temp\\ipykernel_14556\\3920393980.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  descriptions_padded_tensor = torch.tensor(descriptions_padded, device=device)\n",
      "C:\\Users\\basse\\AppData\\Local\\Temp\\ipykernel_14556\\3920393980.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  fsummaries_padded_tensor = torch.tensor(summaries_padded, device=device)\n",
      "C:\\Users\\basse\\AppData\\Local\\Temp\\ipykernel_14556\\3920393980.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output_0_1 = torch.tensor((output > 0.5), dtype=torch.float32)\n",
      "  0%|          | 0/566871 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m DLModel(embedding_matrix, embedding_matrix\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 54\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     50\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(function_names_padded_tensor, function_segments_padded_tensor, descriptions_padded_tensor, fsummaries_padded_tensor)\n\u001b[0;32m     52\u001b[0m output_0_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor((output \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 54\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch_tensor\u001b[49m\u001b[43m)\u001b[49m,requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m     56\u001b[0m total_loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_loss\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:619\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 619\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\envs\\cuda_env\\lib\\site-packages\\torch\\nn\\functional.py:3095\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3089\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3092\u001b[0m     )\n\u001b[0;32m   3094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3095\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3096\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[0;32m   3098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight, reduction_enum)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "model = DLModel(embedding_matrix, embedding_matrix.shape)\n",
    "train(model, dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '1stModel_5batch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('1stModel_5batch.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, outputFile,batch_size=5):\n",
    "  \"\"\"\n",
    "  This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
    "  Inputs:\n",
    "  - model: a NER model\n",
    "  - test_dataset: dataset of type NERDataset\n",
    "  \"\"\"\n",
    "  ########################### TODO: Replace the Nones in the following code ##########################\n",
    "\n",
    "  # (1) create the test data loader\n",
    "  test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, collate_fn=customCollate , shuffle=False)\n",
    "\n",
    "  total_acc_test = 0\n",
    "  n_samples = 0\n",
    "\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "    \n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n",
    "    # device=\"cpu\"\n",
    "  if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "  with torch.no_grad():\n",
    "    noBatch=0\n",
    "    for function_names_padded, function_segments_padded, descriptions_padded, summaries_padded, y_batch in tqdm(test_dataloader):\n",
    "            function_names_padded_tensor = torch.tensor(function_names_padded, device=device)\n",
    "            function_segments_padded_tensor = torch.tensor(function_segments_padded, device=device)\n",
    "            descriptions_padded_tensor = torch.tensor(descriptions_padded, device=device)\n",
    "            fsummaries_padded_tensor = torch.tensor(summaries_padded, device=device)\n",
    "            y_batch_tensor = torch.tensor(y_batch,dtype = torch.float32,device=device)\n",
    "\n",
    "            output = model.forward(function_names_padded_tensor, function_segments_padded_tensor, descriptions_padded_tensor, fsummaries_padded_tensor)\n",
    "\n",
    "            output_0_1 = torch.tensor((output > 0.5), dtype=torch.float32)\n",
    "\n",
    "            n_true = sum((output_0_1 == y_batch_tensor))\n",
    "            n_samples += y_batch_tensor.size(0)\n",
    "            total_acc_test += n_true\n",
    "       \n",
    "    # (6) calculate the over all accuracy\n",
    "    total_acc_test =100*total_acc_test/n_samples\n",
    "  ##################################################################################################\n",
    "\n",
    "  print(f'\\nTest Accuracy: {total_acc_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, dataset_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py399",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
