{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "from difflib import unified_diff\n",
    "from imports import *\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from PreprocessingFunctions import *\n",
    "import random\n",
    "\n",
    "preprocessor_functions = PreProcessorFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "1) get bug reports --> done\n",
    "2) check if marked as resolved or closed --> done\n",
    "3) fix versions --> bug fix location?\n",
    "4) get the bug id and version nums? --> done\n",
    "5) get the project version and then check the commits in that version to detect the commits that contain bug fixes.\n",
    "6) get the code commit  to get the buggy version of the code\n",
    "'''\n",
    "con = sqlite3.connect(\"Dataset/errai_dataset/errai.sqlite3\")\n",
    "cur = con.cursor()\n",
    "\n",
    "# 1) get bug reports & 2)check if marked as resolved or closed\n",
    "issue_df = pd.read_sql_query(\"SELECT issue_id FROM issue WHERE type = 'Bug' AND resolution = 'Done';\", con)\n",
    "print(issue_df.shape)\n",
    "\n",
    "# 3) fix versions --> inside table issue_fix_version\n",
    "issue_commits_hash = pd.read_sql_query(\"SELECT issue_id, commit_hash FROM change_set_link;\", con)\n",
    "print(issue_commits_hash.shape)\n",
    "\n",
    "# joining the 2 pandas dfs into 1 df\n",
    "issue_data = pd.merge(issue_df, issue_commits_hash, on='issue_id', how='inner')\n",
    "# print(issue_data)\n",
    "# duplicates = issue_data['issue_id'].duplicated()\n",
    "# num_duplicates = duplicates.sum()\n",
    "# print(f'Number of duplicate rows: {num_duplicates}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo = git.Repo.clone_from('https://github.com/errai/errai', 'errai_repo_clone')  # Clone the repository\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "owner = 'errai'\n",
    "repo = 'errai'\n",
    "token = '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'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'token {token}'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "repo_local= git.Repo(\"errai_repo_clone\")\n",
    "old_new_files = list()\n",
    "counter = 0\n",
    "for index, row in issue_data.iterrows():\n",
    "    url = f'https://api.github.com/repos/{owner}/{repo}/commits/{row[\"commit_hash\"]}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    commit_global = response.json()\n",
    "\n",
    "    # getting the parent commit hash to traverse on parents until we reach the last version of the file\n",
    "    commit_local = repo_local.commit(row[\"commit_hash\"])\n",
    "    commit_parent_hash_str = commit_local.parents[0].hexsha\n",
    "    commit_parent_hash = repo_local.commit(commit_parent_hash_str)\n",
    "\n",
    "    files_in_commit = commit_global['files']\n",
    "    # new_content = repo.git.show(f\"{\"ef996fff8504ca6da3f5cea2b8dc8950573936cc\"}:{file_path}\")\n",
    "\n",
    "    # status_code = response.status_code\n",
    "    for file_name in files_in_commit:\n",
    "        # new_file_content = requests.get(file_name['raw_url'], headers=headers)\n",
    "        if file_name['filename'].lower().endswith('.java'):\n",
    "            try: \n",
    "                old_content = commit_parent_hash.tree[file_name['filename']].data_stream.read().decode('utf-8')\n",
    "                new_content = commit_local.tree[file_name['filename']].data_stream.read().decode('utf-8')\n",
    "                temp = (row[\"issue_id\"],new_content, old_content)\n",
    "                if len(temp) != 0:\n",
    "                    old_new_files.append(temp)\n",
    "            except:\n",
    "                counter+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./pickles/old_new_files.npy',old_new_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(old_new_files[0])\n",
    "old_new_files = np.load('./pickles/old_new_files.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(old_new_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"Dataset/errai_dataset/errai.sqlite3\")\n",
    "cur = con.cursor()\n",
    "issue_df = pd.read_sql_query(\"SELECT issue_id, summary, description FROM issue\", con)\n",
    "con.close()\n",
    "\n",
    "# Language.build_library(\n",
    "#         # Store the library in the `build` directory\n",
    "#     \"build/my-languages.so\",\n",
    "#     # Include one or more languages\n",
    "#     [\"./tree-sitter-java\"],\n",
    "#     )\n",
    "\n",
    "JAVA_LANGUAGE = Language(tsjava.language())\n",
    "parser = Parser(JAVA_LANGUAGE)\n",
    "\n",
    "# JAVA = Language(\"build/my-languages.so\", \"java\")\n",
    "# parser = Parser()\n",
    "# parser.set_language(JAVA)\n",
    "\n",
    "data = list()\n",
    "\n",
    "for issue_id ,new, old in old_new_files:\n",
    "\n",
    "    src_new = bytes(\n",
    "    new,\n",
    "    \"utf-8\",\n",
    "    )\n",
    "    src_old = bytes(\n",
    "    old,\n",
    "    \"utf-8\",\n",
    "    )\n",
    "\n",
    "    tree_new = parser.parse(src_new)\n",
    "    curr_node_new = tree_new.root_node\n",
    "    queue_new=list()\n",
    "    queue_new.append(curr_node_new)\n",
    "\n",
    "    tree_old = parser.parse(src_old)\n",
    "    curr_node_old = tree_old.root_node\n",
    "    queue_old=list()\n",
    "    queue_old.append(curr_node_old)\n",
    "\n",
    "    method_dict = dict()\n",
    "\n",
    "\n",
    "    description_summary_row = issue_df[issue_df['issue_id'] == issue_id]\n",
    "    summary = description_summary_row.iloc[0][\"summary\"]\n",
    "    description = description_summary_row.iloc[0][ \"description\"]\n",
    "\n",
    "    if summary == None or description == None:\n",
    "        continue\n",
    "    \n",
    "    while(len(queue_new)):\n",
    "        curr_node = queue_new.pop(0)\n",
    "        for child in curr_node.children:\n",
    "            queue_new.append(child)\n",
    "            if(child.parent.type == \"method_declaration\" and child.type == \"block\"):\n",
    "                segment = new[child.start_byte:child.end_byte]\n",
    "                method_name=new[child.parent.children[2].start_byte:child.parent.children[2].end_byte]\n",
    "                method_dict[method_name] = segment\n",
    "\n",
    "                \n",
    "    while(len(queue_old)):\n",
    "        curr_node = queue_old.pop(0)\n",
    "        for child in curr_node.children:\n",
    "            queue_old.append(child)\n",
    "            if(child.parent.type == \"method_declaration\" and child.type == \"block\"):\n",
    "                segment = old[child.start_byte:child.end_byte]\n",
    "                method_name=old[child.parent.children[2].start_byte:child.parent.children[2].end_byte]\n",
    "                \n",
    "                # we put the functions that were deleted in the new commit but not the function that was added\n",
    "                # as the deleted func isa buggy one but the added one is not a buggy one\n",
    "\n",
    "                if (method_dict.get(method_name) != None and method_dict[method_name] != segment) or method_dict.get(method_name) == None:\n",
    "                    data.append([method_name+segment,summary+description,1])\n",
    "                elif random.randint(1, 3) == 3: #just a dummy number\n",
    "                    data.append([method_name+segment,summary+description,0])\n",
    "\n",
    "                           \n",
    "# print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test= train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "uc_stat_stuff = list()\n",
    "cc_stat_stuff = list()\n",
    "\n",
    "x_train_tokenized = list()\n",
    "train_labels = list()\n",
    "\n",
    "x_test_tokenized = list()\n",
    "test_labels = list()\n",
    "\n",
    "for lst in x_train:\n",
    "    cc_stat_stuff.append(len(lst[0]))\n",
    "    uc_stat_stuff.append(len(lst[1]))\n",
    "\n",
    "    method_tokenized = preprocessor_functions.PreProcessorFuncDeepLearning(lst[0] ,'cc', \"train\")\n",
    "    uc_tokenized = preprocessor_functions.PreProcessorFuncDeepLearning(lst[1] , 'uc' , \"train\")\n",
    "\n",
    "    x_train_tokenized.append([method_tokenized,uc_tokenized])\n",
    "\n",
    "    train_labels.append(lst[2])\n",
    "\n",
    "for lst in x_test:\n",
    "    \n",
    "    method_tokenized = preprocessor_functions.PreProcessorFuncDeepLearning(lst[0] ,'cc', \"test\")\n",
    "    uc_tokenized = preprocessor_functions.PreProcessorFuncDeepLearning(lst[1] ,'uc', \"test\")\n",
    "\n",
    "    x_test_tokenized.append([method_tokenized,uc_tokenized])\n",
    "\n",
    "    test_labels.append(lst[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: if accuracy is low, we will clip at mean to reduce padded inputs\n",
    "min_len = 100000000\n",
    "#cc => 4000\n",
    "import statistics\n",
    "\n",
    "# Calculate the mean\n",
    "mean_value = statistics.mean(cc_stat_stuff)\n",
    "print(mean_value)\n",
    "\n",
    "counter=0\n",
    "for cc in cc_stat_stuff:\n",
    "    if cc > 2000:\n",
    "        counter+=1\n",
    "print(counter)\n",
    "\n",
    "mean_value = statistics.mean(uc_stat_stuff)\n",
    "print(mean_value)\n",
    "\n",
    "counter=0\n",
    "for uc in uc_stat_stuff:\n",
    "    if uc > 4000:\n",
    "        counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_functions.setUpUnknown( x_train_tokenized , \"train\" )\n",
    "preprocessor_functions.setUpUnknown( x_test_tokenized , \"test\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/errai_pickles/x_train_tokenized', 'wb') as f:\n",
    "    pickle.dump(x_train_tokenized, f)\n",
    "with open('./pickles/DeepLearning/errai_pickles/train_labels', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)\n",
    "with open('./pickles/DeepLearning/errai_pickles/x_test_tokenized', 'wb') as f:\n",
    "    pickle.dump(x_test_tokenized, f)\n",
    "with open('./pickles/DeepLearning/errai_pickles/test_labels', 'wb') as f:\n",
    "    pickle.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokenized = np.load('./pickles/DeepLearning/errai_pickles/x_train_tokenized',allow_pickle=True)\n",
    "train_labels = np.load('./pickles/DeepLearning/errai_pickles/train_labels',allow_pickle=True)\n",
    "x_test_tokenized = np.load('./pickles/DeepLearning/errai_pickles/x_test_tokenized',allow_pickle=True)\n",
    "test_labels = np.load('./pickles/DeepLearning/errai_pickles/test_labels',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec.load(\"./Dataset/teiid_dataset/UC_CC_WORD2VEC\")\n",
    "\n",
    "\n",
    "functions , uc_documents =zip(*x_train_tokenized)\n",
    "\n",
    "word2vec_model.build_vocab(functions + uc_documents, update=True)\n",
    "\n",
    "word2vec_model.train( functions + uc_documents , total_examples = len(functions + uc_documents), epochs=10)\n",
    "\n",
    "word2vec_model.save(\"./pickles/DeepLearning/errai_pickles/UC_CC_WORD2VEC\")\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"./pickles/DeepLearning/errai_pickles/UC_CC_WORD2VEC\")\n",
    "\n",
    "word2vec_vocab = word2vec_model.wv.index_to_key\n",
    "\n",
    "\n",
    "# Initialize an empty embedding matrix\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2vec_vocab) + 1, word2vec_model.vector_size))\n",
    "\n",
    "# Fill the embedding matrix with the embeddings of each word\n",
    "for i, word in enumerate(word2vec_vocab):\n",
    "    embedding_vector = word2vec_model.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i + 1] = embedding_vector\n",
    "\n",
    "with open('./pickles/DeepLearning/errai_pickles/word2vec_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(word2vec_vocab, f)\n",
    "with open('./pickles/DeepLearning/errai_pickles/embedding_matrix.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_matrix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.load('./pickles/DeepLearning/errai_pickles/embedding_matrix.pkl',allow_pickle=True)\n",
    "word2vec_vocab = np.load('./pickles/DeepLearning/errai_pickles/word2vec_vocab.pkl',allow_pickle=True)\n",
    "\n",
    "print(len(word2vec_vocab))\n",
    "print(word2vec_vocab)\n",
    "\n",
    "print(len(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_functions.vocabToIndex(word2vec_vocab)\n",
    "\n",
    "preprocessor_functions.dataSetToIndex(x_train_tokenized)\n",
    "preprocessor_functions.dataSetToIndex(x_test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TracibilityLinkDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x_train, y_train):\n",
    "        \n",
    "        self.tensor_x_train = x_train\n",
    "        self.tensor_y_train = torch.tensor(y_train)\n",
    "\n",
    "\n",
    "    # len and get item are very important --> used by dataloader\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_x_train[idx], self.tensor_y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TracibilityLinkDataset(x_train_tokenized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = TracibilityLinkDataset(x_test_tokenized, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/DeepLearning/errai_pickles/x_train_tokenized', 'wb') as f:\n",
    "    pickle.dump(x_train_tokenized, f)\n",
    "with open('./pickles/DeepLearning/errai_pickles/x_test_tokenized', 'wb') as f:\n",
    "    pickle.dump(x_test_tokenized, f)\n",
    "\n",
    "with open('./pickles/DeepLearning/errai_pickles/train_labels', 'wb') as f:\n",
    "    pickle.dump(train_labels, f)\n",
    "with open('./pickles/DeepLearning/errai_pickles/test_labels', 'wb') as f: \n",
    "    pickle.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_small = []\n",
    "label_small = []\n",
    "counter_1 = 0\n",
    "counter_0 = 0\n",
    "while(counter_1 < 11 or counter_0 < 11):\n",
    "    index = random.randint(0, len(x_train))\n",
    "\n",
    "    if train_labels[index] == 1 and counter_1 < 11 :\n",
    "        x_train_small.append(x_train_tokenized[index])\n",
    "        label_small.append(1)\n",
    "        counter_1 +=1\n",
    "\n",
    "    elif train_labels[index] == 0 and counter_0 < 11 :\n",
    "        x_train_small.append(x_train_tokenized[index])\n",
    "        label_small.append(0)\n",
    "        counter_0 +=1\n",
    "\n",
    "dataset_small = TracibilityLinkDataset(x_train_small, label_small)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DLModel(nn.Module):\n",
    "\n",
    "  def __init__(self,embedding_matrix : np.array, embedding_dim : tuple,UC_size: int, CC_size:int, hidden_size:int = 128, classes: int = 2):\n",
    "      super(DLModel, self).__init__()\n",
    "\n",
    " \n",
    "      self.embedding = nn.Embedding(num_embeddings = embedding_dim[0], embedding_dim = embedding_dim[1], _weight = torch.tensor(embedding_matrix))\n",
    "      \n",
    "      self.lstm_CC = nn.LSTM(embedding_dim[1],hidden_size,batch_first=True, bidirectional=True)\n",
    "        \n",
    "      self.linear_post_flatten = nn.Linear(CC_size*hidden_size*2+UC_size*hidden_size*2,hidden_size*2)\n",
    "      self.linear_post_flatten2 = nn.Linear(hidden_size*2,hidden_size)\n",
    "\n",
    "      #self.conv_layer = nn.Conv1d(CC_size+UC_size,hidden_size*2 ,3,stride=3)\n",
    "      self.linear = nn.Linear(hidden_size,classes)\n",
    "\n",
    "      self.softmax = nn.Softmax(dim=1)\n",
    "      self.relu = nn.PReLU()\n",
    "      \n",
    "      # self.cosine_similarity = nn.CosineSimilarity(dim=1) # code = 1*(vocab_size)*(feature_vector) , req = code = 1*1000*200 \n",
    "\n",
    "      # self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "  def forward(self, CC,UC ):\n",
    "    #function_names.shape --> batch_no * no_tokens* 150\n",
    "    \n",
    "    CC_embedding  = self.embedding(CC)\n",
    "    UC_embedding = self.embedding(UC)\n",
    "    \n",
    "    #function_names.shape --> batch_no * no_tokens* 150\n",
    "    \n",
    "    \n",
    "    UC_lstm, _ = self.lstm_CC (UC_embedding.float())\n",
    "    CC_lstm, _ = self.lstm_CC (CC_embedding.float())\n",
    "\n",
    "    UC_lstm_flatten  = UC_lstm.reshape(UC_lstm.shape[0],-1)\n",
    "    CC_lstm_flatten  = CC_lstm.reshape(CC_lstm.shape[0],-1) # batch_no * (feature_vector * no_tokens)\n",
    "    \n",
    "    \n",
    "\n",
    "    linear_output_one = self.linear_post_flatten(torch.cat([UC_lstm_flatten,CC_lstm_flatten],axis=1).float())\n",
    "    # print(linear_output_one)\n",
    "    linear_output_two = self.linear_post_flatten2(self.relu(linear_output_one))\n",
    "    # print(conv.shape)\n",
    "    linear_output = self.linear(self.relu(linear_output_two))\n",
    "    \n",
    "    \n",
    "\n",
    "    return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UC_size  = 4000\n",
    "CC_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def weighted_binary_cross_entropy(output, target):\n",
    "        \n",
    "#     loss = weights[1] *(target * torch.log(output)) + \\\n",
    "#            weights[0] * ((1 - target) * torch.log(1 - output))\n",
    "#     output --> kber ,loss \n",
    "    loss = (target * torch.log(output)) + \\\n",
    "            ((1 - target) * torch.log(1 - output))\n",
    "\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def customCollate(batch: list):\n",
    "    # batch --> tuple (x,y)\n",
    "    # x --> list of 4 lists\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    \n",
    "    CC,UC = zip(*x_batch)\n",
    "    \n",
    "    CC_padded = []\n",
    "    UC_padded = []\n",
    "    \n",
    "    for i  in range(len(x_batch)):\n",
    "        \n",
    "        CC_padded.append( list(CC[i]) + [0] * (CC_size - len(CC[i])) )\n",
    "        UC_padded.append( list(UC[i]) + [0] * (UC_size - len(UC[i])) )\n",
    "        \n",
    "\n",
    "    return  CC_padded,UC_padded, y_batch\n",
    "\n",
    "def train(model, train_dataset, batch_size=10, epochs=20, learning_rate=0.0001):\n",
    "    \n",
    "    # (1) create the dataloader of the training set IMPORTANT (make the shuffle=True)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True ,collate_fn = customCollate)\n",
    "    \n",
    "    \n",
    "    # (2) make the criterion cross entropy loss\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # (3) create the optimizer (Adam)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\") \n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "   \n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        n_samples = 0\n",
    "        y_size = 0\n",
    "        true_pos = 0\n",
    "        false_neg = 0\n",
    "        false_pos = 0 \n",
    "        true_neg = 0\n",
    "        for CC,UC, y_batch in tqdm(train_dataloader):\n",
    "            # torch.cuda.empty_cache()     \n",
    "            CC_tensor =torch.tensor(CC, device=device)\n",
    "            UC_tensor = torch.tensor(UC, device=device)\n",
    "           \n",
    "            y_batch_tensor = torch.tensor(y_batch,dtype = torch.float32 ,device=device)\n",
    "            \n",
    "            output = model.forward(CC_tensor,UC_tensor)\n",
    "            \n",
    "            _,predicted=torch.max(output,dim=1)\n",
    "            batch_loss = criterion(output,y_batch_tensor.long())\n",
    "            # batch_loss = criterion(output, y_batch_tensor)\n",
    "            # torch.cuda.empty_cache()\n",
    "            total_loss_train += batch_loss\n",
    "            # print(torch.cuda.memory_reserved()/1024**3)\n",
    "              \n",
    "\n",
    "            # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "            \n",
    "    #       (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "            acc = (predicted==y_batch_tensor).sum().item()\n",
    "            n_samples += y_batch_tensor.size(0)\n",
    "            total_acc_train += acc\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    #       (11) do the backward pass\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "    #       (12) update the weights with your optimizer\n",
    "            optimizer.step()\n",
    "            # (10) zero your gradients\n",
    "            \n",
    "    #   epoch loss\n",
    "        epoch_loss = total_loss_train / n_samples\n",
    "\n",
    "    #   (13) calculate the accuracy\n",
    "        epoch_acc =100*total_acc_train/n_samples\n",
    "\n",
    "        print(\n",
    "           f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "           | Train Accuracy: {epoch_acc}\\n')\n",
    "        print(\"true_pos = \", true_pos)\n",
    "        print(\"false_neg = \",false_neg)\n",
    "        print(\"false_pos = \",false_pos)\n",
    "        print(\"true_neg = \", true_neg)\n",
    "        \n",
    "\n",
    "  ##############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DLModel(embedding_matrix, embedding_matrix.shape,UC_size,CC_size, hidden_size=128, classes=2)    \n",
    "train(model, dataset_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
