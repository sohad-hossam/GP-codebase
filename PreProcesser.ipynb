{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from treelib import Node, Tree\n",
    "# from tree_sitter import Language, Parser\n",
    "import math\n",
    "import re\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download() #download_folder = C:/nltk_data \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the java code\n",
    "\n",
    "dataset_txt = open(\"operation1.java\", \"r\", encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "railo\n",
      "runtime\n",
      "sql\n",
      "exp\n",
      "op\n",
      "railo\n",
      "runtime\n",
      "sql\n",
      "exp\n",
      "Expression\n",
      "railo\n",
      "runtime\n",
      "sql\n",
      "exp\n",
      "Expression Support\n",
      "Operation\n",
      "Expression Support\n",
      "Operation\n",
      "Expression\n",
      "exp\n",
      "operator\n",
      "exp\n",
      "Expression\n",
      "get Exp\n",
      "exp\n",
      "operator\n",
      "get Operator\n",
      "operator\n",
      "Operation\n",
      "Expression\n",
      "exp\n",
      "operator\n",
      "exp\n",
      "exp\n",
      "operator\n",
      "operator\n",
      "String\n",
      "to String\n",
      "no Alias\n",
      "has Alias\n",
      "no Alias\n",
      "operator\n",
      "OPERATION\n",
      "IS\n",
      "NULL\n",
      "operator\n",
      "OPERATION\n",
      "IS\n",
      "NOT\n",
      "NULL\n",
      "exp\n",
      "to String\n",
      "true\n",
      "Operation\n",
      "to String\n",
      "operator\n",
      "Operation\n",
      "to String\n",
      "operator\n",
      "exp\n",
      "to String\n",
      "true\n",
      "to String\n",
      "true\n",
      "get Alias\n",
      "['railo', 'runtim', 'sql', 'exp', 'op', 'railo', 'runtim', 'sql', 'exp', 'express', 'railo', 'runtim', 'sql', 'exp', 'express', 'support', 'oper', 'express', 'support', 'oper', 'express', 'exp', 'oper', 'exp', 'express', 'get', 'exp', 'exp', 'oper', 'get', 'oper', 'oper', 'oper', 'express', 'exp', 'oper', 'exp', 'exp', 'oper', 'oper', 'string', 'to', 'string', 'no', 'alia', 'ha', 'alia', 'no', 'alia', 'oper', 'oper', 'is', 'null', 'oper', 'oper', 'is', 'not', 'null', 'exp', 'to', 'string', 'true', 'oper', 'to', 'string', 'oper', 'oper', 'to', 'string', 'oper', 'exp', 'to', 'string', 'true', 'to', 'string', 'true', 'get', 'alia']\n"
     ]
    }
   ],
   "source": [
    "# 1) All the interpunction was removed. \n",
    "\n",
    "chars_to_remove = r'''(?x)\n",
    "_|\\s*;\\s*|\\s*,\\s*|\\s*\\.\\s*|  #Standalone Punctuation Marks\n",
    "\\s*\\+\\s*|\\s*-\\s*|\\s*\\/\\s*|\\s*\\*\\s*|  #arethmatic operations\n",
    "\\s*=\\s*|\\s*==\\s*|\\s*!=\\s*|\\s*>\\s*|\\s*>=\\s*|\\s*<\\s*|\\s*<=\\s*|\\s*&&\\s*|\n",
    "\\s*\\|\\|\\s*\n",
    "|\\s*!\\s*|\\s*\\\"\\s*|\\s*\\'\\s*|    #assignment \n",
    "\\s*\\(\\s*|\\s*\\)\\s*|\\s*{\\s*|\\s*}\\s*|\\s*\\[\\s*|\\s*\\]\\s*|\\s*@\\s*|\\s*:\\s*'''   #brackets\n",
    "SourceCodeCleaned =re.split(chars_to_remove,dataset_txt)\n",
    "\n",
    "#print(SourceCodeCleaned)\n",
    "\n",
    "\n",
    "#print(SourceCodeLower)\n",
    "\n",
    "# 2) All numeric characters were removed. \n",
    "numeric_chars_to_remove = r\"[0-9]\" \n",
    "SourceCodeCleanedOfNumbers = [re.sub(numeric_chars_to_remove, \"\", x) for x in SourceCodeCleaned]\n",
    "#print(SourceCodeCleanedOfNumbers)\n",
    "#3) All sentences were tokenized with NLTK. \n",
    "#4) The stop words corpus from NLTK was used to eliminate all stop words. \n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_tokenized=list()\n",
    "\n",
    "#5) All remaining terms were stemmed using the Porter Stemming Algorithm\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "#6) remove keywords in java\n",
    "keywords_java={\n",
    "    \"abstract\", \"continue\", \"for\", \"new\", \"switch\",\n",
    "    \"assert\", \"default\", \"goto\", \"package\", \"synchronized\",\n",
    "    \"boolean\", \"do\", \"if\", \"private\", \"this\",\n",
    "    \"break\", \"double\", \"implements\", \"protected\", \"throw\",\n",
    "    \"byte\", \"else\", \"import\", \"public\", \"throws\",\n",
    "    \"case\", \"enum\", \"instanceof\", \"return\", \"transient\",\n",
    "    \"catch\", \"extends\", \"int\", \"short\", \"try\",\n",
    "    \"char\", \"final\", \"interface\", \"static\", \"void\",\n",
    "    \"class\", \"finally\", \"long\", \"strictfp\", \"volatile\",\"Override\",\"Deprecated\",\"SafeVarArgs\",\"SuppressWarnings\",\"FuntionalInterface\",\"Inherited\",\"Documented\",\"Target\",\"Retention\",\"Repeatable\"} #java non_primitive datatypes not added ex: \"List\"\n",
    "\n",
    "for sentence in SourceCodeCleanedOfNumbers:\n",
    "    #7) All words were lowercased. \n",
    "    NTLKTokenized=word_tokenize(sentence)\n",
    "    for word in NTLKTokenized:             \n",
    "        if word not in stop_words and word != '' and word not in keywords_java:\n",
    "            split_words = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', word)\n",
    "            split_words_tokenized=word_tokenize(split_words)\n",
    "            print(split_words)\n",
    "            for word_part in split_words_tokenized:\n",
    "                word_lower=word_part.lower()\n",
    "                word_stem=porter_stemmer.stem(word_lower)\n",
    "                words_tokenized.append(word_stem)\n",
    "\n",
    "print(words_tokenized)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
