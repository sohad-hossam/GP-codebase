{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from PreProcessor import *\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AvgIDF: 4.82526895044485\n",
      "MaxIDF: 5.07753744390572\n",
      "DevIDF: 1.585410059886101e-08\n",
      "AvgICTF: 4.501321697645494\n",
      "MaxICTF: 4.7535901911063645\n",
      "DevICTF: 3.676128774581396e-08\n",
      "AvgEntropy: 44.00564850716837\n",
      "MedEntropy: 40.20253647247681\n",
      "MaxEntropy: 923.3237116658274\n",
      "DevEntropy: 54.026552915509114\n",
      "QS: 1.0\n",
      "AvgVAR: 5.050796994833571e-06\n",
      "MaxVAR: 0.00015949586034741763\n",
      "SumVAR: 0.009030825026762425\n",
      "AvgSCQ: 5.078069476532483\n",
      "MaxSCQ: 8.113287403020829\n",
      "SumSCQ: 9079.588224040079\n",
      "AvgPMI: 0.007684088421279792\n",
      "MaxPMI: 0.5050949490570055\n",
      "AvgIDF for Code: 4.371698719030342\n",
      "MaxIDF for Code: 5.762173934797756\n",
      "DevIDF for Code: 1.018918609432195e-08\n",
      "AvgICTF for Code: 3.363114975338951\n",
      "MaxICTF for Code: 4.7535901911063645\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 295\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvgICTF for Code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, AvgICTF(ictf_code))\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaxICTF for Code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, MaxICTF(ictf_code))\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevICTF for Code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mDevICTF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mictf_code\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m#print(entropy_code)\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvgEntropy for Code:\u001b[39m\u001b[38;5;124m\"\u001b[39m, AvgEntropy(entropy_code))\n",
      "Cell \u001b[1;32mIn[2], line 196\u001b[0m, in \u001b[0;36mDevICTF\u001b[1;34m(ictf_values)\u001b[0m\n\u001b[0;32m    194\u001b[0m avg_ictf \u001b[38;5;241m=\u001b[39mAvgICTF(ictf_values)\n\u001b[0;32m    195\u001b[0m diffs \u001b[38;5;241m=\u001b[39m [(ictf \u001b[38;5;241m-\u001b[39m avg_ictf) \u001b[38;5;28;01mfor\u001b[39;00m ictf \u001b[38;5;129;01min\u001b[39;00m ictf_values]\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdiffs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdiffs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "\n",
    "CodeTokens = set()\n",
    "UCTokens = set()\n",
    "UC_documents = list()\n",
    "code_documents = list()\n",
    "TotalTokens=set()\n",
    "entropy_uc = []\n",
    "entropy_code = []\n",
    "variance_uc = {}\n",
    "variance_code = {}\n",
    "SCQ_uc={}\n",
    "SCQ_code={}\n",
    "term_co_occurrences_uc = {}\n",
    "PMI_uc = {}\n",
    "term_co_occurrences_code= {}\n",
    "PMI_code = {}\n",
    "\n",
    "\n",
    "# Q, the set of query terms; q, a term in the query; D, the set of documents in the collection;\n",
    "# Dt, the set of documents containing term t\n",
    "# d, a document in the document collection D;\n",
    "# tf(t, D), the frequency of term t in all docs;\n",
    "# tf(t, d), the frequency of term t in d;\n",
    "# tf(t, Q), the frequency of term t in the query;\n",
    "# sim(di, dj), the cosine similarity between the vector-space representations of di and dj\n",
    "# idf(t) = log( |D|/|Dt|).\n",
    "# ictf(t) = log( |D|/tf(t,D) ).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"./UC\"):\n",
    "    filepath = os.path.join(\"./UC\", filename)\n",
    "    tokens = PreProcessor().UCPreProcessor(filepath)\n",
    "    UC_documents.append(tokens)\n",
    "    UCTokens.update(tokens.split())\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"./CC\"):\n",
    "    filepath = os.path.join(\"./CC\", filename)\n",
    "    tokens = PreProcessor().CodePreProcessor(filepath)\n",
    "    code_documents.append(tokens)\n",
    "    CodeTokens.update(tokens.split())\n",
    "\n",
    "TotalTokens = CodeTokens.union(UCTokens)\n",
    "\n",
    "\n",
    "vectorizer_uc = TfidfVectorizer(vocabulary=TotalTokens)\n",
    "tf_matrix_uc=vectorizer_uc.fit_transform(UC_documents)\n",
    "idf_uc = vectorizer_uc.idf_\n",
    "\n",
    "\n",
    "\n",
    "df_uc = np.sum(tf_matrix_uc > 0, axis=0).A1\n",
    "\n",
    "total_documents_uc = len(code_documents)\n",
    "ictf_uc = np.log(total_documents_uc / (df_uc + 1))\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_uc.get_feature_names_out()):\n",
    "    tf_term_doc = tf_matrix_uc[:, term_index].toarray().sum()+1  #tf(t,d)\n",
    "    SCQ_uc[term] = (1 + np.log(tf_term_doc)) * idf_uc[term_index]\n",
    "\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_uc.get_feature_names_out()):\n",
    "    term_entropy = 0\n",
    "    for doc in UC_documents:\n",
    "        tf_term_doc = doc.count(term)\n",
    "        tf_term_collection = df_uc[term_index]\n",
    "        \n",
    "        smoothing_factor = 1  \n",
    "        tf_term_doc = tf_term_doc + smoothing_factor\n",
    "        tf_term_collection = tf_term_collection + smoothing_factor\n",
    "        term_entropy += (tf_term_doc / tf_term_collection) * np.log((tf_term_doc / tf_term_collection)+1)\n",
    "    entropy_uc.append(term_entropy)\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_uc.get_feature_names_out()):\n",
    "    term_weights = []\n",
    "    for doc_index, doc in enumerate(UC_documents):\n",
    "        tf_term_doc = doc.count(term)\n",
    "        weight_term_doc = (1 / len(doc)) * np.log(1 + tf_term_doc) * idf_uc[term_index]\n",
    "        term_weights.append(weight_term_doc)\n",
    "\n",
    "    avg_weight_term = np.mean(term_weights)\n",
    "    variance_term = np.mean([(weight - avg_weight_term) ** 2 for weight in term_weights])\n",
    "    variance_uc[term] = variance_term\n",
    "\n",
    "\n",
    "for doc in UC_documents:\n",
    "  \n",
    "    unique_terms = set(doc)\n",
    "    for term1 in unique_terms:\n",
    "        for term2 in unique_terms:\n",
    "            if term1 != term2:\n",
    "                term_co_occurrences_uc[(term1, term2)] = term_co_occurrences_uc.get((term1, term2), 0) + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for term_pair, co_occurrence_count in term_co_occurrences_uc.items():\n",
    "    term1, term2 = term_pair\n",
    "    pt1_t2_D = co_occurrence_count / len(UC_documents)\n",
    "    pt1_D = sum(1 for doc in UC_documents if term1 in doc) / len(UC_documents)\n",
    "    pt2_D = sum(1 for doc in UC_documents if term2 in doc) / len(UC_documents)\n",
    "    pt_D = pt1_D * pt2_D\n",
    "\n",
    "    if pt_D != 0:\n",
    "        PMI_uc[term_pair] = np.log(pt1_t2_D / pt_D)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer_code = TfidfVectorizer(vocabulary=TotalTokens)\n",
    "tf_matrix_code=vectorizer_code.fit_transform(code_documents)\n",
    "idf_code = vectorizer_code.idf_\n",
    "df_code = np.sum(tf_matrix_code > 0, axis=0).A1\n",
    "total_documents_code = len(code_documents)\n",
    "ictf_code = np.log(total_documents_code / (df_code + 1))\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_code.get_feature_names_out()):\n",
    "    tf_term_doc = tf_matrix_code[:, term_index].toarray().sum()+1\n",
    "    SCQ_code[term] = (1 + np.log(tf_term_doc)) * idf_code[term_index]\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_code.get_feature_names_out()):\n",
    "    term_entropy = 0\n",
    "    for doc in code_documents:\n",
    "        tf_term_doc = doc.count(term)\n",
    "        tf_term_collection = df_code[term_index]\n",
    "\n",
    "        smoothing_factor = 1  \n",
    "        tf_term_doc = tf_term_doc + smoothing_factor\n",
    "        tf_term_collection = tf_term_collection + smoothing_factor\n",
    "        term_entropy += (tf_term_doc / tf_term_collection) * np.log((tf_term_doc / tf_term_collection)+1)\n",
    "    entropy_code.append(term_entropy)\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_code.get_feature_names_out()):\n",
    "    term_weights = []\n",
    "    for doc_index, doc in enumerate(code_documents):\n",
    "        tf_term_doc = doc.count(term)\n",
    "        weight_term_doc = (1 / len(doc)) * np.log(1 + tf_term_doc) * idf_code[term_index]\n",
    "        term_weights.append(weight_term_doc)\n",
    "\n",
    "    avg_weight_term = np.mean(term_weights)\n",
    "    variance_term = np.mean([(weight - avg_weight_term) ** 2 for weight in term_weights])\n",
    "    variance_code[term] = variance_term\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for doc in code_documents:\n",
    "    unique_terms = set(doc)\n",
    "    for term1 in unique_terms:\n",
    "        for term2 in unique_terms:\n",
    "            if term1 != term2:\n",
    "                term_co_occurrences_code[(term1, term2)] = term_co_occurrences_code.get((term1, term2), 0) + 1\n",
    "\n",
    "\n",
    "\n",
    "for term_pair, co_occurrence_count in term_co_occurrences_code.items():\n",
    "    term1, term2 = term_pair\n",
    "    pt1_t2_D = co_occurrence_count / len(code_documents)\n",
    "    pt1_D = sum(1 for doc in code_documents if term1 in doc) / len(code_documents)\n",
    "    pt2_D = sum(1 for doc in code_documents if term2 in doc) / len(code_documents)\n",
    "    pt_D = pt1_D * pt2_D\n",
    "\n",
    "    if pt_D != 0:\n",
    "        PMI_code[term_pair] = np.log(pt1_t2_D / pt_D)\n",
    "\n",
    "\n",
    "def AvgIDF(idf_values):\n",
    "    return np.mean(idf_values)\n",
    "\n",
    "def MaxIDF(idf_values):\n",
    "    return np.max(idf_values)\n",
    "\n",
    "def DevIDF(idf_values):\n",
    "    avg_idf =AvgIDF(idf_values)\n",
    "    diffs = [(idf - avg_idf) for idf in idf_values]\n",
    "    return np.sqrt(sum(diffs) / len(diffs))\n",
    "\n",
    "def AvgICTF(ictf_values):\n",
    "     return np.mean(ictf_values)\n",
    "\n",
    "def MaxICTF(ictf_values):\n",
    "    return max(ictf_values)\n",
    "\n",
    "\n",
    "def DevICTF(ictf_values):\n",
    "    avg_ictf =AvgICTF(ictf_values)\n",
    "    diffs = [(ictf - avg_ictf) for ictf in ictf_values]\n",
    "    return math.sqrt(sum(diffs) / len(diffs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def AvgEntropy(entropy_values):\n",
    "    return np.mean(entropy_values)\n",
    "\n",
    "\n",
    "def MedEntropy(entropy_values):\n",
    "    return np.median(entropy_values)\n",
    "\n",
    "def MaxEntropy(entropy_values):\n",
    "    return max(entropy_values)\n",
    "\n",
    "def DevEntropy(entropy_values):\n",
    "    avg_entropy =AvgEntropy(entropy_values)\n",
    "    diffs = [(entropy - avg_entropy)** 2 for entropy in entropy_values] #made an assumption en fe square l2n fe negative values w bgd i no longer know gaya mnen , i can aslo asumme abs bs msh 3aref\n",
    "    if(np.mean(diffs)<0):\n",
    "        print(\"NEGATIVE A3AAAAAAAAAA\")\n",
    "        print(np.mean(diffs))\n",
    "    return math.sqrt(np.mean(diffs))\n",
    "\n",
    "\n",
    "\n",
    "def QS(Tokens,Docuemnts):           #so not sure of it wm3mltsh el SCS wel CS brdo yooh\n",
    "    documents_with_query_terms=0\n",
    "    for document in Docuemnts:\n",
    "        contains_query_term = any(term in document for term in Tokens)\n",
    "        if contains_query_term:\n",
    "            documents_with_query_terms += 1\n",
    "    query_scope = documents_with_query_terms / len(Docuemnts)\n",
    "\n",
    "    return query_scope\n",
    "\n",
    " \n",
    "def AvgVAR(var_values):\n",
    "    return sum(list(var_values.values()))/len(var_values)\n",
    "\n",
    "def MaxVAR(var_values):\n",
    "    return max(var_values.values())\n",
    "\n",
    "def SumVAR(var_values):\n",
    "    return sum(var_values.values()) \n",
    "\n",
    "\n",
    "\n",
    "def AvgSCQ(scq_values):\n",
    "    return sum(list(scq_values.values()))/len(scq_values) \n",
    "\n",
    "def MaxSCQ(scq_values):\n",
    "    return max(scq_values.values())\n",
    "\n",
    "def SumSCQ(scq_values):\n",
    "    return sum(scq_values.values()) \n",
    "\n",
    "\n",
    "def AvgPMI(PMI_values):\n",
    "    num_terms = len(PMI_values)\n",
    "    return 2 * sum(PMI_values.values()) * math.exp(math.lgamma(num_terms - 1) - math.lgamma(num_terms))\n",
    "\n",
    "\n",
    "def MaxPMI(PMI_values):\n",
    "    return max(PMI_values.values())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"AvgIDF:\", AvgIDF(idf_uc))\n",
    "print(\"MaxIDF:\", MaxIDF(idf_uc))\n",
    "print(\"DevIDF:\", DevIDF(idf_uc))\n",
    "print(\"AvgICTF:\", AvgICTF(ictf_uc))\n",
    "print(\"MaxICTF:\", MaxICTF(ictf_uc))\n",
    "print(\"DevICTF:\", DevICTF(ictf_uc))\n",
    "#print(entropy_uc)\n",
    "print(\"AvgEntropy:\", AvgEntropy(entropy_uc))\n",
    "print(\"MedEntropy:\", MedEntropy(entropy_uc))\n",
    "print(\"MaxEntropy:\", MaxEntropy(entropy_uc))\n",
    "print(\"DevEntropy:\", DevEntropy(entropy_uc))\n",
    "\n",
    "print(\"QS:\", QS(UCTokens,UC_documents))\n",
    "\n",
    "\n",
    "print(\"AvgVAR:\", AvgVAR(variance_uc))\n",
    "print(\"MaxVAR:\", MaxVAR(variance_uc))\n",
    "print(\"SumVAR:\", SumVAR(variance_uc))\n",
    "print(\"AvgSCQ:\", AvgSCQ(SCQ_uc))\n",
    "print(\"MaxSCQ:\", MaxSCQ(SCQ_uc))\n",
    "print(\"SumSCQ:\", SumSCQ(SCQ_uc))\n",
    "#print(PMI_uc)\n",
    "print(\"AvgPMI:\", AvgPMI(PMI_uc))\n",
    "print(\"MaxPMI:\", MaxPMI(PMI_uc))\n",
    "\n",
    "\n",
    "print(\"AvgIDF for Code:\", AvgIDF(idf_code))\n",
    "print(\"MaxIDF for Code:\", MaxIDF(idf_code))\n",
    "print(\"DevIDF for Code:\", DevIDF(idf_code))\n",
    "print(\"AvgICTF for Code:\", AvgICTF(ictf_code))\n",
    "print(\"MaxICTF for Code:\", MaxICTF(ictf_code))\n",
    "print(\"DevICTF for Code:\", DevICTF(ictf_code))\n",
    "\n",
    "\n",
    "#print(entropy_code)\n",
    "print(\"AvgEntropy for Code:\", AvgEntropy(entropy_code))\n",
    "print(\"MedEntropy for Code:\", MedEntropy(entropy_code))\n",
    "print(\"MaxEntropy for Code:\", MaxEntropy(entropy_code))\n",
    "print(\"DevEntropy for Code:\", DevEntropy(entropy_code))\n",
    "\n",
    "print(\"QS for Code:\", QS(CodeTokens,code_documents))\n",
    "\n",
    "print(\"AvgVAR for Code:\", AvgVAR(variance_code))\n",
    "print(\"MaxVAR for Code:\", MaxVAR(variance_code))\n",
    "print(\"SumVAR for Code:\", SumVAR(variance_code))\n",
    "\n",
    "print(\"AvgSCQ for Code:\", AvgSCQ(SCQ_code))\n",
    "print(\"MaxSCQ for Code:\", MaxSCQ(SCQ_code))\n",
    "print(\"SumSCQ for Code:\", SumSCQ(SCQ_code))\n",
    "\n",
    "\n",
    "print(\"AvgPMI for Code:\", AvgPMI(PMI_code))\n",
    "print(\"MaxPMI for Code:\", MaxPMI(PMI_code))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1788,)\n"
     ]
    }
   ],
   "source": [
    "print(idf_uc.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
