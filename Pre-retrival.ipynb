{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from PreProcessor import *\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AvgIDF: 4.831024993482794\n",
      "MaxIDF: 5.07753744390572\n",
      "DevIDF: 3.298274440662826e-08\n",
      "AvgICTF: 4.50707774068344\n",
      "MaxICTF: 4.7535901911063645\n",
      "DevICTF: 1.0602468939887225e-08\n",
      "AvgEntropy: 44.230479262323676\n",
      "MedEntropy: 40.20253647247681\n",
      "MaxEntropy: 926.913714085121\n",
      "DevEntropy: 54.315705854454464\n",
      "QS: 1.0\n",
      "AvgVAR: 5.013187025614618e-06\n",
      "MaxVAR: 0.00015988511630180347\n",
      "SumVAR: 0.009209224566054053\n",
      "AvgSCQ: 5.07952713853131\n",
      "MaxSCQ: 8.113287403020829\n",
      "SumSCQ: 9331.091353482016\n",
      "AvgPMI: 0.009093315896660323\n",
      "MaxPMI: 0.5050949490570055\n",
      "AvgIDF for Code: 4.357158367434626\n",
      "MaxIDF for Code: 5.762173934797756\n",
      "DevIDF for Code: 1.9388567773946154e-08\n",
      "AvgICTF for Code: 3.3485746237432346\n",
      "MaxICTF for Code: 4.7535901911063645\n",
      "DevICTF for Code: 1.100523689204449e-08\n",
      "AvgEntropy for Code: 127.4474984269733\n",
      "MedEntropy for Code: 24.00739089677943\n",
      "MaxEntropy for Code: 14259.758480303997\n",
      "DevEntropy for Code: 682.6859747849345\n",
      "QS for Code: 1.0\n",
      "AvgVAR for Code: 1.0158852934714935e-06\n",
      "MaxVAR for Code: 3.0518496348751306e-05\n",
      "SumVAR for Code: 0.0018661812841071336\n",
      "AvgSCQ for Code: 5.0331702104045855\n",
      "MaxSCQ for Code: 9.5540493893014\n",
      "SumSCQ for Code: 9245.933676513223\n",
      "AvgPMI for Code: 0.007515761927792084\n",
      "MaxPMI for Code: 0.4631307499579735\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CodeTokens = set()\n",
    "UCTokens = set()\n",
    "UC_documents = list()\n",
    "code_documents = list()\n",
    "TotalTokens=set()\n",
    "entropy_uc = []\n",
    "entropy_code = []\n",
    "variance_uc = {}\n",
    "variance_code = {}\n",
    "SCQ_uc={}\n",
    "SCQ_code={}\n",
    "term_co_occurrences_uc = {}\n",
    "PMI_uc = {}\n",
    "term_co_occurrences_code= {}\n",
    "PMI_code = {}\n",
    "\n",
    "\n",
    "# Q, the set of query terms; q, a term in the query; D, the set of documents in the collection;\n",
    "# Dt, the set of documents containing term t\n",
    "# d, a document in the document collection D;\n",
    "# tf(t, D), the frequency of term t in all docs;\n",
    "# tf(t, d), the frequency of term t in d;\n",
    "# tf(t, Q), the frequency of term t in the query;\n",
    "# sim(di, dj), the cosine similarity between the vector-space representations of di and dj\n",
    "# idf(t) = log( |D|/|Dt|).\n",
    "# ictf(t) = log( |D|/tf(t,D) ).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"./UC\"):\n",
    "    filepath = os.path.join(\"./UC\", filename)\n",
    "    tokens = UCPreProcessor(filepath)\n",
    "    UC_documents.append(tokens)\n",
    "    UCTokens.update(tokens.split())\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"./CC\"):\n",
    "    filepath = os.path.join(\"./CC\", filename)\n",
    "    tokens = CodePreProcessor(filepath)\n",
    "    code_documents.append(tokens)\n",
    "    CodeTokens.update(tokens.split())\n",
    "\n",
    "TotalTokens = CodeTokens.union(UCTokens)\n",
    "\n",
    "\n",
    "vectorizer_uc = TfidfVectorizer(vocabulary=TotalTokens)\n",
    "tf_matrix_uc=vectorizer_uc.fit_transform(UC_documents)\n",
    "idf_uc = vectorizer_uc.idf_\n",
    "df_uc = np.sum(tf_matrix_uc > 0, axis=0).A1\n",
    "\n",
    "total_documents_uc = len(code_documents)\n",
    "ictf_uc = np.log(total_documents_uc / (df_uc + 1))\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_uc.get_feature_names_out()):\n",
    "    tf_term_doc = tf_matrix_uc[:, term_index].toarray().sum()+1  #tf(t,d)\n",
    "    SCQ_uc[term] = (1 + np.log(tf_term_doc)) * idf_uc[term_index]\n",
    "\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_uc.get_feature_names_out()):\n",
    "    term_entropy = 0\n",
    "    for doc in UC_documents:\n",
    "        tf_term_doc = doc.count(term)\n",
    "        tf_term_collection = df_uc[term_index]\n",
    "        \n",
    "        smoothing_factor = 1  \n",
    "        tf_term_doc = tf_term_doc + smoothing_factor\n",
    "        tf_term_collection = tf_term_collection + smoothing_factor\n",
    "        term_entropy += (tf_term_doc / tf_term_collection) * np.log((tf_term_doc / tf_term_collection)+1)\n",
    "    entropy_uc.append(term_entropy)\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_uc.get_feature_names_out()):\n",
    "    term_weights = []\n",
    "    for doc_index, doc in enumerate(UC_documents):\n",
    "        tf_term_doc = doc.count(term)\n",
    "        weight_term_doc = (1 / len(doc)) * np.log(1 + tf_term_doc) * idf_uc[term_index]\n",
    "        term_weights.append(weight_term_doc)\n",
    "\n",
    "    avg_weight_term = np.mean(term_weights)\n",
    "    variance_term = np.mean([(weight - avg_weight_term) ** 2 for weight in term_weights])\n",
    "    variance_uc[term] = variance_term\n",
    "\n",
    "\n",
    "for doc in UC_documents:\n",
    "  \n",
    "    unique_terms = set(doc)\n",
    "    for term1 in unique_terms:\n",
    "        for term2 in unique_terms:\n",
    "            if term1 != term2:\n",
    "                term_co_occurrences_uc[(term1, term2)] = term_co_occurrences_uc.get((term1, term2), 0) + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for term_pair, co_occurrence_count in term_co_occurrences_uc.items():\n",
    "    term1, term2 = term_pair\n",
    "    pt1_t2_D = co_occurrence_count / len(UC_documents)\n",
    "    pt1_D = sum(1 for doc in UC_documents if term1 in doc) / len(UC_documents)\n",
    "    pt2_D = sum(1 for doc in UC_documents if term2 in doc) / len(UC_documents)\n",
    "    pt_D = pt1_D * pt2_D\n",
    "\n",
    "    if pt_D != 0:\n",
    "        PMI_uc[term_pair] = np.log(pt1_t2_D / pt_D)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer_code = TfidfVectorizer(vocabulary=TotalTokens)\n",
    "tf_matrix_code=vectorizer_code.fit_transform(code_documents)\n",
    "idf_code = vectorizer_code.idf_\n",
    "df_code = np.sum(tf_matrix_code > 0, axis=0).A1\n",
    "total_documents_code = len(code_documents)\n",
    "ictf_code = np.log(total_documents_code / (df_code + 1))\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_code.get_feature_names_out()):\n",
    "    tf_term_doc = tf_matrix_code[:, term_index].toarray().sum()+1\n",
    "    SCQ_code[term] = (1 + np.log(tf_term_doc)) * idf_code[term_index]\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_code.get_feature_names_out()):\n",
    "    term_entropy = 0\n",
    "    for doc in code_documents:\n",
    "        tf_term_doc = doc.count(term)\n",
    "        tf_term_collection = df_code[term_index]\n",
    "\n",
    "        smoothing_factor = 1  \n",
    "        tf_term_doc = tf_term_doc + smoothing_factor\n",
    "        tf_term_collection = tf_term_collection + smoothing_factor\n",
    "        term_entropy += (tf_term_doc / tf_term_collection) * np.log((tf_term_doc / tf_term_collection)+1)\n",
    "    entropy_code.append(term_entropy)\n",
    "\n",
    "\n",
    "for term_index, term in enumerate(vectorizer_code.get_feature_names_out()):\n",
    "    term_weights = []\n",
    "    for doc_index, doc in enumerate(code_documents):\n",
    "        tf_term_doc = doc.count(term)\n",
    "        weight_term_doc = (1 / len(doc)) * np.log(1 + tf_term_doc) * idf_code[term_index]\n",
    "        term_weights.append(weight_term_doc)\n",
    "\n",
    "    avg_weight_term = np.mean(term_weights)\n",
    "    variance_term = np.mean([(weight - avg_weight_term) ** 2 for weight in term_weights])\n",
    "    variance_code[term] = variance_term\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for doc in code_documents:\n",
    "    unique_terms = set(doc)\n",
    "    for term1 in unique_terms:\n",
    "        for term2 in unique_terms:\n",
    "            if term1 != term2:\n",
    "                term_co_occurrences_code[(term1, term2)] = term_co_occurrences_code.get((term1, term2), 0) + 1\n",
    "\n",
    "\n",
    "\n",
    "for term_pair, co_occurrence_count in term_co_occurrences_code.items():\n",
    "    term1, term2 = term_pair\n",
    "    pt1_t2_D = co_occurrence_count / len(code_documents)\n",
    "    pt1_D = sum(1 for doc in code_documents if term1 in doc) / len(code_documents)\n",
    "    pt2_D = sum(1 for doc in code_documents if term2 in doc) / len(code_documents)\n",
    "    pt_D = pt1_D * pt2_D\n",
    "\n",
    "    if pt_D != 0:\n",
    "        PMI_code[term_pair] = np.log(pt1_t2_D / pt_D)\n",
    "\n",
    "\n",
    "def AvgIDF(idf_values):\n",
    "    return np.mean(idf_values)\n",
    "\n",
    "def MaxIDF(idf_values):\n",
    "    return np.max(idf_values)\n",
    "\n",
    "def DevIDF(idf_values):\n",
    "    avg_idf =AvgIDF(idf_values)\n",
    "    diffs = [(idf - avg_idf) for idf in idf_values]\n",
    "    return np.sqrt(sum(diffs) / len(diffs))\n",
    "\n",
    "def AvgICTF(ictf_values):\n",
    "     return np.mean(ictf_values)\n",
    "\n",
    "def MaxICTF(ictf_values):\n",
    "    return max(ictf_values)\n",
    "\n",
    "\n",
    "def DevICTF(ictf_values):\n",
    "    avg_ictf =AvgICTF(ictf_values)\n",
    "    diffs = [(ictf - avg_ictf) for ictf in ictf_values]\n",
    "    return math.sqrt(sum(diffs) / len(diffs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def AvgEntropy(entropy_values):\n",
    "    return np.mean(entropy_values)\n",
    "\n",
    "\n",
    "def MedEntropy(entropy_values):\n",
    "    return np.median(entropy_values)\n",
    "\n",
    "def MaxEntropy(entropy_values):\n",
    "    return max(entropy_values)\n",
    "\n",
    "def DevEntropy(entropy_values):\n",
    "    avg_entropy =AvgEntropy(entropy_values)\n",
    "    diffs = [(entropy - avg_entropy)** 2 for entropy in entropy_values] #made an assumption en fe square l2n fe negative values w bgd i no longer know gaya mnen , i can aslo asumme abs bs msh 3aref\n",
    "    if(np.mean(diffs)<0):\n",
    "        print(\"NEGATIVE A3AAAAAAAAAA\")\n",
    "        print(np.mean(diffs))\n",
    "    return math.sqrt(np.mean(diffs))\n",
    "\n",
    "\n",
    "\n",
    "def QS(Tokens,Docuemnts):           #so not sure of it wm3mltsh el SCS wel CS brdo yooh\n",
    "    documents_with_query_terms=0\n",
    "    for document in Docuemnts:\n",
    "        contains_query_term = any(term in document for term in Tokens)\n",
    "        if contains_query_term:\n",
    "            documents_with_query_terms += 1\n",
    "    query_scope = documents_with_query_terms / len(Docuemnts)\n",
    "\n",
    "    return query_scope\n",
    "\n",
    " \n",
    "def AvgVAR(var_values):\n",
    "    return sum(list(var_values.values()))/len(var_values)\n",
    "\n",
    "def MaxVAR(var_values):\n",
    "    return max(var_values.values())\n",
    "\n",
    "def SumVAR(var_values):\n",
    "    return sum(var_values.values()) \n",
    "\n",
    "\n",
    "\n",
    "def AvgSCQ(scq_values):\n",
    "    return sum(list(scq_values.values()))/len(scq_values) \n",
    "\n",
    "def MaxSCQ(scq_values):\n",
    "    return max(scq_values.values())\n",
    "\n",
    "def SumSCQ(scq_values):\n",
    "    return sum(scq_values.values()) \n",
    "\n",
    "\n",
    "def AvgPMI(PMI_values):\n",
    "    num_terms = len(PMI_values)\n",
    "    return 2 * sum(PMI_values.values()) * math.exp(math.lgamma(num_terms - 1) - math.lgamma(num_terms))\n",
    "\n",
    "\n",
    "def MaxPMI(PMI_values):\n",
    "    return max(PMI_values.values())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"AvgIDF:\", AvgIDF(idf_uc))\n",
    "print(\"MaxIDF:\", MaxIDF(idf_uc))\n",
    "print(\"DevIDF:\", DevIDF(idf_uc))\n",
    "print(\"AvgICTF:\", AvgICTF(ictf_uc))\n",
    "print(\"MaxICTF:\", MaxICTF(ictf_uc))\n",
    "print(\"DevICTF:\", DevICTF(ictf_uc))\n",
    "#print(entropy_uc)\n",
    "print(\"AvgEntropy:\", AvgEntropy(entropy_uc))\n",
    "print(\"MedEntropy:\", MedEntropy(entropy_uc))\n",
    "print(\"MaxEntropy:\", MaxEntropy(entropy_uc))\n",
    "print(\"DevEntropy:\", DevEntropy(entropy_uc))\n",
    "\n",
    "print(\"QS:\", QS(UCTokens,UC_documents))\n",
    "\n",
    "\n",
    "print(\"AvgVAR:\", AvgVAR(variance_uc))\n",
    "print(\"MaxVAR:\", MaxVAR(variance_uc))\n",
    "print(\"SumVAR:\", SumVAR(variance_uc))\n",
    "print(\"AvgSCQ:\", AvgSCQ(SCQ_uc))\n",
    "print(\"MaxSCQ:\", MaxSCQ(SCQ_uc))\n",
    "print(\"SumSCQ:\", SumSCQ(SCQ_uc))\n",
    "#print(PMI_uc)\n",
    "print(\"AvgPMI:\", AvgPMI(PMI_uc))\n",
    "print(\"MaxPMI:\", MaxPMI(PMI_uc))\n",
    "\n",
    "\n",
    "print(\"AvgIDF for Code:\", AvgIDF(idf_code))\n",
    "print(\"MaxIDF for Code:\", MaxIDF(idf_code))\n",
    "print(\"DevIDF for Code:\", DevIDF(idf_code))\n",
    "print(\"AvgICTF for Code:\", AvgICTF(ictf_code))\n",
    "print(\"MaxICTF for Code:\", MaxICTF(ictf_code))\n",
    "print(\"DevICTF for Code:\", DevICTF(ictf_code))\n",
    "\n",
    "\n",
    "#print(entropy_code)\n",
    "print(\"AvgEntropy for Code:\", AvgEntropy(entropy_code))\n",
    "print(\"MedEntropy for Code:\", MedEntropy(entropy_code))\n",
    "print(\"MaxEntropy for Code:\", MaxEntropy(entropy_code))\n",
    "print(\"DevEntropy for Code:\", DevEntropy(entropy_code))\n",
    "\n",
    "print(\"QS for Code:\", QS(CodeTokens,code_documents))\n",
    "\n",
    "print(\"AvgVAR for Code:\", AvgVAR(variance_code))\n",
    "print(\"MaxVAR for Code:\", MaxVAR(variance_code))\n",
    "print(\"SumVAR for Code:\", SumVAR(variance_code))\n",
    "\n",
    "print(\"AvgSCQ for Code:\", AvgSCQ(SCQ_code))\n",
    "print(\"MaxSCQ for Code:\", MaxSCQ(SCQ_code))\n",
    "print(\"SumSCQ for Code:\", SumSCQ(SCQ_code))\n",
    "\n",
    "\n",
    "print(\"AvgPMI for Code:\", AvgPMI(PMI_code))\n",
    "print(\"MaxPMI for Code:\", MaxPMI(PMI_code))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
