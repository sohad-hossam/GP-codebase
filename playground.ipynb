{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from VSM import *\n",
    "from FeatureExtraction import *\n",
    "import pandas as pd\n",
    "from statistics import stdev\n",
    "from dask.distributed import Client, progress\n",
    "import dask.array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PreProcessor = PreProcessor()\n",
    "UC_documents, code_documents, UCTokens, CodeTokens = _PreProcessor.setup(\n",
    "            \"./UC\", \"./CC\"\n",
    "        )\n",
    "UCTokens.update(CodeTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_components = min(tfidf_matrix_uc.shape[0], tfidf_matrix_code.shape[1])\n",
    "# num_components = min(num_components, 100)\n",
    "# LSA_model = TruncatedSVD(n_components=num_components)\n",
    "\n",
    "# # if train_or_test == \"train\":\n",
    "# LSA_data_useCases = LSA_model.fit_transform(tfidf_matrix_uc)\n",
    "# LSA_data_codes = LSA_model.fit_transform(tfidf_matrix_code) \n",
    "# # else:\n",
    "# #     LSA_data_useCases = LSA_model.transform(tfidf_matrix_uc)\n",
    "# #     LSA_data_codes = LSA_model.transform(tfidf_matrix_code) \n",
    "\n",
    "# LSA_similraity_matrix = cosine_similarity(LSA_data_useCases, LSA_data_codes)\n",
    "# print(LSA_similraity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = FeatureExtraction()\n",
    "# test.LSA(tfidf_matrix_uc, tfidf_matrix_code, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usecases (100*usecases_nu)\n",
    "#code (100*codes_nu)\n",
    "#LSA_similraity_matrix (usecases_nu*codes_nu)\n",
    "#d1,d2,d3,d4\n",
    "#u1,u2,u3,u4\n",
    "#u1*d1,u1*d2,u1*d3 ...\n",
    "#d1*u1,d1*u2,d1*u3 ...\n",
    "#each row coresspond between the use cases and each document\n",
    "# [[d1*c1 d1*c2],[d2*c1 d2*c2]]\n",
    "\n",
    "# LSA_data_codes=np.array([[1,2,3,4,5] , [6,7,8,9,10]])\n",
    "# LSA_data_useCases=np.array([[11,12,13,14,15] , [16,17,18,19,21],[2,5,9,11,42]])\n",
    "\n",
    "# LSA_data_codes_norm=np.repeat(np.linalg.norm(LSA_data_codes,axis=1),LSA_data_useCases.shape[0])\n",
    "# LSA_data_useCases_norm=np.repeat(np.linalg.norm(LSA_data_useCases,axis=1),LSA_data_codes.shape[0])\n",
    "\n",
    "# LSA_data_codes_norm=LSA_data_codes_norm.reshape(LSA_data_codes.shape[0],LSA_data_useCases.shape[0]).T \n",
    "# LSA_data_useCases_norm=LSA_data_useCases_norm.reshape(LSA_data_useCases.shape[0],LSA_data_codes.shape[0])\n",
    "\n",
    "# print(LSA_data_codes_norm)\n",
    "# print(LSA_data_useCases_norm)\n",
    "\n",
    "# LSA_similraity_matrix=cosine_similarity(LSA_data_useCases, LSA_data_codes)\n",
    "\n",
    "# print(LSA_similraity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen-Shannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = FeatureExtraction()\n",
    "test.JensenShannon(UCTokens, UC_documents, code_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getOverlap(query_term_list: list, total_query_list: list) -> int:\n",
    "    query_term_list = pd.Series(query_term_list)\n",
    "    query_term_list = query_term_list.nlargest(10)\n",
    "    query_term_set = set(query_term_list.index.values)\n",
    "\n",
    "    total_query_list = pd.Series(total_query_list)\n",
    "    total_query_list = total_query_list.nlargest(10)\n",
    "    total_query_set = set(total_query_list.index.values)\n",
    "\n",
    "    return(len(query_term_set & total_query_set))\n",
    "\n",
    "\n",
    "\n",
    "#running the query as a whole\n",
    "feature_extraction=FeatureExtraction(UCTokens)\n",
    "UC_count_matrix, code_count_matrix = feature_extraction.CountVectorizerModel(UC_documents, code_documents, 'train')\n",
    "\n",
    "### Run the original query q, and obtain the result list R \n",
    "total_score_whole_query_jensen_shannon = feature_extraction.JensenShannon(UC_count_matrix, code_count_matrix) \n",
    "\n",
    "### Run each individual query term qt in the original query as a separate query and obtain the result list Rt.\n",
    "overall_queries_score = list()\n",
    "for code_idx, code in enumerate(code_documents):\n",
    "    code_words = code.split(\" \")\n",
    "    query__term_overlap_list = list()\n",
    "    for word in code_words:\n",
    "        if word == '':\n",
    "            continue\n",
    "        # word_count_matrix = np.zeros((code_count_matrix.shape[0], code_count_matrix.shape[1]))\n",
    "        word_index = feature_extraction.code_vocab_index[word]\n",
    "        # word_count_matrix[:, word_index] = code_count_matrix[:, word_index]\n",
    "        # print(feature_extraction.count_vectorizer.vocabulary_[\"if\"])\n",
    "        # print(code_count_matrix[:, 752])\n",
    "        # print(code_count_matrix[:, word_index])\n",
    "        term_vector=np.zeros((1, UC_count_matrix.shape[1]))\n",
    "        term_vector[:,word_index]=code_count_matrix[code_idx, word_index]\n",
    "        query_term_jensen_shannon = feature_extraction.JensenShannon(UC_count_matrix, term_vector)\n",
    "        # print(query_term_jensen_shannon.shape)\n",
    "        query_term_len = query_term_jensen_shannon.shape[0] * query_term_jensen_shannon.shape[1]\n",
    "        query__term_overlap_list.append(getOverlap(query_term_jensen_shannon.reshape(query_term_len), total_score_whole_query_jensen_shannon[:, code_idx]))\n",
    "\n",
    "    overall_queries_score.append(stdev(query__term_overlap_list))\n",
    "# print(overall_queries_score)\n",
    "        # print(total_score_query_term_jensen_shannon)\n",
    "\n",
    "        \n",
    "    # break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(total_score_whole_query_jensen_shannon[:,7])\n",
    "# print(total_score_whole_query_jensen_shannon[:,6])\n",
    "# print(code[7])\n",
    "# for i, temp in enumerate(code[7]):\n",
    "#     if temp != 0:\n",
    "#         print(i)\n",
    "# # 3 features to be called\n",
    "#1st case => if the query is the code\n",
    "# total_score_whole_query_jensen_shannon=feature_extraction.JensenShannon(UC_documents,,\"train\") #58*116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverlap(query_term_list: np.ndarray, total_query_list: np.ndarray) -> int:\n",
    "    query_term_list = pd.Series(query_term_list)\n",
    "    query_term_list = query_term_list.nlargest(10)\n",
    "    query_term_set = set(query_term_list.index.values)\n",
    "\n",
    "    total_query_list = pd.Series(total_query_list)\n",
    "    total_query_list = total_query_list.nlargest(10)\n",
    "    total_query_set = set(total_query_list.index.values)\n",
    "\n",
    "    return(len(query_term_set & total_query_set))\n",
    "\n",
    "def SubqueryOverlapTerms(query_term: str, code_idx: int, UC_count_matrix: np.ndarray, code_count_matrix: np.ndarray, feature_extraction: object) -> np.ndarray:\n",
    "    word_index = feature_extraction.code_vocab_index[query_term]\n",
    "    term_vector = dask.array.zeros((1, UC_count_matrix.shape[1])) #1*\n",
    "    term_vector[:,word_index]=code_count_matrix[code_idx, word_index]\n",
    "\n",
    "    query_term_jensen_shannon = feature_extraction.JensenShannon(UC_count_matrix, term_vector)\n",
    "    query_term_len = query_term_jensen_shannon.shape[0] * query_term_jensen_shannon.shape[1]\n",
    "\n",
    "    query_term_jensen_shannon = query_term_jensen_shannon.reshape(query_term_len)\n",
    "    return query_term_jensen_shannon\n",
    "\n",
    "def SubqueryOverlapCode(code_idx, code, UC_count_matrix: np.ndarray, code_count_matrix: np.ndarray, feature_extraction: object, total_query_list: np.ndarray):\n",
    "\n",
    "    code_idx = int(code_idx)\n",
    "    code_words = code.split(\" \")\n",
    "    code_words = dask.array.from_array(code_words[0:-1])\n",
    "    vSubqueryOverlap = np.vectorize(lambda query_term: SubqueryOverlapTerms(query_term, code_idx, UC_count_matrix, code_count_matrix, feature_extraction), otypes=[np.ndarray])\n",
    "    \n",
    "    query_term_jensen_shannon = vSubqueryOverlap(code_words)\n",
    "   \n",
    "    vGetOverlap = np.vectorize(lambda query_term_jensen_shannon: getOverlap(query_term_jensen_shannon, total_query_list[:, code_idx]), otypes=[np.int16])\n",
    "    query__term_overlap = vGetOverlap(query_term_jensen_shannon)\n",
    "    \n",
    "    overall_queries_score = stdev(query__term_overlap.tolist())\n",
    "    return overall_queries_score\n",
    "\n",
    "\n",
    "# #running the query as a whole\n",
    "\n",
    "# feature_extraction=FeatureExtraction(UCTokens)\n",
    "# UC_count_matrix, code_count_matrix = feature_extraction.CountVectorizerModel(UC_documents, code_documents, 'train')\n",
    "\n",
    "# ### Run the original query q, and obtain the result list R \n",
    "# total_score_whole_query_jensen_shannon = feature_extraction.JensenShannon(UC_count_matrix, code_count_matrix) \n",
    "\n",
    "# ### Run each individual query term qt in the original query as a separate query and obtain the result list Rt.\n",
    "# code_documents_tuples = np.array(list(enumerate(code_documents)))\n",
    "# vSubqueryOverlapCode = np.vectorize(lambda code_idx, code_doc: SubqueryOverlapCode(code_idx, code_doc, UC_count_matrix, code_count_matrix, feature_extraction, total_score_whole_query_jensen_shannon), otypes=[np.float16])\n",
    "\n",
    "# overall_queries_score = vSubqueryOverlapCode(code_documents_tuples[:, 0], code_documents_tuples[:, 1])\n",
    "\n",
    "# print(overall_queries_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\basse\\anaconda3\\Lib\\site-packages\\scipy\\spatial\\distance.py:1250: RuntimeWarning: invalid value encountered in divide\n",
      "  q = q / np.sum(q, axis=axis, keepdims=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m vSubqueryOverlapCode \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m code_idx, code_doc: SubqueryOverlapCode(code_idx, code_doc, UC_count_matrix, code_count_matrix, feature_extraction, total_score_whole_query_jensen_shannon), otypes\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat16])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# print(code_documents_tuples[:,0].compute())\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m overall_queries_score \u001b[38;5;241m=\u001b[39m vSubqueryOverlapCode(code_documents_tuples[:,\u001b[38;5;241m0\u001b[39m], code_documents_tuples[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(overall_queries_score\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2329\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[0;32m   2327\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[1;32m-> 2329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorize_call(func\u001b[38;5;241m=\u001b[39mfunc, args\u001b[38;5;241m=\u001b[39mvargs)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2412\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[0;32m   2410\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m-> 2412\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ufunc(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     res \u001b[38;5;241m=\u001b[39m asanyarray(outputs, dtype\u001b[38;5;241m=\u001b[39motypes[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(code_idx, code_doc)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m### Run each individual query term qt in the original query as a separate query and obtain the result list Rt.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# code_documents_tuples = np.array(list(enumerate(code_documents)))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m code_documents_tuples \u001b[38;5;241m=\u001b[39m  dask\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mfrom_array(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(code_documents)))\n\u001b[1;32m---> 12\u001b[0m vSubqueryOverlapCode \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m code_idx, code_doc: SubqueryOverlapCode(code_idx, code_doc, UC_count_matrix, code_count_matrix, feature_extraction, total_score_whole_query_jensen_shannon), otypes\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat16])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# print(code_documents_tuples[:,0].compute())\u001b[39;00m\n\u001b[0;32m     15\u001b[0m overall_queries_score \u001b[38;5;241m=\u001b[39m vSubqueryOverlapCode(code_documents_tuples[:,\u001b[38;5;241m0\u001b[39m], code_documents_tuples[:,\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m, in \u001b[0;36mSubqueryOverlapCode\u001b[1;34m(code_idx, code, UC_count_matrix, code_count_matrix, feature_extraction, total_query_list)\u001b[0m\n\u001b[0;32m     27\u001b[0m code_words \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mfrom_array(code_words[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     28\u001b[0m vSubqueryOverlap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m query_term: SubqueryOverlapTerms(query_term, code_idx, UC_count_matrix, code_count_matrix, feature_extraction), otypes\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray])\n\u001b[1;32m---> 30\u001b[0m query_term_jensen_shannon \u001b[38;5;241m=\u001b[39m vSubqueryOverlap(code_words)\n\u001b[0;32m     32\u001b[0m vGetOverlap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m query_term_jensen_shannon: getOverlap(query_term_jensen_shannon, total_query_list[:, code_idx]), otypes\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mint16])\n\u001b[0;32m     33\u001b[0m query__term_overlap \u001b[38;5;241m=\u001b[39m vGetOverlap(query_term_jensen_shannon)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2329\u001b[0m, in \u001b[0;36mvectorize.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m     vargs \u001b[38;5;241m=\u001b[39m [args[_i] \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m inds]\n\u001b[0;32m   2327\u001b[0m     vargs\u001b[38;5;241m.\u001b[39mextend([kwargs[_n] \u001b[38;5;28;01mfor\u001b[39;00m _n \u001b[38;5;129;01min\u001b[39;00m names])\n\u001b[1;32m-> 2329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vectorize_call(func\u001b[38;5;241m=\u001b[39mfunc, args\u001b[38;5;241m=\u001b[39mvargs)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:2412\u001b[0m, in \u001b[0;36mvectorize._vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;66;03m# Convert args to object arrays first\u001b[39;00m\n\u001b[0;32m   2410\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [asanyarray(a, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m-> 2412\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ufunc(\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mnout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     res \u001b[38;5;241m=\u001b[39m asanyarray(outputs, dtype\u001b[38;5;241m=\u001b[39motypes[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m, in \u001b[0;36mSubqueryOverlapCode.<locals>.<lambda>\u001b[1;34m(query_term)\u001b[0m\n\u001b[0;32m     26\u001b[0m code_words \u001b[38;5;241m=\u001b[39m code\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m code_words \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mfrom_array(code_words[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 28\u001b[0m vSubqueryOverlap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m query_term: SubqueryOverlapTerms(query_term, code_idx, UC_count_matrix, code_count_matrix, feature_extraction), otypes\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray])\n\u001b[0;32m     30\u001b[0m query_term_jensen_shannon \u001b[38;5;241m=\u001b[39m vSubqueryOverlap(code_words)\n\u001b[0;32m     32\u001b[0m vGetOverlap \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvectorize(\u001b[38;5;28;01mlambda\u001b[39;00m query_term_jensen_shannon: getOverlap(query_term_jensen_shannon, total_query_list[:, code_idx]), otypes\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mint16])\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mSubqueryOverlapTerms\u001b[1;34m(query_term, code_idx, UC_count_matrix, code_count_matrix, feature_extraction)\u001b[0m\n\u001b[0;32m     14\u001b[0m term_vector \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, UC_count_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;66;03m#1*\u001b[39;00m\n\u001b[0;32m     15\u001b[0m term_vector[:,word_index]\u001b[38;5;241m=\u001b[39mcode_count_matrix[code_idx, word_index]\n\u001b[1;32m---> 17\u001b[0m query_term_jensen_shannon \u001b[38;5;241m=\u001b[39m feature_extraction\u001b[38;5;241m.\u001b[39mJensenShannon(UC_count_matrix, term_vector)\n\u001b[0;32m     18\u001b[0m query_term_len \u001b[38;5;241m=\u001b[39m query_term_jensen_shannon\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m query_term_jensen_shannon\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     20\u001b[0m query_term_jensen_shannon \u001b[38;5;241m=\u001b[39m query_term_jensen_shannon\u001b[38;5;241m.\u001b[39mreshape(query_term_len)\n",
      "File \u001b[1;32md:\\College Items\\GP code\\GP-codebase\\FeatureExtraction.py:71\u001b[0m, in \u001b[0;36mFeatureExtraction.JensenShannon\u001b[1;34m(self, UC_count_matrix, code_count_matrix)\u001b[0m\n\u001b[0;32m     63\u001b[0m UC_count_matrix_repeated \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(\n\u001b[0;32m     64\u001b[0m     UC_count_matrix, code_count_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     66\u001b[0m code_count_matrix_repeated \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(\n\u001b[0;32m     67\u001b[0m     code_count_matrix, (UC_count_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     68\u001b[0m )\n\u001b[0;32m     70\u001b[0m JS_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mpow\u001b[39m(\n\u001b[1;32m---> 71\u001b[0m     jensenshannon(UC_count_matrix_repeated, code_count_matrix_repeated, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     74\u001b[0m JS_matrix \u001b[38;5;241m=\u001b[39m JS_matrix\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m     75\u001b[0m     UC_count_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], code_count_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     76\u001b[0m )\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# ------------------------------loop approach---------------------------#\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# loop-approach\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# JS_matrix = np.zeros((UC_count_matrix.shape[0], code_count_matrix.shape[0]))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m#         JS_matrix[i][j] = pow(jensenshannon(UC_count_vector, code_count_vector), 2)\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# print(JS_matrix.shape) => (58,116)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\scipy\\spatial\\distance.py:1248\u001b[0m, in \u001b[0;36mjensenshannon\u001b[1;34m(p, q, base, axis, keepdims)\u001b[0m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;124;03mCompute the Jensen-Shannon distance (metric) between\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;124;03mtwo probability arrays. This is the square root\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \n\u001b[0;32m   1246\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m p \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(p)\n\u001b[1;32m-> 1248\u001b[0m q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(q)\n\u001b[0;32m   1249\u001b[0m p \u001b[38;5;241m=\u001b[39m p \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(p, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1250\u001b[0m q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(q, axis\u001b[38;5;241m=\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\dask\\array\\core.py:1701\u001b[0m, in \u001b[0;36mArray.__array__\u001b[1;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[0;32m   1700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1701\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[0;32m   1703\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\dask\\base.py:310\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\dask\\base.py:595\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[0;32m    593\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m--> 595\u001b[0m results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\distributed\\client.py:3243\u001b[0m, in \u001b[0;36mClient.get\u001b[1;34m(self, dsk, keys, workers, allow_other_workers, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[0;32m   3241\u001b[0m         should_rejoin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3243\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(packed, asynchronous\u001b[38;5;241m=\u001b[39masynchronous, direct\u001b[38;5;241m=\u001b[39mdirect)\n\u001b[0;32m   3244\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   3245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m futures\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\distributed\\client.py:2368\u001b[0m, in \u001b[0;36mClient.gather\u001b[1;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[0;32m   2366\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m   2367\u001b[0m     local_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync(\n\u001b[0;32m   2369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather,\n\u001b[0;32m   2370\u001b[0m     futures,\n\u001b[0;32m   2371\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   2372\u001b[0m     direct\u001b[38;5;241m=\u001b[39mdirect,\n\u001b[0;32m   2373\u001b[0m     local_worker\u001b[38;5;241m=\u001b[39mlocal_worker,\n\u001b[0;32m   2374\u001b[0m     asynchronous\u001b[38;5;241m=\u001b[39masynchronous,\n\u001b[0;32m   2375\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\distributed\\utils.py:351\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[1;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, callback_timeout\u001b[38;5;241m=\u001b[39mcallback_timeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    353\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\distributed\\utils.py:414\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m e\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[1;32m--> 414\u001b[0m         wait(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[0;32m    417\u001b[0m     typ, exc, tb \u001b[38;5;241m=\u001b[39m error\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\site-packages\\distributed\\utils.py:403\u001b[0m, in \u001b[0;36msync.<locals>.wait\u001b[1;34m(timeout)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(timeout):\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         loop\u001b[38;5;241m.\u001b[39madd_callback(cancel)\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\basse\\anaconda3\\Lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "client = Client(processes=False, threads_per_worker=4,n_workers=1, memory_limit='2GB')\n",
    "\n",
    "feature_extraction=FeatureExtraction(UCTokens)\n",
    "UC_count_matrix, code_count_matrix = feature_extraction.CountVectorizerModel(UC_documents, code_documents, 'train')\n",
    "\n",
    "### Run the original query q, and obtain the result list R \n",
    "total_score_whole_query_jensen_shannon = feature_extraction.JensenShannon(UC_count_matrix, code_count_matrix) \n",
    "\n",
    "### Run each individual query term qt in the original query as a separate query and obtain the result list Rt.\n",
    "# code_documents_tuples = np.array(list(enumerate(code_documents)))\n",
    "code_documents_tuples =  dask.array.from_array(list(enumerate(code_documents)))\n",
    "vSubqueryOverlapCode = np.vectorize(lambda code_idx, code_doc: SubqueryOverlapCode(code_idx, code_doc, UC_count_matrix, code_count_matrix, feature_extraction, total_score_whole_query_jensen_shannon), otypes=[np.float16])\n",
    "# print(code_documents_tuples[:,0].compute())\n",
    "\n",
    "overall_queries_score = vSubqueryOverlapCode(code_documents_tuples[:,0], code_documents_tuples[:,1])\n",
    "\n",
    "print(overall_queries_score.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play ground yasmenya ama nshof eh el araf da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UCPreProcessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./UC\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     23\u001b[0m     filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./UC\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[1;32m---> 24\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m UCPreProcessor(filepath)\n\u001b[0;32m     25\u001b[0m     UC_documents\u001b[38;5;241m.\u001b[39mappend(tokens)\n\u001b[0;32m     26\u001b[0m     UCTokens\u001b[38;5;241m.\u001b[39mupdate(tokens\u001b[38;5;241m.\u001b[39msplit())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'UCPreProcessor' is not defined"
     ]
    }
   ],
   "source": [
    "from imports import *\n",
    "from PreProcessor import *\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "CodeTokens = set()\n",
    "UCTokens = set()\n",
    "UC_documents = list()\n",
    "code_documents = list()\n",
    "TotalTokens=set()\n",
    "entropy_uc = []\n",
    "entropy_code = []\n",
    "variance_uc = {}\n",
    "variance_code = {}\n",
    "SCQ_uc={}\n",
    "SCQ_code={}\n",
    "term_co_occurrences_uc = {}\n",
    "PMI_uc = {}\n",
    "term_co_occurrences_code= {}\n",
    "PMI_code = {}\n",
    "\n",
    "for filename in os.listdir(\"./UC\"):\n",
    "    filepath = os.path.join(\"./UC\", filename)\n",
    "    tokens = UCPreProcessor(filepath)\n",
    "    UC_documents.append(tokens)\n",
    "    UCTokens.update(tokens.split())\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"./CC\"):\n",
    "    filepath = os.path.join(\"./CC\", filename)\n",
    "    tokens = CodePreProcessor(filepath)\n",
    "    code_documents.append(tokens)\n",
    "    CodeTokens.update(tokens.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_uc = TfidfVectorizer(vocabulary=UCTokens)\n",
    "tf_matrix_uc=vectorizer_uc.fit_transform(UC_documents)\n",
    "idf_uc = vectorizer_uc.idf_\n",
    "\n",
    "\n",
    "\n",
    "# Get the vocabulary from the TfidfVectorizer object\n",
    "vocabulary_uc = vectorizer_uc.get_feature_names_out()\n",
    "\n",
    "# Pair terms with their corresponding document frequencies\n",
    "term_document_frequencies = dict(zip(vocabulary_uc,idf_uc))\n",
    "\n",
    "print(term_document_frequencies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
